{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-08T01:37:30.601974Z",
     "start_time": "2019-12-08T01:37:30.598007Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import io\n",
    "# from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, f1_score,\\\n",
    "                            precision_score, recall_score, accuracy_score,\\\n",
    "                            average_precision_score, precision_recall_curve\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from src.helper_functions import load_data, get_model_perfs, init_model_perfs,\\\n",
    "                                 save_model, save_model_perfs, check_is_best,\\\n",
    "                                 read_model, evaluate_model_predictions,\\\n",
    "                                 update_model_perfs, check_and_save,\\\n",
    "                                 adjusted_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-08T01:37:36.137723Z",
     "start_time": "2019-12-08T01:37:36.133565Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.5'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "from keras.callbacks import Callback\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-08T01:37:51.318040Z",
     "start_time": "2019-12-08T01:37:39.889560Z"
    }
   },
   "outputs": [],
   "source": [
    "x_tr, y_tr, x_te, y_te = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Build Neural Network\n",
    "\n",
    "Following the desciption in section 2.2.4 of the [DeepTox article](https://www.frontiersin.org/articles/10.3389/fenvs.2015.00080/full), I tried to use intermediate values in [Table 2](https://www.frontiersin.org/articles/10.3389/fenvs.2015.00080/full#T2) to build the neural network:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Following [this question/answer](https://datascience.stackexchange.com/questions/45165/how-to-get-accuracy-f1-precision-and-recall-for-a-keras-model) and [this question/answer](https://stackoverflow.com/questions/54065733/how-to-employ-the-scikit-learn-evaluation-metrics-functions-with-keras-in-python) to implement usage of recall in model training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T00:31:25.480771Z",
     "start_time": "2019-12-03T00:31:25.327888Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/sean/anaconda3/envs/metis/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Users/sean/anaconda3/envs/metis/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "h0_sigmoid_activation (Dense (None, 1024)              1684480   \n",
      "_________________________________________________________________\n",
      "Dropout0 (Dropout)           (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "h1_sigmoid_activation (Dense (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "Dropout1 (Dropout)           (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "h2_sigmoid_activation (Dense (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "Dropout2 (Dropout)           (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 1025      \n",
      "=================================================================\n",
      "Total params: 3,784,705\n",
      "Trainable params: 3,784,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "drop_out=0.5    # DeepTox range: 0.5, 0.2, 0\n",
    "L2_reg = 0.0001 # Default = 0.01\n",
    "layers = 3      # DeepTox range: 1, 2, 3, 4\n",
    "act = 'sigmoid' # Consider sigmoid and tanh\n",
    "neurons = 1024  # DeepTox range: 1024, 2048, 4096, 8192, 16384\n",
    "# Info on decay: https://datascience.stackexchange.com/questions/26112/decay-parameter-in-keras-optimizers\n",
    "decay = 0       # DeepTox range: 10^-4, 10^-5, 10^-6\n",
    "learn_rate = 0.1  #Research appropriate range\n",
    "DNN = keras.Sequential()\n",
    "DNN.add(Dense(neurons, activation=act,input_shape=x_tr.shape[1:],name='h0_'+act+'_activation'))\n",
    "DNN.add(Dropout(rate=drop_out,name='Dropout0'))\n",
    "for i in range(1,layers):\n",
    "    DNN.add(Dense(units=neurons, activation=act,\\\n",
    "                  name='h'+str(i)+'_'+act+'_activation',\\\n",
    "                  kernel_regularizer=keras.regularizers.l2(L2_reg)))\n",
    "    DNN.add(Dropout(rate=drop_out,name='Dropout'+str(i)))\n",
    "DNN.add(Dense(units=1, activation='sigmoid'))\n",
    "keras.optimizers.Adam(lr=learn_rate, beta_1=0.9,\\\n",
    "                      beta_2=0.999, decay=decay, amsgrad=False)\n",
    "DNN.compile(optimizer='adam', loss='binary_crossentropy',\\\n",
    "            metrics=['accuracy'])\n",
    "DNN.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train DNN's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T00:45:37.293279Z",
     "start_time": "2019-12-03T00:31:53.696343Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/sean/anaconda3/envs/metis/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 11934 samples, validate on 1689 samples\n",
      "Epoch 1/100\n",
      "11934/11934 [==============================] - 3s 227us/step - loss: 1.1133 - acc: 0.5215 - val_loss: 0.7070 - val_acc: 0.8840\n",
      "Epoch 2/100\n",
      "11934/11934 [==============================] - 2s 175us/step - loss: 0.8746 - acc: 0.5747 - val_loss: 0.8319 - val_acc: 0.4938\n",
      "Epoch 3/100\n",
      "11934/11934 [==============================] - 2s 181us/step - loss: 0.7788 - acc: 0.6629 - val_loss: 0.7846 - val_acc: 0.5595\n",
      "Epoch 4/100\n",
      "11934/11934 [==============================] - 3s 210us/step - loss: 0.7400 - acc: 0.6865 - val_loss: 0.6602 - val_acc: 0.7105\n",
      "Epoch 5/100\n",
      "11934/11934 [==============================] - 3s 210us/step - loss: 0.7156 - acc: 0.6886 - val_loss: 0.5235 - val_acc: 0.8502\n",
      "Epoch 6/100\n",
      "11934/11934 [==============================] - 2s 200us/step - loss: 0.6984 - acc: 0.7056 - val_loss: 0.8266 - val_acc: 0.5317\n",
      "Epoch 7/100\n",
      "11934/11934 [==============================] - 2s 201us/step - loss: 0.6794 - acc: 0.7106 - val_loss: 0.5944 - val_acc: 0.7821\n",
      "Epoch 8/100\n",
      "11934/11934 [==============================] - 2s 201us/step - loss: 0.6682 - acc: 0.7123 - val_loss: 0.6837 - val_acc: 0.6092\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 9/100\n",
      "11934/11934 [==============================] - 2s 203us/step - loss: 0.6513 - acc: 0.7200 - val_loss: 0.7557 - val_acc: 0.5921\n",
      "Epoch 10/100\n",
      "11934/11934 [==============================] - 2s 205us/step - loss: 0.6357 - acc: 0.7319 - val_loss: 0.6479 - val_acc: 0.6750\n",
      "Epoch 11/100\n",
      "11934/11934 [==============================] - 2s 205us/step - loss: 0.6234 - acc: 0.7360 - val_loss: 0.5659 - val_acc: 0.8040\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 12/100\n",
      "11934/11934 [==============================] - 2s 205us/step - loss: 0.6146 - acc: 0.7387 - val_loss: 0.5735 - val_acc: 0.7916\n",
      "Epoch 13/100\n",
      "11934/11934 [==============================] - 2s 203us/step - loss: 0.6016 - acc: 0.7479 - val_loss: 0.6328 - val_acc: 0.6809\n",
      "Epoch 14/100\n",
      "11934/11934 [==============================] - 2s 206us/step - loss: 0.5965 - acc: 0.7479 - val_loss: 0.5107 - val_acc: 0.8378\n",
      "Epoch 15/100\n",
      "11934/11934 [==============================] - 3s 214us/step - loss: 0.5918 - acc: 0.7517 - val_loss: 0.5822 - val_acc: 0.7762\n",
      "Epoch 16/100\n",
      "11934/11934 [==============================] - 3s 210us/step - loss: 0.5871 - acc: 0.7556 - val_loss: 0.6022 - val_acc: 0.7010\n",
      "Epoch 17/100\n",
      "11934/11934 [==============================] - 2s 208us/step - loss: 0.5775 - acc: 0.7600 - val_loss: 0.5301 - val_acc: 0.8188\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 18/100\n",
      "11934/11934 [==============================] - 2s 207us/step - loss: 0.5706 - acc: 0.7588 - val_loss: 0.5502 - val_acc: 0.7833\n",
      "Epoch 19/100\n",
      "11934/11934 [==============================] - 3s 211us/step - loss: 0.5669 - acc: 0.7599 - val_loss: 0.5472 - val_acc: 0.7709\n",
      "Epoch 20/100\n",
      "11934/11934 [==============================] - 3s 212us/step - loss: 0.5605 - acc: 0.7637 - val_loss: 0.5162 - val_acc: 0.8265\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 21/100\n",
      "11934/11934 [==============================] - 2s 209us/step - loss: 0.5540 - acc: 0.7702 - val_loss: 0.5447 - val_acc: 0.7910\n",
      "Epoch 22/100\n",
      "11934/11934 [==============================] - 3s 215us/step - loss: 0.5547 - acc: 0.7665 - val_loss: 0.5377 - val_acc: 0.7845\n",
      "Epoch 23/100\n",
      "11934/11934 [==============================] - 3s 215us/step - loss: 0.5521 - acc: 0.7691 - val_loss: 0.5375 - val_acc: 0.7904\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 24/100\n",
      "11934/11934 [==============================] - 3s 210us/step - loss: 0.5515 - acc: 0.7733 - val_loss: 0.5496 - val_acc: 0.7815\n",
      "Epoch 25/100\n",
      "11934/11934 [==============================] - 3s 212us/step - loss: 0.5456 - acc: 0.7753 - val_loss: 0.5423 - val_acc: 0.7975\n",
      "Epoch 26/100\n",
      "11934/11934 [==============================] - 3s 210us/step - loss: 0.5473 - acc: 0.7756 - val_loss: 0.5340 - val_acc: 0.8011\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "Epoch 27/100\n",
      "11934/11934 [==============================] - 3s 214us/step - loss: 0.5476 - acc: 0.7745 - val_loss: 0.5473 - val_acc: 0.7869\n",
      "Epoch 28/100\n",
      "11934/11934 [==============================] - 3s 215us/step - loss: 0.5416 - acc: 0.7775 - val_loss: 0.5436 - val_acc: 0.7946\n",
      "Epoch 29/100\n",
      "11934/11934 [==============================] - 2s 208us/step - loss: 0.5424 - acc: 0.7764 - val_loss: 0.5403 - val_acc: 0.8005\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "Epoch 30/100\n",
      "11934/11934 [==============================] - 2s 205us/step - loss: 0.5402 - acc: 0.7745 - val_loss: 0.5377 - val_acc: 0.8034\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00030: early stopping\n",
      "         NR.AhR: 0.86472\n",
      "Model saved and metrics table updated.\n",
      "Model saved and metrics table updated.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>threshold</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>auc_roc</th>\n",
       "      <th>avg_precision</th>\n",
       "      <th>confusion_matrix</th>\n",
       "      <th>model_filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.911475</td>\n",
       "      <td>0.756757</td>\n",
       "      <td>0.383562</td>\n",
       "      <td>0.509091</td>\n",
       "      <td>0.905028</td>\n",
       "      <td>0.602102</td>\n",
       "      <td>[[528, 9], [45, 28]]</td>\n",
       "      <td>RF0.joblib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>RF_modT</td>\n",
       "      <td>0.235000</td>\n",
       "      <td>0.842623</td>\n",
       "      <td>0.424837</td>\n",
       "      <td>0.890411</td>\n",
       "      <td>0.575221</td>\n",
       "      <td>0.905028</td>\n",
       "      <td>0.602102</td>\n",
       "      <td>[[449, 88], [8, 65]]</td>\n",
       "      <td>RF_modT0.joblib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>DNN</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.814754</td>\n",
       "      <td>0.364865</td>\n",
       "      <td>0.739726</td>\n",
       "      <td>0.488688</td>\n",
       "      <td>0.864723</td>\n",
       "      <td>0.524576</td>\n",
       "      <td>[[443, 94], [19, 54]]</td>\n",
       "      <td>DNN0.h5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>DNN_modT</td>\n",
       "      <td>0.773516</td>\n",
       "      <td>0.901639</td>\n",
       "      <td>0.603175</td>\n",
       "      <td>0.520548</td>\n",
       "      <td>0.558824</td>\n",
       "      <td>0.864723</td>\n",
       "      <td>0.524576</td>\n",
       "      <td>[[512, 25], [35, 38]]</td>\n",
       "      <td>DNN_modT0.h5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      model  threshold  accuracy  precision    recall        f1   auc_roc  \\\n",
       "0        RF   0.500000  0.911475   0.756757  0.383562  0.509091  0.905028   \n",
       "1   RF_modT   0.235000  0.842623   0.424837  0.890411  0.575221  0.905028   \n",
       "2       DNN   0.500000  0.814754   0.364865  0.739726  0.488688  0.864723   \n",
       "3  DNN_modT   0.773516  0.901639   0.603175  0.520548  0.558824  0.864723   \n",
       "\n",
       "   avg_precision       confusion_matrix   model_filename  \n",
       "0       0.602102   [[528, 9], [45, 28]]       RF0.joblib  \n",
       "1       0.602102   [[449, 88], [8, 65]]  RF_modT0.joblib  \n",
       "2       0.524576  [[443, 94], [19, 54]]          DNN0.h5  \n",
       "3       0.524576  [[512, 25], [35, 38]]     DNN_modT0.h5  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14834 samples, validate on 1931 samples\n",
      "Epoch 1/100\n",
      "14834/14834 [==============================] - 3s 221us/step - loss: 1.0468 - acc: 0.4847 - val_loss: 0.9818 - val_acc: 0.4443\n",
      "Epoch 2/100\n",
      "14834/14834 [==============================] - 3s 221us/step - loss: 0.9139 - acc: 0.5450 - val_loss: 0.9631 - val_acc: 0.4459\n",
      "Epoch 3/100\n",
      "14834/14834 [==============================] - 3s 215us/step - loss: 0.8575 - acc: 0.5810 - val_loss: 0.8899 - val_acc: 0.4858\n",
      "Epoch 4/100\n",
      "14834/14834 [==============================] - 3s 217us/step - loss: 0.8338 - acc: 0.5866 - val_loss: 0.8453 - val_acc: 0.5049\n",
      "Epoch 5/100\n",
      "14834/14834 [==============================] - 3s 217us/step - loss: 0.8115 - acc: 0.6066 - val_loss: 0.8269 - val_acc: 0.5308\n",
      "Epoch 6/100\n",
      "14834/14834 [==============================] - 3s 209us/step - loss: 0.7909 - acc: 0.6201 - val_loss: 0.8087 - val_acc: 0.5526\n",
      "Epoch 7/100\n",
      "14834/14834 [==============================] - 3s 212us/step - loss: 0.7809 - acc: 0.6283 - val_loss: 0.7948 - val_acc: 0.5769\n",
      "Epoch 8/100\n",
      "14834/14834 [==============================] - 3s 207us/step - loss: 0.7699 - acc: 0.6393 - val_loss: 0.7865 - val_acc: 0.6100\n",
      "Epoch 9/100\n",
      "14834/14834 [==============================] - 3s 209us/step - loss: 0.7588 - acc: 0.6507 - val_loss: 0.7722 - val_acc: 0.6277\n",
      "Epoch 10/100\n",
      "14834/14834 [==============================] - 3s 212us/step - loss: 0.7521 - acc: 0.6536 - val_loss: 0.7701 - val_acc: 0.6282\n",
      "Epoch 11/100\n",
      "14834/14834 [==============================] - 3s 209us/step - loss: 0.7520 - acc: 0.6553 - val_loss: 0.7625 - val_acc: 0.6308\n",
      "Epoch 12/100\n",
      "14834/14834 [==============================] - 3s 207us/step - loss: 0.7426 - acc: 0.6642 - val_loss: 0.7514 - val_acc: 0.6349\n",
      "Epoch 13/100\n",
      "14834/14834 [==============================] - 3s 208us/step - loss: 0.7341 - acc: 0.6741 - val_loss: 0.7471 - val_acc: 0.6359\n",
      "Epoch 14/100\n",
      "14834/14834 [==============================] - 3s 211us/step - loss: 0.7322 - acc: 0.6757 - val_loss: 0.7426 - val_acc: 0.6401\n",
      "Epoch 15/100\n",
      "14834/14834 [==============================] - 3s 205us/step - loss: 0.7234 - acc: 0.6853 - val_loss: 0.7338 - val_acc: 0.6432\n",
      "Epoch 16/100\n",
      "14834/14834 [==============================] - 3s 203us/step - loss: 0.7211 - acc: 0.6891 - val_loss: 0.7311 - val_acc: 0.6432\n",
      "Epoch 17/100\n",
      "14834/14834 [==============================] - 3s 208us/step - loss: 0.7185 - acc: 0.6904 - val_loss: 0.7301 - val_acc: 0.6484\n",
      "Epoch 18/100\n",
      "14834/14834 [==============================] - 3s 204us/step - loss: 0.7134 - acc: 0.6927 - val_loss: 0.7207 - val_acc: 0.6530\n",
      "Epoch 19/100\n",
      "14834/14834 [==============================] - 3s 204us/step - loss: 0.7119 - acc: 0.6976 - val_loss: 0.7194 - val_acc: 0.6546\n",
      "Epoch 20/100\n",
      "14834/14834 [==============================] - 3s 205us/step - loss: 0.7103 - acc: 0.6972 - val_loss: 0.7152 - val_acc: 0.6587\n",
      "Epoch 21/100\n",
      "14834/14834 [==============================] - 3s 201us/step - loss: 0.7053 - acc: 0.7008 - val_loss: 0.7118 - val_acc: 0.6598\n",
      "Epoch 22/100\n",
      "14834/14834 [==============================] - 3s 219us/step - loss: 0.7015 - acc: 0.7063 - val_loss: 0.7116 - val_acc: 0.6608\n",
      "Epoch 23/100\n",
      "14834/14834 [==============================] - 3s 205us/step - loss: 0.7019 - acc: 0.7053 - val_loss: 0.7125 - val_acc: 0.6613\n",
      "Epoch 24/100\n",
      "14834/14834 [==============================] - 3s 212us/step - loss: 0.6991 - acc: 0.7087 - val_loss: 0.7069 - val_acc: 0.6639\n",
      "Epoch 25/100\n",
      "14834/14834 [==============================] - 3s 202us/step - loss: 0.6962 - acc: 0.7134 - val_loss: 0.7000 - val_acc: 0.6665\n",
      "Epoch 26/100\n",
      "14834/14834 [==============================] - 3s 204us/step - loss: 0.6957 - acc: 0.7093 - val_loss: 0.7000 - val_acc: 0.6655\n",
      "Epoch 27/100\n",
      "14834/14834 [==============================] - 3s 200us/step - loss: 0.6924 - acc: 0.7108 - val_loss: 0.6999 - val_acc: 0.6649\n",
      "Epoch 28/100\n",
      "14834/14834 [==============================] - 3s 207us/step - loss: 0.6893 - acc: 0.7152 - val_loss: 0.6927 - val_acc: 0.6696\n",
      "Epoch 29/100\n",
      "14834/14834 [==============================] - 3s 212us/step - loss: 0.6886 - acc: 0.7161 - val_loss: 0.6924 - val_acc: 0.6706\n",
      "Epoch 30/100\n",
      "14834/14834 [==============================] - 3s 203us/step - loss: 0.6873 - acc: 0.7136 - val_loss: 0.6918 - val_acc: 0.6712\n",
      "Epoch 31/100\n",
      "14834/14834 [==============================] - 3s 202us/step - loss: 0.6852 - acc: 0.7179 - val_loss: 0.6865 - val_acc: 0.6743\n",
      "Epoch 32/100\n",
      "14834/14834 [==============================] - 3s 201us/step - loss: 0.6816 - acc: 0.7201 - val_loss: 0.6897 - val_acc: 0.6727\n",
      "Epoch 33/100\n",
      "14834/14834 [==============================] - 3s 211us/step - loss: 0.6827 - acc: 0.7188 - val_loss: 0.6831 - val_acc: 0.6774\n",
      "Epoch 34/100\n",
      "14834/14834 [==============================] - 3s 218us/step - loss: 0.6815 - acc: 0.7245 - val_loss: 0.6882 - val_acc: 0.6753\n",
      "Epoch 35/100\n",
      "14834/14834 [==============================] - 3s 200us/step - loss: 0.6784 - acc: 0.7219 - val_loss: 0.6830 - val_acc: 0.6789\n",
      "Epoch 36/100\n",
      "14834/14834 [==============================] - 3s 205us/step - loss: 0.6777 - acc: 0.7226 - val_loss: 0.6808 - val_acc: 0.6800\n",
      "Epoch 37/100\n",
      "14834/14834 [==============================] - 3s 201us/step - loss: 0.6747 - acc: 0.7241 - val_loss: 0.6827 - val_acc: 0.6794\n",
      "Epoch 38/100\n",
      "14834/14834 [==============================] - 3s 202us/step - loss: 0.6711 - acc: 0.7267 - val_loss: 0.6767 - val_acc: 0.6815\n",
      "Epoch 39/100\n",
      "14834/14834 [==============================] - 3s 211us/step - loss: 0.6739 - acc: 0.7264 - val_loss: 0.6728 - val_acc: 0.6836\n",
      "Epoch 40/100\n",
      "14834/14834 [==============================] - 3s 218us/step - loss: 0.6725 - acc: 0.7226 - val_loss: 0.6704 - val_acc: 0.6846\n",
      "Epoch 41/100\n",
      "14834/14834 [==============================] - 3s 221us/step - loss: 0.6710 - acc: 0.7265 - val_loss: 0.6765 - val_acc: 0.6831\n",
      "Epoch 42/100\n",
      "14834/14834 [==============================] - 3s 209us/step - loss: 0.6683 - acc: 0.7299 - val_loss: 0.6699 - val_acc: 0.6882\n",
      "Epoch 43/100\n",
      "14834/14834 [==============================] - 3s 214us/step - loss: 0.6661 - acc: 0.7270 - val_loss: 0.6664 - val_acc: 0.6898\n",
      "Epoch 44/100\n",
      "14834/14834 [==============================] - 3s 200us/step - loss: 0.6609 - acc: 0.7363 - val_loss: 0.6657 - val_acc: 0.6919\n",
      "Epoch 45/100\n",
      "14834/14834 [==============================] - 3s 206us/step - loss: 0.6661 - acc: 0.7308 - val_loss: 0.6673 - val_acc: 0.6914\n",
      "Epoch 46/100\n",
      "14834/14834 [==============================] - 3s 205us/step - loss: 0.6634 - acc: 0.7304 - val_loss: 0.6627 - val_acc: 0.6929\n",
      "Epoch 47/100\n",
      "14834/14834 [==============================] - 3s 202us/step - loss: 0.6588 - acc: 0.7340 - val_loss: 0.6579 - val_acc: 0.6950\n",
      "Epoch 48/100\n",
      "14834/14834 [==============================] - 3s 207us/step - loss: 0.6595 - acc: 0.7340 - val_loss: 0.6645 - val_acc: 0.6929\n",
      "Epoch 49/100\n",
      "14834/14834 [==============================] - 3s 211us/step - loss: 0.6568 - acc: 0.7337 - val_loss: 0.6578 - val_acc: 0.6965\n",
      "Epoch 50/100\n",
      "14834/14834 [==============================] - 3s 207us/step - loss: 0.6545 - acc: 0.7350 - val_loss: 0.6500 - val_acc: 0.6996\n",
      "Epoch 51/100\n",
      "14834/14834 [==============================] - 3s 206us/step - loss: 0.6552 - acc: 0.7355 - val_loss: 0.6539 - val_acc: 0.6981\n",
      "Epoch 52/100\n",
      "14834/14834 [==============================] - 3s 206us/step - loss: 0.6546 - acc: 0.7355 - val_loss: 0.6441 - val_acc: 0.7033\n",
      "Epoch 53/100\n",
      "14834/14834 [==============================] - 3s 204us/step - loss: 0.6525 - acc: 0.7329 - val_loss: 0.6566 - val_acc: 0.6991\n",
      "Epoch 54/100\n",
      "14834/14834 [==============================] - 3s 201us/step - loss: 0.6486 - acc: 0.7361 - val_loss: 0.6474 - val_acc: 0.7027\n",
      "Epoch 55/100\n",
      "14834/14834 [==============================] - 3s 213us/step - loss: 0.6481 - acc: 0.7407 - val_loss: 0.6474 - val_acc: 0.7043\n",
      "\n",
      "Epoch 00055: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "Epoch 56/100\n",
      "14834/14834 [==============================] - 3s 202us/step - loss: 0.6482 - acc: 0.7383 - val_loss: 0.6472 - val_acc: 0.7038\n",
      "Epoch 57/100\n",
      "14834/14834 [==============================] - 3s 204us/step - loss: 0.6509 - acc: 0.7384 - val_loss: 0.6416 - val_acc: 0.7074\n",
      "Epoch 58/100\n",
      "14834/14834 [==============================] - 3s 202us/step - loss: 0.6473 - acc: 0.7371 - val_loss: 0.6424 - val_acc: 0.7090\n",
      "Epoch 59/100\n",
      "14834/14834 [==============================] - 3s 220us/step - loss: 0.6479 - acc: 0.7403 - val_loss: 0.6392 - val_acc: 0.7095\n",
      "Epoch 60/100\n",
      "14834/14834 [==============================] - 3s 203us/step - loss: 0.6470 - acc: 0.7397 - val_loss: 0.6413 - val_acc: 0.7084\n",
      "Epoch 61/100\n",
      "14834/14834 [==============================] - 3s 203us/step - loss: 0.6461 - acc: 0.7375 - val_loss: 0.6384 - val_acc: 0.7100\n",
      "Epoch 62/100\n",
      "14834/14834 [==============================] - 3s 205us/step - loss: 0.6441 - acc: 0.7381 - val_loss: 0.6373 - val_acc: 0.7100\n",
      "Epoch 63/100\n",
      "14834/14834 [==============================] - 3s 214us/step - loss: 0.6458 - acc: 0.7405 - val_loss: 0.6407 - val_acc: 0.7090\n",
      "Epoch 64/100\n",
      "14834/14834 [==============================] - 3s 209us/step - loss: 0.6437 - acc: 0.7392 - val_loss: 0.6404 - val_acc: 0.7084\n",
      "Epoch 65/100\n",
      "14834/14834 [==============================] - 3s 207us/step - loss: 0.6416 - acc: 0.7410 - val_loss: 0.6358 - val_acc: 0.7141\n",
      "Epoch 66/100\n",
      "14834/14834 [==============================] - 3s 208us/step - loss: 0.6403 - acc: 0.7414 - val_loss: 0.6347 - val_acc: 0.7147\n",
      "Epoch 67/100\n",
      "14834/14834 [==============================] - 3s 203us/step - loss: 0.6412 - acc: 0.7417 - val_loss: 0.6335 - val_acc: 0.7147\n",
      "Epoch 68/100\n",
      "14834/14834 [==============================] - 3s 203us/step - loss: 0.6412 - acc: 0.7392 - val_loss: 0.6373 - val_acc: 0.7136\n",
      "Epoch 69/100\n",
      "14834/14834 [==============================] - 3s 201us/step - loss: 0.6404 - acc: 0.7396 - val_loss: 0.6307 - val_acc: 0.7157\n",
      "Epoch 70/100\n",
      "14834/14834 [==============================] - 3s 206us/step - loss: 0.6370 - acc: 0.7425 - val_loss: 0.6339 - val_acc: 0.7141\n",
      "Epoch 71/100\n",
      "14834/14834 [==============================] - 3s 204us/step - loss: 0.6425 - acc: 0.7370 - val_loss: 0.6329 - val_acc: 0.7141\n",
      "Epoch 72/100\n",
      "14834/14834 [==============================] - 3s 207us/step - loss: 0.6358 - acc: 0.7437 - val_loss: 0.6319 - val_acc: 0.7162\n",
      "\n",
      "Epoch 00072: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "Epoch 73/100\n",
      "14834/14834 [==============================] - 3s 200us/step - loss: 0.6385 - acc: 0.7396 - val_loss: 0.6294 - val_acc: 0.7172\n",
      "Epoch 74/100\n",
      "14834/14834 [==============================] - 3s 204us/step - loss: 0.6360 - acc: 0.7432 - val_loss: 0.6295 - val_acc: 0.7172\n",
      "Epoch 75/100\n",
      "14834/14834 [==============================] - 3s 202us/step - loss: 0.6371 - acc: 0.7459 - val_loss: 0.6288 - val_acc: 0.7178\n",
      "Epoch 76/100\n",
      "14834/14834 [==============================] - 3s 210us/step - loss: 0.6363 - acc: 0.7440 - val_loss: 0.6299 - val_acc: 0.7178\n",
      "Epoch 77/100\n",
      "14834/14834 [==============================] - 3s 204us/step - loss: 0.6368 - acc: 0.7387 - val_loss: 0.6263 - val_acc: 0.7188\n",
      "Epoch 78/100\n",
      "14834/14834 [==============================] - 3s 205us/step - loss: 0.6379 - acc: 0.7449 - val_loss: 0.6283 - val_acc: 0.7183\n",
      "Epoch 79/100\n",
      "14834/14834 [==============================] - 3s 207us/step - loss: 0.6361 - acc: 0.7454 - val_loss: 0.6271 - val_acc: 0.7193\n",
      "Epoch 80/100\n",
      "14834/14834 [==============================] - 3s 207us/step - loss: 0.6364 - acc: 0.7444 - val_loss: 0.6230 - val_acc: 0.7214\n",
      "Epoch 81/100\n",
      "14834/14834 [==============================] - 3s 201us/step - loss: 0.6374 - acc: 0.7439 - val_loss: 0.6251 - val_acc: 0.7204\n",
      "Epoch 82/100\n",
      "14834/14834 [==============================] - 3s 205us/step - loss: 0.6359 - acc: 0.7435 - val_loss: 0.6254 - val_acc: 0.7198\n",
      "Epoch 83/100\n",
      "14834/14834 [==============================] - 3s 203us/step - loss: 0.6355 - acc: 0.7449 - val_loss: 0.6255 - val_acc: 0.7204\n",
      "\n",
      "Epoch 00083: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
      "Epoch 84/100\n",
      "14834/14834 [==============================] - 3s 204us/step - loss: 0.6337 - acc: 0.7447 - val_loss: 0.6267 - val_acc: 0.7193\n",
      "Epoch 85/100\n",
      "14834/14834 [==============================] - 3s 205us/step - loss: 0.6345 - acc: 0.7437 - val_loss: 0.6248 - val_acc: 0.7209\n",
      "Epoch 86/100\n",
      "14834/14834 [==============================] - 3s 206us/step - loss: 0.6341 - acc: 0.7436 - val_loss: 0.6255 - val_acc: 0.7204\n",
      "\n",
      "Epoch 00086: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
      "Epoch 87/100\n",
      "14834/14834 [==============================] - 3s 202us/step - loss: 0.6353 - acc: 0.7441 - val_loss: 0.6252 - val_acc: 0.7209\n",
      "Epoch 88/100\n",
      "14834/14834 [==============================] - 3s 208us/step - loss: 0.6361 - acc: 0.7421 - val_loss: 0.6250 - val_acc: 0.7214\n",
      "Epoch 89/100\n",
      "14834/14834 [==============================] - 3s 208us/step - loss: 0.6325 - acc: 0.7479 - val_loss: 0.6248 - val_acc: 0.7214\n",
      "\n",
      "Epoch 00089: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
      "Epoch 90/100\n",
      "14834/14834 [==============================] - 3s 204us/step - loss: 0.6316 - acc: 0.7439 - val_loss: 0.6247 - val_acc: 0.7214\n",
      "Epoch 91/100\n",
      "14834/14834 [==============================] - 3s 203us/step - loss: 0.6362 - acc: 0.7445 - val_loss: 0.6245 - val_acc: 0.7219\n",
      "Epoch 92/100\n",
      "14834/14834 [==============================] - 3s 210us/step - loss: 0.6372 - acc: 0.7421 - val_loss: 0.6245 - val_acc: 0.7219\n",
      "\n",
      "Epoch 00092: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.\n",
      "Epoch 93/100\n",
      "14834/14834 [==============================] - 3s 205us/step - loss: 0.6326 - acc: 0.7457 - val_loss: 0.6245 - val_acc: 0.7214\n",
      "Epoch 94/100\n",
      "14834/14834 [==============================] - 3s 207us/step - loss: 0.6328 - acc: 0.7454 - val_loss: 0.6247 - val_acc: 0.7214\n",
      "Epoch 95/100\n",
      "14834/14834 [==============================] - 3s 208us/step - loss: 0.6340 - acc: 0.7419 - val_loss: 0.6246 - val_acc: 0.7214\n",
      "\n",
      "Epoch 00095: ReduceLROnPlateau reducing learning rate to 6.103515914901436e-08.\n",
      "Epoch 96/100\n",
      "14834/14834 [==============================] - 3s 207us/step - loss: 0.6356 - acc: 0.7445 - val_loss: 0.6246 - val_acc: 0.7219\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00096: early stopping\n",
      "          NR.AR: 0.74151\n",
      "Model saved and metrics table updated.\n",
      "Model saved and metrics table updated.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>threshold</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>auc_roc</th>\n",
       "      <th>avg_precision</th>\n",
       "      <th>confusion_matrix</th>\n",
       "      <th>model_filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.981229</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.678934</td>\n",
       "      <td>0.232723</td>\n",
       "      <td>[[573, 1], [10, 2]]</td>\n",
       "      <td>RF0.joblib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>RF_modT</td>\n",
       "      <td>0.730000</td>\n",
       "      <td>0.982935</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.678934</td>\n",
       "      <td>0.232723</td>\n",
       "      <td>[[574, 0], [10, 2]]</td>\n",
       "      <td>RF_modT0.joblib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>DNN</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.721843</td>\n",
       "      <td>0.042424</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.079096</td>\n",
       "      <td>0.741507</td>\n",
       "      <td>0.168663</td>\n",
       "      <td>[[416, 158], [5, 7]]</td>\n",
       "      <td>DNN0.h5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>DNN_modT</td>\n",
       "      <td>0.792447</td>\n",
       "      <td>0.965870</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.741507</td>\n",
       "      <td>0.168663</td>\n",
       "      <td>[[562, 12], [8, 4]]</td>\n",
       "      <td>DNN_modT0.h5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      model  threshold  accuracy  precision    recall        f1   auc_roc  \\\n",
       "0        RF   0.500000  0.981229   0.666667  0.166667  0.266667  0.678934   \n",
       "1   RF_modT   0.730000  0.982935   1.000000  0.166667  0.285714  0.678934   \n",
       "2       DNN   0.500000  0.721843   0.042424  0.583333  0.079096  0.741507   \n",
       "3  DNN_modT   0.792447  0.965870   0.250000  0.333333  0.285714  0.741507   \n",
       "\n",
       "   avg_precision      confusion_matrix   model_filename  \n",
       "0       0.232723   [[573, 1], [10, 2]]       RF0.joblib  \n",
       "1       0.232723   [[574, 0], [10, 2]]  RF_modT0.joblib  \n",
       "2       0.168663  [[416, 158], [5, 7]]          DNN0.h5  \n",
       "3       0.168663   [[562, 12], [8, 4]]     DNN_modT0.h5  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13670 samples, validate on 1771 samples\n",
      "Epoch 1/100\n",
      "13670/13670 [==============================] - 3s 207us/step - loss: 0.6926 - acc: 0.7134 - val_loss: 0.5940 - val_acc: 0.7493\n",
      "Epoch 2/100\n",
      "13670/13670 [==============================] - 3s 214us/step - loss: 0.6916 - acc: 0.7143 - val_loss: 0.5947 - val_acc: 0.7493\n",
      "Epoch 3/100\n",
      "13670/13670 [==============================] - 3s 212us/step - loss: 0.6977 - acc: 0.7115 - val_loss: 0.5955 - val_acc: 0.7487\n",
      "Epoch 4/100\n",
      "13670/13670 [==============================] - 3s 211us/step - loss: 0.6946 - acc: 0.7108 - val_loss: 0.5961 - val_acc: 0.7487\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 3.051757957450718e-08.\n",
      "Epoch 5/100\n",
      "13670/13670 [==============================] - 3s 210us/step - loss: 0.6950 - acc: 0.7102 - val_loss: 0.5964 - val_acc: 0.7487\n",
      "Epoch 6/100\n",
      "13670/13670 [==============================] - 3s 212us/step - loss: 0.6970 - acc: 0.7123 - val_loss: 0.5967 - val_acc: 0.7487\n",
      "Epoch 7/100\n",
      "13670/13670 [==============================] - 3s 208us/step - loss: 0.6924 - acc: 0.7127 - val_loss: 0.5971 - val_acc: 0.7487\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 1.525878978725359e-08.\n",
      "Epoch 8/100\n",
      "13670/13670 [==============================] - 3s 214us/step - loss: 0.6927 - acc: 0.7119 - val_loss: 0.5972 - val_acc: 0.7487\n",
      "Epoch 9/100\n",
      "13670/13670 [==============================] - 3s 212us/step - loss: 0.6910 - acc: 0.7113 - val_loss: 0.5974 - val_acc: 0.7487\n",
      "Epoch 10/100\n",
      "13670/13670 [==============================] - 3s 212us/step - loss: 0.6926 - acc: 0.7102 - val_loss: 0.5976 - val_acc: 0.7487\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 7.629394893626795e-09.\n",
      "Epoch 11/100\n",
      "13670/13670 [==============================] - 3s 209us/step - loss: 0.6916 - acc: 0.7091 - val_loss: 0.5976 - val_acc: 0.7487\n",
      "Epoch 12/100\n",
      "13670/13670 [==============================] - 3s 208us/step - loss: 0.6926 - acc: 0.7125 - val_loss: 0.5977 - val_acc: 0.7487\n",
      "Epoch 13/100\n",
      "13670/13670 [==============================] - 3s 204us/step - loss: 0.6938 - acc: 0.7119 - val_loss: 0.5978 - val_acc: 0.7487\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 3.814697446813398e-09.\n",
      "Epoch 14/100\n",
      "13670/13670 [==============================] - 3s 206us/step - loss: 0.6929 - acc: 0.7125 - val_loss: 0.5978 - val_acc: 0.7487\n",
      "Epoch 15/100\n",
      "13670/13670 [==============================] - 3s 212us/step - loss: 0.6896 - acc: 0.7121 - val_loss: 0.5979 - val_acc: 0.7487\n",
      "Epoch 16/100\n",
      "13670/13670 [==============================] - 3s 209us/step - loss: 0.6905 - acc: 0.7138 - val_loss: 0.5979 - val_acc: 0.7487\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.907348723406699e-09.\n",
      "Epoch 17/100\n",
      "13670/13670 [==============================] - 3s 207us/step - loss: 0.6933 - acc: 0.7110 - val_loss: 0.5979 - val_acc: 0.7487\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00017: early stopping\n",
      "      NR.AR.LBD: 0.63513\n",
      "Model saved and metrics table updated.\n",
      "Model saved and metrics table updated.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>threshold</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>auc_roc</th>\n",
       "      <th>avg_precision</th>\n",
       "      <th>confusion_matrix</th>\n",
       "      <th>model_filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.982818</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.763284</td>\n",
       "      <td>0.060344</td>\n",
       "      <td>[[572, 2], [8, 0]]</td>\n",
       "      <td>RF0.joblib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>RF_modT</td>\n",
       "      <td>0.270000</td>\n",
       "      <td>0.979381</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.763284</td>\n",
       "      <td>0.060344</td>\n",
       "      <td>[[569, 5], [7, 1]]</td>\n",
       "      <td>RF_modT0.joblib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>DNN</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.738832</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.037975</td>\n",
       "      <td>0.635126</td>\n",
       "      <td>0.030153</td>\n",
       "      <td>[[427, 147], [5, 3]]</td>\n",
       "      <td>DNN0.h5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>DNN_modT</td>\n",
       "      <td>0.762549</td>\n",
       "      <td>0.929553</td>\n",
       "      <td>0.054054</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.088889</td>\n",
       "      <td>0.635126</td>\n",
       "      <td>0.030153</td>\n",
       "      <td>[[539, 35], [6, 2]]</td>\n",
       "      <td>DNN_modT0.h5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      model  threshold  accuracy  precision  recall        f1   auc_roc  \\\n",
       "0        RF   0.500000  0.982818   0.000000   0.000  0.000000  0.763284   \n",
       "1   RF_modT   0.270000  0.979381   0.166667   0.125  0.142857  0.763284   \n",
       "2       DNN   0.500000  0.738832   0.020000   0.375  0.037975  0.635126   \n",
       "3  DNN_modT   0.762549  0.929553   0.054054   0.250  0.088889  0.635126   \n",
       "\n",
       "   avg_precision      confusion_matrix   model_filename  \n",
       "0       0.060344    [[572, 2], [8, 0]]       RF0.joblib  \n",
       "1       0.060344    [[569, 5], [7, 1]]  RF_modT0.joblib  \n",
       "2       0.030153  [[427, 147], [5, 3]]          DNN0.h5  \n",
       "3       0.030153   [[539, 35], [6, 2]]     DNN_modT0.h5  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11300 samples, validate on 1488 samples\n",
      "Epoch 1/100\n",
      "11300/11300 [==============================] - 2s 201us/step - loss: 0.7350 - acc: 0.6379 - val_loss: 0.5618 - val_acc: 0.7708\n",
      "Epoch 2/100\n",
      "11300/11300 [==============================] - 2s 209us/step - loss: 0.7377 - acc: 0.6377 - val_loss: 0.5619 - val_acc: 0.7708\n",
      "Epoch 3/100\n",
      "11300/11300 [==============================] - 2s 214us/step - loss: 0.7319 - acc: 0.6388 - val_loss: 0.5619 - val_acc: 0.7708\n",
      "Epoch 4/100\n",
      "11300/11300 [==============================] - 2s 210us/step - loss: 0.7355 - acc: 0.6370 - val_loss: 0.5620 - val_acc: 0.7708\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 9.536743617033494e-10.\n",
      "Epoch 5/100\n",
      "11300/11300 [==============================] - 2s 213us/step - loss: 0.7381 - acc: 0.6356 - val_loss: 0.5620 - val_acc: 0.7708\n",
      "Epoch 6/100\n",
      "11300/11300 [==============================] - 2s 214us/step - loss: 0.7360 - acc: 0.6382 - val_loss: 0.5620 - val_acc: 0.7708\n",
      "Epoch 7/100\n",
      "11300/11300 [==============================] - 2s 215us/step - loss: 0.7324 - acc: 0.6381 - val_loss: 0.5621 - val_acc: 0.7702\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 4.768371808516747e-10.\n",
      "Epoch 8/100\n",
      "11300/11300 [==============================] - 2s 215us/step - loss: 0.7318 - acc: 0.6371 - val_loss: 0.5621 - val_acc: 0.7702\n",
      "Epoch 9/100\n",
      "11300/11300 [==============================] - 2s 215us/step - loss: 0.7365 - acc: 0.6379 - val_loss: 0.5621 - val_acc: 0.7702\n",
      "Epoch 10/100\n",
      "11300/11300 [==============================] - 2s 212us/step - loss: 0.7357 - acc: 0.6411 - val_loss: 0.5621 - val_acc: 0.7702\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 2.3841859042583735e-10.\n",
      "Epoch 11/100\n",
      "11300/11300 [==============================] - 3s 224us/step - loss: 0.7339 - acc: 0.6389 - val_loss: 0.5621 - val_acc: 0.7702\n",
      "Epoch 12/100\n",
      "11300/11300 [==============================] - 2s 219us/step - loss: 0.7361 - acc: 0.6339 - val_loss: 0.5621 - val_acc: 0.7702\n",
      "Epoch 13/100\n",
      "11300/11300 [==============================] - 2s 215us/step - loss: 0.7371 - acc: 0.6361 - val_loss: 0.5621 - val_acc: 0.7702\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 1.1920929521291868e-10.\n",
      "Epoch 14/100\n",
      "11300/11300 [==============================] - 2s 214us/step - loss: 0.7354 - acc: 0.6356 - val_loss: 0.5621 - val_acc: 0.7702\n",
      "Epoch 15/100\n",
      "11300/11300 [==============================] - 2s 214us/step - loss: 0.7362 - acc: 0.6336 - val_loss: 0.5621 - val_acc: 0.7702\n",
      "Epoch 16/100\n",
      "11300/11300 [==============================] - 2s 204us/step - loss: 0.7342 - acc: 0.6396 - val_loss: 0.5621 - val_acc: 0.7702\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 5.960464760645934e-11.\n",
      "Epoch 17/100\n",
      "11300/11300 [==============================] - 2s 210us/step - loss: 0.7347 - acc: 0.6350 - val_loss: 0.5621 - val_acc: 0.7702\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00017: early stopping\n",
      "   NR.Aromatase: 0.71816\n",
      "Model saved and metrics table updated.\n",
      "Model saved and metrics table updated.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>threshold</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>auc_roc</th>\n",
       "      <th>avg_precision</th>\n",
       "      <th>confusion_matrix</th>\n",
       "      <th>model_filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.928030</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.771931</td>\n",
       "      <td>0.311884</td>\n",
       "      <td>[[489, 0], [38, 1]]</td>\n",
       "      <td>RF0.joblib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>RF_modT</td>\n",
       "      <td>0.130000</td>\n",
       "      <td>0.840909</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.512821</td>\n",
       "      <td>0.322581</td>\n",
       "      <td>0.771931</td>\n",
       "      <td>0.311884</td>\n",
       "      <td>[[424, 65], [19, 20]]</td>\n",
       "      <td>RF_modT0.joblib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>DNN</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.772727</td>\n",
       "      <td>0.159664</td>\n",
       "      <td>0.487179</td>\n",
       "      <td>0.240506</td>\n",
       "      <td>0.718158</td>\n",
       "      <td>0.143086</td>\n",
       "      <td>[[389, 100], [20, 19]]</td>\n",
       "      <td>DNN0.h5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>DNN_modT</td>\n",
       "      <td>0.475931</td>\n",
       "      <td>0.767045</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.254545</td>\n",
       "      <td>0.718158</td>\n",
       "      <td>0.143086</td>\n",
       "      <td>[[384, 105], [18, 21]]</td>\n",
       "      <td>DNN_modT0.h5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      model  threshold  accuracy  precision    recall        f1   auc_roc  \\\n",
       "0        RF   0.500000  0.928030   1.000000  0.025641  0.050000  0.771931   \n",
       "1   RF_modT   0.130000  0.840909   0.235294  0.512821  0.322581  0.771931   \n",
       "2       DNN   0.500000  0.772727   0.159664  0.487179  0.240506  0.718158   \n",
       "3  DNN_modT   0.475931  0.767045   0.166667  0.538462  0.254545  0.718158   \n",
       "\n",
       "   avg_precision        confusion_matrix   model_filename  \n",
       "0       0.311884     [[489, 0], [38, 1]]       RF0.joblib  \n",
       "1       0.311884   [[424, 65], [19, 20]]  RF_modT0.joblib  \n",
       "2       0.143086  [[389, 100], [20, 19]]          DNN0.h5  \n",
       "3       0.143086  [[384, 105], [18, 21]]     DNN_modT0.h5  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11196 samples, validate on 1593 samples\n",
      "Epoch 1/100\n",
      "11196/11196 [==============================] - 2s 200us/step - loss: 0.7824 - acc: 0.6451 - val_loss: 0.5880 - val_acc: 0.7589\n",
      "Epoch 2/100\n",
      "11196/11196 [==============================] - 2s 217us/step - loss: 0.7853 - acc: 0.6436 - val_loss: 0.5880 - val_acc: 0.7589\n",
      "Epoch 3/100\n",
      "11196/11196 [==============================] - 2s 214us/step - loss: 0.7875 - acc: 0.6437 - val_loss: 0.5880 - val_acc: 0.7589\n",
      "Epoch 4/100\n",
      "11196/11196 [==============================] - 2s 209us/step - loss: 0.7822 - acc: 0.6476 - val_loss: 0.5880 - val_acc: 0.7589\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 2.980232380322967e-11.\n",
      "Epoch 5/100\n",
      "11196/11196 [==============================] - 2s 208us/step - loss: 0.7862 - acc: 0.6417 - val_loss: 0.5880 - val_acc: 0.7589\n",
      "Epoch 6/100\n",
      "11196/11196 [==============================] - 2s 212us/step - loss: 0.7820 - acc: 0.6442 - val_loss: 0.5880 - val_acc: 0.7589\n",
      "Epoch 7/100\n",
      "11196/11196 [==============================] - 2s 214us/step - loss: 0.7882 - acc: 0.6391 - val_loss: 0.5880 - val_acc: 0.7589\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 1.4901161901614834e-11.\n",
      "Epoch 8/100\n",
      "11196/11196 [==============================] - 2s 211us/step - loss: 0.7848 - acc: 0.6426 - val_loss: 0.5880 - val_acc: 0.7589\n",
      "Epoch 9/100\n",
      "11196/11196 [==============================] - 2s 208us/step - loss: 0.7884 - acc: 0.6458 - val_loss: 0.5880 - val_acc: 0.7589\n",
      "Epoch 10/100\n",
      "11196/11196 [==============================] - 2s 209us/step - loss: 0.7862 - acc: 0.6430 - val_loss: 0.5880 - val_acc: 0.7589\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 7.450580950807417e-12.\n",
      "Epoch 11/100\n",
      "11196/11196 [==============================] - 2s 215us/step - loss: 0.7821 - acc: 0.6445 - val_loss: 0.5880 - val_acc: 0.7589\n",
      "Epoch 12/100\n",
      "11196/11196 [==============================] - 2s 209us/step - loss: 0.7886 - acc: 0.6399 - val_loss: 0.5880 - val_acc: 0.7589\n",
      "Epoch 13/100\n",
      "11196/11196 [==============================] - 2s 209us/step - loss: 0.7858 - acc: 0.6439 - val_loss: 0.5880 - val_acc: 0.7589\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 3.725290475403709e-12.\n",
      "Epoch 14/100\n",
      "11196/11196 [==============================] - 2s 209us/step - loss: 0.7855 - acc: 0.6454 - val_loss: 0.5880 - val_acc: 0.7589\n",
      "Epoch 15/100\n",
      "11196/11196 [==============================] - 2s 203us/step - loss: 0.7872 - acc: 0.6462 - val_loss: 0.5880 - val_acc: 0.7589\n",
      "Epoch 16/100\n",
      "11196/11196 [==============================] - 2s 208us/step - loss: 0.7828 - acc: 0.6470 - val_loss: 0.5880 - val_acc: 0.7589\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.8626452377018543e-12.\n",
      "Epoch 17/100\n",
      "11196/11196 [==============================] - 2s 199us/step - loss: 0.7822 - acc: 0.6457 - val_loss: 0.5880 - val_acc: 0.7589\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00017: early stopping\n",
      "          NR.ER: 0.73637\n",
      "Model saved and metrics table updated.\n",
      "Model saved and metrics table updated.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>threshold</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>auc_roc</th>\n",
       "      <th>avg_precision</th>\n",
       "      <th>confusion_matrix</th>\n",
       "      <th>model_filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.215686</td>\n",
       "      <td>0.338462</td>\n",
       "      <td>0.769007</td>\n",
       "      <td>0.445565</td>\n",
       "      <td>[[462, 3], [40, 11]]</td>\n",
       "      <td>RF0.joblib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>RF_modT</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.912791</td>\n",
       "      <td>0.607143</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.430380</td>\n",
       "      <td>0.769007</td>\n",
       "      <td>0.445565</td>\n",
       "      <td>[[454, 11], [34, 17]]</td>\n",
       "      <td>RF_modT0.joblib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>DNN</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.748062</td>\n",
       "      <td>0.203008</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.293478</td>\n",
       "      <td>0.736369</td>\n",
       "      <td>0.280064</td>\n",
       "      <td>[[359, 106], [24, 27]]</td>\n",
       "      <td>DNN0.h5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>DNN_modT</td>\n",
       "      <td>0.625295</td>\n",
       "      <td>0.815891</td>\n",
       "      <td>0.270833</td>\n",
       "      <td>0.509804</td>\n",
       "      <td>0.353741</td>\n",
       "      <td>0.736369</td>\n",
       "      <td>0.280064</td>\n",
       "      <td>[[395, 70], [25, 26]]</td>\n",
       "      <td>DNN_modT0.h5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      model  threshold  accuracy  precision    recall        f1   auc_roc  \\\n",
       "0        RF   0.500000  0.916667   0.785714  0.215686  0.338462  0.769007   \n",
       "1   RF_modT   0.400000  0.912791   0.607143  0.333333  0.430380  0.769007   \n",
       "2       DNN   0.500000  0.748062   0.203008  0.529412  0.293478  0.736369   \n",
       "3  DNN_modT   0.625295  0.815891   0.270833  0.509804  0.353741  0.736369   \n",
       "\n",
       "   avg_precision        confusion_matrix   model_filename  \n",
       "0       0.445565    [[462, 3], [40, 11]]       RF0.joblib  \n",
       "1       0.445565   [[454, 11], [34, 17]]  RF_modT0.joblib  \n",
       "2       0.280064  [[359, 106], [24, 27]]          DNN0.h5  \n",
       "3       0.280064   [[395, 70], [25, 26]]     DNN_modT0.h5  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13734 samples, validate on 1808 samples\n",
      "Epoch 1/100\n",
      "13734/13734 [==============================] - 3s 200us/step - loss: 0.7634 - acc: 0.6589 - val_loss: 0.6040 - val_acc: 0.7406\n",
      "Epoch 2/100\n",
      "13734/13734 [==============================] - 3s 211us/step - loss: 0.7638 - acc: 0.6589 - val_loss: 0.6040 - val_acc: 0.7406\n",
      "Epoch 3/100\n",
      "13734/13734 [==============================] - 3s 211us/step - loss: 0.7642 - acc: 0.6591 - val_loss: 0.6040 - val_acc: 0.7406\n",
      "Epoch 4/100\n",
      "13734/13734 [==============================] - 3s 208us/step - loss: 0.7649 - acc: 0.6557 - val_loss: 0.6040 - val_acc: 0.7406\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 9.313226188509272e-13.\n",
      "Epoch 5/100\n",
      "13734/13734 [==============================] - 3s 212us/step - loss: 0.7620 - acc: 0.6594 - val_loss: 0.6040 - val_acc: 0.7406\n",
      "Epoch 6/100\n",
      "13734/13734 [==============================] - 3s 210us/step - loss: 0.7610 - acc: 0.6589 - val_loss: 0.6040 - val_acc: 0.7406\n",
      "Epoch 7/100\n",
      "13734/13734 [==============================] - 3s 209us/step - loss: 0.7661 - acc: 0.6593 - val_loss: 0.6040 - val_acc: 0.7406\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 4.656613094254636e-13.\n",
      "Epoch 8/100\n",
      "13734/13734 [==============================] - 3s 213us/step - loss: 0.7624 - acc: 0.6576 - val_loss: 0.6040 - val_acc: 0.7406\n",
      "Epoch 9/100\n",
      "13734/13734 [==============================] - 3s 211us/step - loss: 0.7625 - acc: 0.6600 - val_loss: 0.6040 - val_acc: 0.7406\n",
      "Epoch 10/100\n",
      "13734/13734 [==============================] - 3s 209us/step - loss: 0.7645 - acc: 0.6574 - val_loss: 0.6040 - val_acc: 0.7406\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 2.328306547127318e-13.\n",
      "Epoch 11/100\n",
      "13734/13734 [==============================] - 3s 212us/step - loss: 0.7647 - acc: 0.6549 - val_loss: 0.6040 - val_acc: 0.7406\n",
      "Epoch 12/100\n",
      "13734/13734 [==============================] - 3s 205us/step - loss: 0.7610 - acc: 0.6553 - val_loss: 0.6040 - val_acc: 0.7406\n",
      "Epoch 13/100\n",
      "13734/13734 [==============================] - 3s 206us/step - loss: 0.7646 - acc: 0.6568 - val_loss: 0.6040 - val_acc: 0.7406\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 1.164153273563659e-13.\n",
      "Epoch 14/100\n",
      "13734/13734 [==============================] - 3s 201us/step - loss: 0.7617 - acc: 0.6571 - val_loss: 0.6040 - val_acc: 0.7406\n",
      "Epoch 15/100\n",
      "13734/13734 [==============================] - 3s 203us/step - loss: 0.7621 - acc: 0.6591 - val_loss: 0.6040 - val_acc: 0.7406\n",
      "Epoch 16/100\n",
      "13734/13734 [==============================] - 3s 207us/step - loss: 0.7641 - acc: 0.6596 - val_loss: 0.6040 - val_acc: 0.7406\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 5.820766367818295e-14.\n",
      "Epoch 17/100\n",
      "13734/13734 [==============================] - 3s 200us/step - loss: 0.7643 - acc: 0.6552 - val_loss: 0.6040 - val_acc: 0.7406\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00017: early stopping\n",
      "      NR.ER.LBD: 0.67832\n",
      "Model saved and metrics table updated.\n",
      "Model saved and metrics table updated.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>threshold</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>auc_roc</th>\n",
       "      <th>avg_precision</th>\n",
       "      <th>confusion_matrix</th>\n",
       "      <th>model_filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.50000</td>\n",
       "      <td>0.971667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.260870</td>\n",
       "      <td>0.750345</td>\n",
       "      <td>0.268778</td>\n",
       "      <td>[[580, 0], [17, 3]]</td>\n",
       "      <td>RF0.joblib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>RF_modT</td>\n",
       "      <td>0.30500</td>\n",
       "      <td>0.965000</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.750345</td>\n",
       "      <td>0.268778</td>\n",
       "      <td>[[573, 7], [14, 6]]</td>\n",
       "      <td>RF_modT0.joblib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>DNN</td>\n",
       "      <td>0.50000</td>\n",
       "      <td>0.725000</td>\n",
       "      <td>0.049689</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.088398</td>\n",
       "      <td>0.678319</td>\n",
       "      <td>0.069591</td>\n",
       "      <td>[[427, 153], [12, 8]]</td>\n",
       "      <td>DNN0.h5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>DNN_modT</td>\n",
       "      <td>0.74828</td>\n",
       "      <td>0.915000</td>\n",
       "      <td>0.102564</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.135593</td>\n",
       "      <td>0.678319</td>\n",
       "      <td>0.069591</td>\n",
       "      <td>[[545, 35], [16, 4]]</td>\n",
       "      <td>DNN_modT0.h5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      model  threshold  accuracy  precision  recall        f1   auc_roc  \\\n",
       "0        RF    0.50000  0.971667   1.000000    0.15  0.260870  0.750345   \n",
       "1   RF_modT    0.30500  0.965000   0.461538    0.30  0.363636  0.750345   \n",
       "2       DNN    0.50000  0.725000   0.049689    0.40  0.088398  0.678319   \n",
       "3  DNN_modT    0.74828  0.915000   0.102564    0.20  0.135593  0.678319   \n",
       "\n",
       "   avg_precision       confusion_matrix   model_filename  \n",
       "0       0.268778    [[580, 0], [17, 3]]       RF0.joblib  \n",
       "1       0.268778    [[573, 7], [14, 6]]  RF_modT0.joblib  \n",
       "2       0.069591  [[427, 153], [12, 8]]          DNN0.h5  \n",
       "3       0.069591   [[545, 35], [16, 4]]     DNN_modT0.h5  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13140 samples, validate on 1691 samples\n",
      "Epoch 1/100\n",
      "13140/13140 [==============================] - 3s 201us/step - loss: 0.8473 - acc: 0.6414 - val_loss: 0.5922 - val_acc: 0.7463\n",
      "Epoch 2/100\n",
      "13140/13140 [==============================] - 3s 213us/step - loss: 0.8494 - acc: 0.6419 - val_loss: 0.5922 - val_acc: 0.7463\n",
      "Epoch 3/100\n",
      "13140/13140 [==============================] - 3s 213us/step - loss: 0.8480 - acc: 0.6416 - val_loss: 0.5922 - val_acc: 0.7463\n",
      "Epoch 4/100\n",
      "13140/13140 [==============================] - 3s 211us/step - loss: 0.8501 - acc: 0.6409 - val_loss: 0.5922 - val_acc: 0.7463\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 2.9103831839091474e-14.\n",
      "Epoch 5/100\n",
      "13140/13140 [==============================] - 3s 210us/step - loss: 0.8500 - acc: 0.6400 - val_loss: 0.5922 - val_acc: 0.7463\n",
      "Epoch 6/100\n",
      "13140/13140 [==============================] - 3s 211us/step - loss: 0.8496 - acc: 0.6407 - val_loss: 0.5922 - val_acc: 0.7463\n",
      "Epoch 7/100\n",
      "13140/13140 [==============================] - 3s 207us/step - loss: 0.8504 - acc: 0.6414 - val_loss: 0.5922 - val_acc: 0.7463\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 1.4551915919545737e-14.\n",
      "Epoch 8/100\n",
      "13140/13140 [==============================] - 3s 211us/step - loss: 0.8466 - acc: 0.6377 - val_loss: 0.5922 - val_acc: 0.7463\n",
      "Epoch 9/100\n",
      "13140/13140 [==============================] - 3s 212us/step - loss: 0.8507 - acc: 0.6404 - val_loss: 0.5922 - val_acc: 0.7463\n",
      "Epoch 10/100\n",
      "13140/13140 [==============================] - 3s 210us/step - loss: 0.8489 - acc: 0.6435 - val_loss: 0.5922 - val_acc: 0.7463\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 7.275957959772868e-15.\n",
      "Epoch 11/100\n",
      "13140/13140 [==============================] - 3s 208us/step - loss: 0.8507 - acc: 0.6406 - val_loss: 0.5922 - val_acc: 0.7463\n",
      "Epoch 12/100\n",
      "13140/13140 [==============================] - 3s 209us/step - loss: 0.8512 - acc: 0.6407 - val_loss: 0.5922 - val_acc: 0.7463\n",
      "Epoch 13/100\n",
      "13140/13140 [==============================] - 3s 214us/step - loss: 0.8524 - acc: 0.6384 - val_loss: 0.5922 - val_acc: 0.7463\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 3.637978979886434e-15.\n",
      "Epoch 14/100\n",
      "13140/13140 [==============================] - 3s 200us/step - loss: 0.8479 - acc: 0.6425 - val_loss: 0.5922 - val_acc: 0.7463\n",
      "Epoch 15/100\n",
      "13140/13140 [==============================] - 3s 202us/step - loss: 0.8506 - acc: 0.6368 - val_loss: 0.5922 - val_acc: 0.7463\n",
      "Epoch 16/100\n",
      "13140/13140 [==============================] - 3s 203us/step - loss: 0.8475 - acc: 0.6420 - val_loss: 0.5922 - val_acc: 0.7463\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.818989489943217e-15.\n",
      "Epoch 17/100\n",
      "13140/13140 [==============================] - 3s 203us/step - loss: 0.8496 - acc: 0.6376 - val_loss: 0.5922 - val_acc: 0.7463\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00017: early stopping\n",
      "  NR.PPAR.gamma: 0.74039\n",
      "Model saved and metrics table updated.\n",
      "Model saved and metrics table updated.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>threshold</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>auc_roc</th>\n",
       "      <th>avg_precision</th>\n",
       "      <th>confusion_matrix</th>\n",
       "      <th>model_filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.947107</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.709846</td>\n",
       "      <td>0.129989</td>\n",
       "      <td>[[573, 1], [31, 0]]</td>\n",
       "      <td>RF0.joblib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>RF_modT</td>\n",
       "      <td>0.130000</td>\n",
       "      <td>0.890909</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.322581</td>\n",
       "      <td>0.232558</td>\n",
       "      <td>0.709846</td>\n",
       "      <td>0.129989</td>\n",
       "      <td>[[529, 45], [21, 10]]</td>\n",
       "      <td>RF_modT0.joblib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>DNN</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.750413</td>\n",
       "      <td>0.115385</td>\n",
       "      <td>0.580645</td>\n",
       "      <td>0.192513</td>\n",
       "      <td>0.740390</td>\n",
       "      <td>0.109550</td>\n",
       "      <td>[[436, 138], [13, 18]]</td>\n",
       "      <td>DNN0.h5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>DNN_modT</td>\n",
       "      <td>0.577705</td>\n",
       "      <td>0.798347</td>\n",
       "      <td>0.130081</td>\n",
       "      <td>0.516129</td>\n",
       "      <td>0.207792</td>\n",
       "      <td>0.740390</td>\n",
       "      <td>0.109550</td>\n",
       "      <td>[[467, 107], [15, 16]]</td>\n",
       "      <td>DNN_modT0.h5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      model  threshold  accuracy  precision    recall        f1   auc_roc  \\\n",
       "0        RF   0.500000  0.947107   0.000000  0.000000  0.000000  0.709846   \n",
       "1   RF_modT   0.130000  0.890909   0.181818  0.322581  0.232558  0.709846   \n",
       "2       DNN   0.500000  0.750413   0.115385  0.580645  0.192513  0.740390   \n",
       "3  DNN_modT   0.577705  0.798347   0.130081  0.516129  0.207792  0.740390   \n",
       "\n",
       "   avg_precision        confusion_matrix   model_filename  \n",
       "0       0.129989     [[573, 1], [31, 0]]       RF0.joblib  \n",
       "1       0.129989   [[529, 45], [21, 10]]  RF_modT0.joblib  \n",
       "2       0.109550  [[436, 138], [13, 18]]          DNN0.h5  \n",
       "3       0.109550  [[467, 107], [15, 16]]     DNN_modT0.h5  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10006 samples, validate on 1481 samples\n",
      "Epoch 1/100\n",
      "10006/10006 [==============================] - 2s 198us/step - loss: 0.8277 - acc: 0.6072 - val_loss: 0.6143 - val_acc: 0.7387\n",
      "Epoch 2/100\n",
      "10006/10006 [==============================] - 2s 209us/step - loss: 0.8290 - acc: 0.6072 - val_loss: 0.6143 - val_acc: 0.7387\n",
      "Epoch 3/100\n",
      "10006/10006 [==============================] - 2s 214us/step - loss: 0.8288 - acc: 0.6097 - val_loss: 0.6143 - val_acc: 0.7387\n",
      "Epoch 4/100\n",
      "10006/10006 [==============================] - 2s 217us/step - loss: 0.8261 - acc: 0.6060 - val_loss: 0.6143 - val_acc: 0.7387\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 9.094947449716085e-16.\n",
      "Epoch 5/100\n",
      "10006/10006 [==============================] - 2s 211us/step - loss: 0.8253 - acc: 0.6076 - val_loss: 0.6143 - val_acc: 0.7387\n",
      "Epoch 6/100\n",
      "10006/10006 [==============================] - 2s 213us/step - loss: 0.8243 - acc: 0.6088 - val_loss: 0.6143 - val_acc: 0.7387\n",
      "Epoch 7/100\n",
      "10006/10006 [==============================] - 2s 213us/step - loss: 0.8269 - acc: 0.6092 - val_loss: 0.6143 - val_acc: 0.7387\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 4.547473724858043e-16.\n",
      "Epoch 8/100\n",
      "10006/10006 [==============================] - 2s 211us/step - loss: 0.8283 - acc: 0.6095 - val_loss: 0.6143 - val_acc: 0.7387\n",
      "Epoch 9/100\n",
      "10006/10006 [==============================] - 2s 218us/step - loss: 0.8291 - acc: 0.6070 - val_loss: 0.6143 - val_acc: 0.7387\n",
      "Epoch 10/100\n",
      "10006/10006 [==============================] - 2s 212us/step - loss: 0.8243 - acc: 0.6079 - val_loss: 0.6143 - val_acc: 0.7387\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 2.2737368624290214e-16.\n",
      "Epoch 11/100\n",
      "10006/10006 [==============================] - 2s 208us/step - loss: 0.8267 - acc: 0.6096 - val_loss: 0.6143 - val_acc: 0.7387\n",
      "Epoch 12/100\n",
      "10006/10006 [==============================] - 2s 218us/step - loss: 0.8263 - acc: 0.6059 - val_loss: 0.6143 - val_acc: 0.7387\n",
      "Epoch 13/100\n",
      "10006/10006 [==============================] - 2s 211us/step - loss: 0.8271 - acc: 0.6047 - val_loss: 0.6143 - val_acc: 0.7387\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 1.1368684312145107e-16.\n",
      "Epoch 14/100\n",
      "10006/10006 [==============================] - 2s 214us/step - loss: 0.8230 - acc: 0.6084 - val_loss: 0.6143 - val_acc: 0.7387\n",
      "Epoch 15/100\n",
      "10006/10006 [==============================] - 2s 208us/step - loss: 0.8242 - acc: 0.6107 - val_loss: 0.6143 - val_acc: 0.7387\n",
      "Epoch 16/100\n",
      "10006/10006 [==============================] - 2s 212us/step - loss: 0.8264 - acc: 0.6108 - val_loss: 0.6143 - val_acc: 0.7387\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 5.684342156072553e-17.\n",
      "Epoch 17/100\n",
      "10006/10006 [==============================] - 2s 214us/step - loss: 0.8261 - acc: 0.6097 - val_loss: 0.6143 - val_acc: 0.7387\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00017: early stopping\n",
      "         SR.ARE: 0.70550\n",
      "Model saved and metrics table updated.\n",
      "Model saved and metrics table updated.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>threshold</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>auc_roc</th>\n",
       "      <th>avg_precision</th>\n",
       "      <th>confusion_matrix</th>\n",
       "      <th>model_filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.841441</td>\n",
       "      <td>0.592593</td>\n",
       "      <td>0.172043</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.780838</td>\n",
       "      <td>0.430711</td>\n",
       "      <td>[[451, 11], [77, 16]]</td>\n",
       "      <td>RF0.joblib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>RF_modT</td>\n",
       "      <td>0.340000</td>\n",
       "      <td>0.823423</td>\n",
       "      <td>0.476190</td>\n",
       "      <td>0.537634</td>\n",
       "      <td>0.505051</td>\n",
       "      <td>0.780838</td>\n",
       "      <td>0.430711</td>\n",
       "      <td>[[407, 55], [43, 50]]</td>\n",
       "      <td>RF_modT0.joblib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>DNN</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.762162</td>\n",
       "      <td>0.355556</td>\n",
       "      <td>0.516129</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>0.705500</td>\n",
       "      <td>0.347718</td>\n",
       "      <td>[[375, 87], [45, 48]]</td>\n",
       "      <td>DNN0.h5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>DNN_modT</td>\n",
       "      <td>0.457539</td>\n",
       "      <td>0.753153</td>\n",
       "      <td>0.353333</td>\n",
       "      <td>0.569892</td>\n",
       "      <td>0.436214</td>\n",
       "      <td>0.705500</td>\n",
       "      <td>0.347718</td>\n",
       "      <td>[[365, 97], [40, 53]]</td>\n",
       "      <td>DNN_modT0.h5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      model  threshold  accuracy  precision    recall        f1   auc_roc  \\\n",
       "0        RF   0.500000  0.841441   0.592593  0.172043  0.266667  0.780838   \n",
       "1   RF_modT   0.340000  0.823423   0.476190  0.537634  0.505051  0.780838   \n",
       "2       DNN   0.500000  0.762162   0.355556  0.516129  0.421053  0.705500   \n",
       "3  DNN_modT   0.457539  0.753153   0.353333  0.569892  0.436214  0.705500   \n",
       "\n",
       "   avg_precision       confusion_matrix   model_filename  \n",
       "0       0.430711  [[451, 11], [77, 16]]       RF0.joblib  \n",
       "1       0.430711  [[407, 55], [43, 50]]  RF_modT0.joblib  \n",
       "2       0.347718  [[375, 87], [45, 48]]          DNN0.h5  \n",
       "3       0.347718  [[365, 97], [40, 53]]     DNN_modT0.h5  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14400 samples, validate on 1873 samples\n",
      "Epoch 1/100\n",
      "14400/14400 [==============================] - 3s 207us/step - loss: 0.7856 - acc: 0.6460 - val_loss: 0.6163 - val_acc: 0.7277\n",
      "Epoch 2/100\n",
      "14400/14400 [==============================] - 3s 216us/step - loss: 0.7849 - acc: 0.6498 - val_loss: 0.6163 - val_acc: 0.7277\n",
      "Epoch 3/100\n",
      "14400/14400 [==============================] - 3s 213us/step - loss: 0.7839 - acc: 0.6488 - val_loss: 0.6163 - val_acc: 0.7277\n",
      "Epoch 4/100\n",
      "14400/14400 [==============================] - 3s 215us/step - loss: 0.7871 - acc: 0.6468 - val_loss: 0.6163 - val_acc: 0.7277\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 2.842171078036277e-17.\n",
      "Epoch 5/100\n",
      "14400/14400 [==============================] - 3s 210us/step - loss: 0.7841 - acc: 0.6508 - val_loss: 0.6163 - val_acc: 0.7277\n",
      "Epoch 6/100\n",
      "14400/14400 [==============================] - 3s 213us/step - loss: 0.7849 - acc: 0.6462 - val_loss: 0.6163 - val_acc: 0.7277\n",
      "Epoch 7/100\n",
      "14400/14400 [==============================] - 3s 212us/step - loss: 0.7881 - acc: 0.6475 - val_loss: 0.6163 - val_acc: 0.7277\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 1.4210855390181384e-17.\n",
      "Epoch 8/100\n",
      "14400/14400 [==============================] - 3s 212us/step - loss: 0.7859 - acc: 0.6503 - val_loss: 0.6163 - val_acc: 0.7277\n",
      "Epoch 9/100\n",
      "14400/14400 [==============================] - 3s 213us/step - loss: 0.7861 - acc: 0.6492 - val_loss: 0.6163 - val_acc: 0.7277\n",
      "Epoch 10/100\n",
      "14400/14400 [==============================] - 3s 212us/step - loss: 0.7865 - acc: 0.6467 - val_loss: 0.6163 - val_acc: 0.7277\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 7.105427695090692e-18.\n",
      "Epoch 11/100\n",
      "14400/14400 [==============================] - 3s 210us/step - loss: 0.7842 - acc: 0.6473 - val_loss: 0.6163 - val_acc: 0.7277\n",
      "Epoch 12/100\n",
      "14400/14400 [==============================] - 3s 214us/step - loss: 0.7872 - acc: 0.6451 - val_loss: 0.6163 - val_acc: 0.7277\n",
      "Epoch 13/100\n",
      "14400/14400 [==============================] - 3s 210us/step - loss: 0.7902 - acc: 0.6468 - val_loss: 0.6163 - val_acc: 0.7277\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 3.552713847545346e-18.\n",
      "Epoch 14/100\n",
      "14400/14400 [==============================] - 3s 214us/step - loss: 0.7862 - acc: 0.6485 - val_loss: 0.6163 - val_acc: 0.7277\n",
      "Epoch 15/100\n",
      "14400/14400 [==============================] - 3s 206us/step - loss: 0.7876 - acc: 0.6485 - val_loss: 0.6163 - val_acc: 0.7277\n",
      "Epoch 16/100\n",
      "14400/14400 [==============================] - 3s 201us/step - loss: 0.7894 - acc: 0.6469 - val_loss: 0.6163 - val_acc: 0.7277\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.776356923772673e-18.\n",
      "Epoch 17/100\n",
      "14400/14400 [==============================] - 3s 214us/step - loss: 0.7857 - acc: 0.6475 - val_loss: 0.6163 - val_acc: 0.7277\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00017: early stopping\n",
      "       SR.ATAD5: 0.71161\n",
      "Model saved and metrics table updated.\n",
      "Model saved and metrics table updated.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>threshold</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>auc_roc</th>\n",
       "      <th>avg_precision</th>\n",
       "      <th>confusion_matrix</th>\n",
       "      <th>model_filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.938907</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.795625</td>\n",
       "      <td>0.427828</td>\n",
       "      <td>[[584, 0], [38, 0]]</td>\n",
       "      <td>RF0.joblib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>RF_modT</td>\n",
       "      <td>0.255000</td>\n",
       "      <td>0.948553</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.795625</td>\n",
       "      <td>0.427828</td>\n",
       "      <td>[[576, 8], [24, 14]]</td>\n",
       "      <td>RF_modT0.joblib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>DNN</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.736334</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.552632</td>\n",
       "      <td>0.203883</td>\n",
       "      <td>0.711608</td>\n",
       "      <td>0.153415</td>\n",
       "      <td>[[437, 147], [17, 21]]</td>\n",
       "      <td>DNN0.h5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>DNN_modT</td>\n",
       "      <td>0.577705</td>\n",
       "      <td>0.789389</td>\n",
       "      <td>0.155556</td>\n",
       "      <td>0.552632</td>\n",
       "      <td>0.242775</td>\n",
       "      <td>0.711608</td>\n",
       "      <td>0.153415</td>\n",
       "      <td>[[470, 114], [17, 21]]</td>\n",
       "      <td>DNN_modT0.h5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      model  threshold  accuracy  precision    recall        f1   auc_roc  \\\n",
       "0        RF   0.500000  0.938907   0.000000  0.000000  0.000000  0.795625   \n",
       "1   RF_modT   0.255000  0.948553   0.636364  0.368421  0.466667  0.795625   \n",
       "2       DNN   0.500000  0.736334   0.125000  0.552632  0.203883  0.711608   \n",
       "3  DNN_modT   0.577705  0.789389   0.155556  0.552632  0.242775  0.711608   \n",
       "\n",
       "   avg_precision        confusion_matrix   model_filename  \n",
       "0       0.427828     [[584, 0], [38, 0]]       RF0.joblib  \n",
       "1       0.427828    [[576, 8], [24, 14]]  RF_modT0.joblib  \n",
       "2       0.153415  [[437, 147], [17, 21]]          DNN0.h5  \n",
       "3       0.153415  [[470, 114], [17, 21]]     DNN_modT0.h5  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12766 samples, validate on 1684 samples\n",
      "Epoch 1/100\n",
      "12766/12766 [==============================] - 3s 201us/step - loss: 0.8939 - acc: 0.5726 - val_loss: 0.6040 - val_acc: 0.7387\n",
      "Epoch 2/100\n",
      "12766/12766 [==============================] - 3s 216us/step - loss: 0.8961 - acc: 0.5725 - val_loss: 0.6040 - val_acc: 0.7387\n",
      "Epoch 3/100\n",
      "12766/12766 [==============================] - 3s 213us/step - loss: 0.8974 - acc: 0.5715 - val_loss: 0.6040 - val_acc: 0.7387\n",
      "Epoch 4/100\n",
      "12766/12766 [==============================] - 3s 213us/step - loss: 0.8990 - acc: 0.5734 - val_loss: 0.6040 - val_acc: 0.7387\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 8.881784618863365e-19.\n",
      "Epoch 5/100\n",
      "12766/12766 [==============================] - 3s 210us/step - loss: 0.8931 - acc: 0.5728 - val_loss: 0.6040 - val_acc: 0.7387\n",
      "Epoch 6/100\n",
      "12766/12766 [==============================] - 3s 209us/step - loss: 0.8964 - acc: 0.5714 - val_loss: 0.6040 - val_acc: 0.7387\n",
      "Epoch 7/100\n",
      "12766/12766 [==============================] - 3s 225us/step - loss: 0.8986 - acc: 0.5725 - val_loss: 0.6040 - val_acc: 0.7387\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 4.440892309431682e-19.\n",
      "Epoch 8/100\n",
      "12766/12766 [==============================] - 3s 214us/step - loss: 0.8973 - acc: 0.5717 - val_loss: 0.6040 - val_acc: 0.7387\n",
      "Epoch 9/100\n",
      "12766/12766 [==============================] - 3s 213us/step - loss: 0.8969 - acc: 0.5734 - val_loss: 0.6040 - val_acc: 0.7387\n",
      "Epoch 10/100\n",
      "12766/12766 [==============================] - 3s 207us/step - loss: 0.8970 - acc: 0.5706 - val_loss: 0.6040 - val_acc: 0.7387\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 2.220446154715841e-19.\n",
      "Epoch 11/100\n",
      "12766/12766 [==============================] - 3s 210us/step - loss: 0.8962 - acc: 0.5743 - val_loss: 0.6040 - val_acc: 0.7387\n",
      "Epoch 12/100\n",
      "12766/12766 [==============================] - 3s 204us/step - loss: 0.8947 - acc: 0.5739 - val_loss: 0.6040 - val_acc: 0.7387\n",
      "Epoch 13/100\n",
      "12766/12766 [==============================] - 3s 214us/step - loss: 0.8954 - acc: 0.5743 - val_loss: 0.6040 - val_acc: 0.7387\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 1.1102230773579206e-19.\n",
      "Epoch 14/100\n",
      "12766/12766 [==============================] - 3s 211us/step - loss: 0.8960 - acc: 0.5734 - val_loss: 0.6040 - val_acc: 0.7387\n",
      "Epoch 15/100\n",
      "12766/12766 [==============================] - 3s 202us/step - loss: 0.9010 - acc: 0.5701 - val_loss: 0.6040 - val_acc: 0.7387\n",
      "Epoch 16/100\n",
      "12766/12766 [==============================] - 3s 202us/step - loss: 0.8929 - acc: 0.5774 - val_loss: 0.6040 - val_acc: 0.7387\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 5.551115386789603e-20.\n",
      "Epoch 17/100\n",
      "12766/12766 [==============================] - 3s 204us/step - loss: 0.8962 - acc: 0.5713 - val_loss: 0.6040 - val_acc: 0.7387\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00017: early stopping\n",
      "         SR.HSE: 0.62674\n",
      "Model saved and metrics table updated.\n",
      "Model saved and metrics table updated.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>threshold</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>auc_roc</th>\n",
       "      <th>avg_precision</th>\n",
       "      <th>confusion_matrix</th>\n",
       "      <th>model_filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.965574</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.759586</td>\n",
       "      <td>0.239563</td>\n",
       "      <td>[[586, 2], [19, 3]]</td>\n",
       "      <td>RF0.joblib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>RF_modT</td>\n",
       "      <td>0.340000</td>\n",
       "      <td>0.965574</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.759586</td>\n",
       "      <td>0.239563</td>\n",
       "      <td>[[583, 5], [16, 6]]</td>\n",
       "      <td>RF_modT0.joblib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>DNN</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.726230</td>\n",
       "      <td>0.055215</td>\n",
       "      <td>0.409091</td>\n",
       "      <td>0.097297</td>\n",
       "      <td>0.626739</td>\n",
       "      <td>0.054621</td>\n",
       "      <td>[[434, 154], [13, 9]]</td>\n",
       "      <td>DNN0.h5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>DNN_modT</td>\n",
       "      <td>0.450345</td>\n",
       "      <td>0.708197</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.118812</td>\n",
       "      <td>0.626739</td>\n",
       "      <td>0.054621</td>\n",
       "      <td>[[420, 168], [10, 12]]</td>\n",
       "      <td>DNN_modT0.h5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      model  threshold  accuracy  precision    recall        f1   auc_roc  \\\n",
       "0        RF   0.500000  0.965574   0.600000  0.136364  0.222222  0.759586   \n",
       "1   RF_modT   0.340000  0.965574   0.545455  0.272727  0.363636  0.759586   \n",
       "2       DNN   0.500000  0.726230   0.055215  0.409091  0.097297  0.626739   \n",
       "3  DNN_modT   0.450345  0.708197   0.066667  0.545455  0.118812  0.626739   \n",
       "\n",
       "   avg_precision        confusion_matrix   model_filename  \n",
       "0       0.239563     [[586, 2], [19, 3]]       RF0.joblib  \n",
       "1       0.239563     [[583, 5], [16, 6]]  RF_modT0.joblib  \n",
       "2       0.054621   [[434, 154], [13, 9]]          DNN0.h5  \n",
       "3       0.054621  [[420, 168], [10, 12]]     DNN_modT0.h5  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10204 samples, validate on 1512 samples\n",
      "Epoch 1/100\n",
      "10204/10204 [==============================] - 2s 201us/step - loss: 0.7619 - acc: 0.6424 - val_loss: 0.6140 - val_acc: 0.7421\n",
      "Epoch 2/100\n",
      "10204/10204 [==============================] - 2s 218us/step - loss: 0.7654 - acc: 0.6400 - val_loss: 0.6140 - val_acc: 0.7421\n",
      "Epoch 3/100\n",
      "10204/10204 [==============================] - 2s 215us/step - loss: 0.7604 - acc: 0.6392 - val_loss: 0.6140 - val_acc: 0.7421\n",
      "Epoch 4/100\n",
      "10204/10204 [==============================] - 2s 219us/step - loss: 0.7587 - acc: 0.6449 - val_loss: 0.6140 - val_acc: 0.7421\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 2.7755576933948015e-20.\n",
      "Epoch 5/100\n",
      "10204/10204 [==============================] - 2s 214us/step - loss: 0.7617 - acc: 0.6446 - val_loss: 0.6140 - val_acc: 0.7421\n",
      "Epoch 6/100\n",
      "10204/10204 [==============================] - 2s 213us/step - loss: 0.7611 - acc: 0.6400 - val_loss: 0.6140 - val_acc: 0.7421\n",
      "Epoch 7/100\n",
      "10204/10204 [==============================] - 2s 212us/step - loss: 0.7633 - acc: 0.6404 - val_loss: 0.6140 - val_acc: 0.7421\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 1.3877788466974007e-20.\n",
      "Epoch 8/100\n",
      "10204/10204 [==============================] - 2s 216us/step - loss: 0.7594 - acc: 0.6465 - val_loss: 0.6140 - val_acc: 0.7421\n",
      "Epoch 9/100\n",
      "10204/10204 [==============================] - 2s 216us/step - loss: 0.7610 - acc: 0.6450 - val_loss: 0.6140 - val_acc: 0.7421\n",
      "Epoch 10/100\n",
      "10204/10204 [==============================] - 2s 215us/step - loss: 0.7598 - acc: 0.6421 - val_loss: 0.6140 - val_acc: 0.7421\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 6.938894233487004e-21.\n",
      "Epoch 11/100\n",
      "10204/10204 [==============================] - 2s 221us/step - loss: 0.7635 - acc: 0.6415 - val_loss: 0.6140 - val_acc: 0.7421\n",
      "Epoch 12/100\n",
      "10204/10204 [==============================] - 2s 213us/step - loss: 0.7603 - acc: 0.6439 - val_loss: 0.6140 - val_acc: 0.7421\n",
      "Epoch 13/100\n",
      "10204/10204 [==============================] - 2s 216us/step - loss: 0.7582 - acc: 0.6419 - val_loss: 0.6140 - val_acc: 0.7421\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 3.469447116743502e-21.\n",
      "Epoch 14/100\n",
      "10204/10204 [==============================] - 2s 216us/step - loss: 0.7601 - acc: 0.6458 - val_loss: 0.6140 - val_acc: 0.7421\n",
      "Epoch 15/100\n",
      "10204/10204 [==============================] - 2s 215us/step - loss: 0.7570 - acc: 0.6437 - val_loss: 0.6140 - val_acc: 0.7421\n",
      "Epoch 16/100\n",
      "10204/10204 [==============================] - 2s 213us/step - loss: 0.7629 - acc: 0.6403 - val_loss: 0.6140 - val_acc: 0.7421\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.734723558371751e-21.\n",
      "Epoch 17/100\n",
      "10204/10204 [==============================] - 2s 209us/step - loss: 0.7598 - acc: 0.6424 - val_loss: 0.6140 - val_acc: 0.7421\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00017: early stopping\n",
      "         SR.MMP: 0.80388\n",
      "Model saved and metrics table updated.\n",
      "Model saved and metrics table updated.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>threshold</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>auc_roc</th>\n",
       "      <th>avg_precision</th>\n",
       "      <th>confusion_matrix</th>\n",
       "      <th>model_filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.906077</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.484848</td>\n",
       "      <td>0.930331</td>\n",
       "      <td>0.560184</td>\n",
       "      <td>[[468, 15], [36, 24]]</td>\n",
       "      <td>RF0.joblib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>RF_modT</td>\n",
       "      <td>0.382500</td>\n",
       "      <td>0.913444</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.624000</td>\n",
       "      <td>0.930331</td>\n",
       "      <td>0.560184</td>\n",
       "      <td>[[457, 26], [21, 39]]</td>\n",
       "      <td>RF_modT0.joblib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>DNN</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.815838</td>\n",
       "      <td>0.330508</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.438202</td>\n",
       "      <td>0.803882</td>\n",
       "      <td>0.296565</td>\n",
       "      <td>[[404, 79], [21, 39]]</td>\n",
       "      <td>DNN0.h5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>DNN_modT</td>\n",
       "      <td>0.505391</td>\n",
       "      <td>0.817680</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.440678</td>\n",
       "      <td>0.803882</td>\n",
       "      <td>0.296565</td>\n",
       "      <td>[[405, 78], [21, 39]]</td>\n",
       "      <td>DNN_modT0.h5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      model  threshold  accuracy  precision  recall        f1   auc_roc  \\\n",
       "0        RF   0.500000  0.906077   0.615385    0.40  0.484848  0.930331   \n",
       "1   RF_modT   0.382500  0.913444   0.600000    0.65  0.624000  0.930331   \n",
       "2       DNN   0.500000  0.815838   0.330508    0.65  0.438202  0.803882   \n",
       "3  DNN_modT   0.505391  0.817680   0.333333    0.65  0.440678  0.803882   \n",
       "\n",
       "   avg_precision       confusion_matrix   model_filename  \n",
       "0       0.560184  [[468, 15], [36, 24]]       RF0.joblib  \n",
       "1       0.560184  [[457, 26], [21, 39]]  RF_modT0.joblib  \n",
       "2       0.296565  [[404, 79], [21, 39]]          DNN0.h5  \n",
       "3       0.296565  [[405, 78], [21, 39]]     DNN_modT0.h5  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13340 samples, validate on 1781 samples\n",
      "Epoch 1/100\n",
      "13340/13340 [==============================] - 3s 206us/step - loss: 0.7926 - acc: 0.6395 - val_loss: 0.6058 - val_acc: 0.7344\n",
      "Epoch 2/100\n",
      "13340/13340 [==============================] - 3s 218us/step - loss: 0.7896 - acc: 0.6397 - val_loss: 0.6058 - val_acc: 0.7344\n",
      "Epoch 3/100\n",
      "13340/13340 [==============================] - 3s 215us/step - loss: 0.7886 - acc: 0.6389 - val_loss: 0.6058 - val_acc: 0.7344\n",
      "Epoch 4/100\n",
      "13340/13340 [==============================] - 3s 214us/step - loss: 0.7935 - acc: 0.6389 - val_loss: 0.6058 - val_acc: 0.7344\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 8.673617791858755e-22.\n",
      "Epoch 5/100\n",
      "13340/13340 [==============================] - 3s 213us/step - loss: 0.7917 - acc: 0.6403 - val_loss: 0.6058 - val_acc: 0.7344\n",
      "Epoch 6/100\n",
      "13340/13340 [==============================] - 3s 212us/step - loss: 0.7906 - acc: 0.6421 - val_loss: 0.6058 - val_acc: 0.7344\n",
      "Epoch 7/100\n",
      "13340/13340 [==============================] - 3s 215us/step - loss: 0.7909 - acc: 0.6407 - val_loss: 0.6058 - val_acc: 0.7344\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 4.336808895929377e-22.\n",
      "Epoch 8/100\n",
      "13340/13340 [==============================] - 3s 219us/step - loss: 0.7873 - acc: 0.6408 - val_loss: 0.6058 - val_acc: 0.7344\n",
      "Epoch 9/100\n",
      "13340/13340 [==============================] - 3s 224us/step - loss: 0.7938 - acc: 0.6364 - val_loss: 0.6058 - val_acc: 0.7344\n",
      "Epoch 10/100\n",
      "13340/13340 [==============================] - 3s 225us/step - loss: 0.7931 - acc: 0.6383 - val_loss: 0.6058 - val_acc: 0.7344\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 2.1684044479646887e-22.\n",
      "Epoch 11/100\n",
      "13340/13340 [==============================] - 3s 221us/step - loss: 0.7897 - acc: 0.6435 - val_loss: 0.6058 - val_acc: 0.7344\n",
      "Epoch 12/100\n",
      "13340/13340 [==============================] - 3s 211us/step - loss: 0.7910 - acc: 0.6357 - val_loss: 0.6058 - val_acc: 0.7344\n",
      "Epoch 13/100\n",
      "13340/13340 [==============================] - 3s 217us/step - loss: 0.7885 - acc: 0.6378 - val_loss: 0.6058 - val_acc: 0.7344\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 1.0842022239823443e-22.\n",
      "Epoch 14/100\n",
      "13340/13340 [==============================] - 3s 209us/step - loss: 0.7954 - acc: 0.6359 - val_loss: 0.6058 - val_acc: 0.7344\n",
      "Epoch 15/100\n",
      "13340/13340 [==============================] - 3s 208us/step - loss: 0.7901 - acc: 0.6382 - val_loss: 0.6058 - val_acc: 0.7344\n",
      "Epoch 16/100\n",
      "13340/13340 [==============================] - 3s 213us/step - loss: 0.7933 - acc: 0.6373 - val_loss: 0.6058 - val_acc: 0.7344\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 5.421011119911722e-23.\n",
      "Epoch 17/100\n",
      "13340/13340 [==============================] - 3s 207us/step - loss: 0.7926 - acc: 0.6397 - val_loss: 0.6058 - val_acc: 0.7344\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00017: early stopping\n",
      "         SR.p53: 0.76870\n",
      "Model saved and metrics table updated.\n",
      "Model saved and metrics table updated.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>threshold</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>auc_roc</th>\n",
       "      <th>avg_precision</th>\n",
       "      <th>confusion_matrix</th>\n",
       "      <th>model_filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.933442</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.024390</td>\n",
       "      <td>0.046512</td>\n",
       "      <td>0.806702</td>\n",
       "      <td>0.221990</td>\n",
       "      <td>[[574, 1], [40, 1]]</td>\n",
       "      <td>RF0.joblib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>RF_modT</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>0.886364</td>\n",
       "      <td>0.254237</td>\n",
       "      <td>0.365854</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.806702</td>\n",
       "      <td>0.221990</td>\n",
       "      <td>[[531, 44], [26, 15]]</td>\n",
       "      <td>RF_modT0.joblib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>DNN</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.753247</td>\n",
       "      <td>0.150943</td>\n",
       "      <td>0.585366</td>\n",
       "      <td>0.240000</td>\n",
       "      <td>0.768696</td>\n",
       "      <td>0.168234</td>\n",
       "      <td>[[440, 135], [17, 24]]</td>\n",
       "      <td>DNN0.h5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>DNN_modT</td>\n",
       "      <td>0.540362</td>\n",
       "      <td>0.780844</td>\n",
       "      <td>0.169014</td>\n",
       "      <td>0.585366</td>\n",
       "      <td>0.262295</td>\n",
       "      <td>0.768696</td>\n",
       "      <td>0.168234</td>\n",
       "      <td>[[457, 118], [17, 24]]</td>\n",
       "      <td>DNN_modT0.h5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      model  threshold  accuracy  precision    recall        f1   auc_roc  \\\n",
       "0        RF   0.500000  0.933442   0.500000  0.024390  0.046512  0.806702   \n",
       "1   RF_modT   0.220000  0.886364   0.254237  0.365854  0.300000  0.806702   \n",
       "2       DNN   0.500000  0.753247   0.150943  0.585366  0.240000  0.768696   \n",
       "3  DNN_modT   0.540362  0.780844   0.169014  0.585366  0.262295  0.768696   \n",
       "\n",
       "   avg_precision        confusion_matrix   model_filename  \n",
       "0       0.221990     [[574, 1], [40, 1]]       RF0.joblib  \n",
       "1       0.221990   [[531, 44], [26, 15]]  RF_modT0.joblib  \n",
       "2       0.168234  [[440, 135], [17, 24]]          DNN0.h5  \n",
       "3       0.168234  [[457, 118], [17, 24]]     DNN_modT0.h5  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for target in y_tr.columns:\n",
    "    # Determine rows with available data\n",
    "    rows_tr = np.isfinite(y_tr[target]).values\n",
    "    rows_te = np.isfinite(y_te[target]).values\n",
    "    x,y = x_tr[rows_tr], y_tr[target][rows_tr]\n",
    "    \n",
    "    # Address Class Imbalance\n",
    "    x_train, x_val, y_train, y_val = train_test_split(x, y, stratify=y, \\\n",
    "                                                      test_size=0.2, random_state=42)\n",
    "    ros = RandomOverSampler(random_state=0)\n",
    "    x_resampled, y_resampled = ros.fit_sample(x_train,y_train)\n",
    "    \n",
    "    # Train the DNN\n",
    "    DNN.fit(\n",
    "        x_resampled, y_resampled, batch_size=512, epochs=100,\\\n",
    "        validation_data=(x_val,y_val), verbose=1,\n",
    "        callbacks=[\n",
    "            keras.callbacks.EarlyStopping(patience=16,verbose=1,\\\n",
    "                                          restore_best_weights=True),\n",
    "            keras.callbacks.ReduceLROnPlateau(factor=0.5,patience=3,verbose=1)\n",
    "        ])\n",
    "    \n",
    "    # Get predictions, calculate model performance and save info\n",
    "    p_te = DNN.predict(x_te[rows_te])[:,0]\n",
    "    y_testing=y_te[target][rows_te]\n",
    "    auc_te = roc_auc_score(y_testing, p_te)\n",
    "    print(\"%15s: %3.5f\" % (target, auc_te))\n",
    "\n",
    "    y_hat_testing=DNN.predict_classes(x_te[rows_te])\n",
    "    average_precision=average_precision_score(y_testing,p_te)\n",
    "    mv=evaluate_model_predictions(target,'DNN',0.5,y_testing,y_hat_testing,\\\n",
    "                                  auc_te,average_precision)\n",
    "    filename = check_and_save(target,mv,DNN,True)\n",
    "    \n",
    "    # Find max F1 varying probability threshold, calculate modified performance, save\n",
    "    precision, recall, thresholds = precision_recall_curve(y_testing,p_te)\n",
    "    # f1 = 2*precision*recall/(precision+recall)  # Sometimes precision=recall=0!\n",
    "    p,r,t = zip(*[i for i in zip(precision,recall,np.append(thresholds,1)) if i[0:2]!=(0,0)])\n",
    "    p,r,t = np.array(p),np.array(r),np.array(t)\n",
    "    f1 = 2*p*r/(p+r)\n",
    "    m_idx = np.argmax(f1)\n",
    "    m_thresh = thresholds[m_idx]\n",
    "    y_hat_testing_adj=adjusted_classes(p_te,m_thresh)\n",
    "    mv=evaluate_model_predictions(target,'DNN_modT',m_thresh,y_testing,y_hat_testing_adj,\\\n",
    "                                  auc_te,average_precision)\n",
    "    if filename is None:\n",
    "        check_and_save(target,mv,DNN,True)\n",
    "    else:\n",
    "        check_and_save(target,mv,filename,True)\n",
    "    display(get_model_perfs(target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Performance Metrics Saving Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T05:43:49.580236Z",
     "start_time": "2019-12-02T05:43:49.383833Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "p_te = DNN.predict(x_te[rows_te])[:,0]\n",
    "y_testing=y_te[target][rows_te]\n",
    "auc_te = roc_auc_score(y_testing, p_te)\n",
    "print(\"%15s: %3.5f\" % (target, auc_te))\n",
    "\n",
    "y_hat_testing=DNN.predict_classes(x_te[rows_te])\n",
    "average_precision=average_precision_score(y_testing,p_te)\n",
    "mv=evaluate_model_predictions(target,'DNN',0.5,y_testing,y_hat_testing,\\\n",
    "                              auc_te,average_precision)\n",
    "#     check_and_save(target,mv,DNN,True)\n",
    "display(mv)\n",
    "precision, recall, thresholds = precision_recall_curve(y_testing,p_te)\n",
    "# f1 = 2*precision*recall/(precision+recall)\n",
    "p,r,t = zip(*[i for i in zip(precision,recall,np.append(thresholds,1)) if i[0:2]!=(0,0)])\n",
    "p,r,t = np.array(p),np.array(r),np.array(t)\n",
    "f1 = 2*p*r/(p+r)\n",
    "m_idx = np.argmax(f1)\n",
    "m_thresh = thresholds[m_idx]\n",
    "y_hat_testing_adj=adjusted_classes(p_te,m_thresh)\n",
    "mv=evaluate_model_predictions(target,'DNN_modT',m_thresh,y_testing,y_hat_testing_adj,\\\n",
    "                              auc_te,average_precision)\n",
    "#     check_and_save(target,mv,DNN,True)\n",
    "# display(get_model_perfs(target))\n",
    "display(mv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Troubleshooting Precision-Recall Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T05:08:08.069861Z",
     "start_time": "2019-12-02T05:08:08.008719Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "f1 = 2*precision*recall/(precision+recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T05:52:54.150467Z",
     "start_time": "2019-12-02T05:52:54.089918Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "a, b = zip(*zip(precision,recall))\n",
    "type(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T05:41:49.450684Z",
     "start_time": "2019-12-02T05:41:49.389635Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "p,r,t = zip(*[i for i in zip(precision,recall,np.append(thresholds,1)) if i[0:2]!=(0,0)])\n",
    "p,r,t = np.array(p),np.array(r),np.array(t)\n",
    "f1 = 2*p*r/(p+r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Testing DNN Model Saving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T00:48:15.368581Z",
     "start_time": "2019-12-03T00:48:15.274930Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DNN_test0.h5'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = save_model(target,'DNN_test',DNN,True)\n",
    "filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T00:48:29.774885Z",
     "start_time": "2019-12-03T00:48:28.661557Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.engine.sequential.Sequential at 0x1276ace10>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_model(target,filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T00:48:35.160766Z",
     "start_time": "2019-12-03T00:48:35.156689Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.5'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Saved DNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-08T01:39:02.735942Z",
     "start_time": "2019-12-08T01:39:01.497973Z"
    }
   },
   "outputs": [],
   "source": [
    "target = 'NR.PPAR.gamma'\n",
    "target = 'NR.AhR'\n",
    "DNN = read_model(target,'DNN0.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance of Read DNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-08T05:27:54.493004Z",
     "start_time": "2019-12-08T05:27:54.418889Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         NR.AhR: 0.86472\n"
     ]
    }
   ],
   "source": [
    "rows_tr = np.isfinite(y_tr[target]).values\n",
    "rows_te = np.isfinite(y_te[target]).values\n",
    "p_te = DNN.predict(x_te[rows_te])[:,0]\n",
    "auc_te = roc_auc_score(y_te[target][rows_te], p_te)\n",
    "average_precision=average_precision_score(y_testing, p_te)\n",
    "print(\"%15s: %3.5f\" % (target, auc_te))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-08T05:27:58.187913Z",
     "start_time": "2019-12-08T05:27:58.107715Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model                                 DNN\n",
       "threshold                             0.5\n",
       "accuracy                         0.814754\n",
       "precision                        0.364865\n",
       "recall                           0.739726\n",
       "f1                               0.488688\n",
       "auc_roc                          0.864723\n",
       "avg_precision                    0.524576\n",
       "confusion_matrix    [[443, 94], [19, 54]]\n",
       "dtype: object"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_testing=y_te[target][rows_te]\n",
    "y_hat_testing=DNN.predict_classes(x_te[rows_te])\n",
    "evaluate_model_predictions(target, 'DNN', 0.5, y_testing, y_hat_testing, auc_te, average_precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-08T03:26:58.141889Z",
     "start_time": "2019-12-08T03:26:58.139422Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-08T03:35:52.798060Z",
     "start_time": "2019-12-08T03:35:52.656523Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_te[target][rows_te], p_te)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "         lw=2, label='ROC curve (area = %0.3f)' % auc_te)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title(target+' Receiver operating characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix at Alternate Decision Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-08T05:31:35.904624Z",
     "start_time": "2019-12-08T05:31:35.886589Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision-Recall Curve \"Average Precision\": 0.5246\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "model                                 DNN\n",
       "threshold                             0.7\n",
       "accuracy                         0.885246\n",
       "precision                            0.52\n",
       "recall                           0.534247\n",
       "f1                               0.527027\n",
       "auc_roc                          0.864723\n",
       "avg_precision                    0.524576\n",
       "confusion_matrix    [[501, 36], [34, 39]]\n",
       "dtype: object"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decision_threshold = 0.7\n",
    "y_testing=y_te[target][rows_te]\n",
    "y_hat_testing_adj=adjusted_classes(p_te,decision_threshold)\n",
    "print('Precision-Recall Curve \"Average Precision\": %0.4f' %average_precision)\n",
    "mv=evaluate_model_predictions(target,'DNN',decision_threshold,y_testing,y_hat_testing_adj,\\\n",
    "                              auc_te,average_precision)\n",
    "mv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision-Recall Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-08T05:33:38.298360Z",
     "start_time": "2019-12-08T05:33:38.289900Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max F1: 0.55882, threshold prob: 0.77352\n",
      "Max F1 Precision: 0.60317, Recall: 0.52055\n"
     ]
    }
   ],
   "source": [
    "precision, recall, thresholds = precision_recall_curve(y_testing,p_te)\n",
    "precision, recall, thresholds = zip(*[i for i in zip(precision,recall,np.append(thresholds,1)) if i[0:2]!=(0,0)])\n",
    "precision, recall, thresholds = np.array(precision),np.array(recall),np.array(thresholds)\n",
    "f1 = 2*precision*recall/(precision+recall)\n",
    "m_idx = np.argmax(f1)\n",
    "m_thresh = thresholds[m_idx]\n",
    "print('Max F1: %0.5f, threshold prob: %0.5f' % (f1[m_idx], m_thresh))\n",
    "print('Max F1 Precision: %0.5f, Recall: %0.5f' % (precision[m_idx],recall[m_idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-08T05:33:40.304338Z",
     "start_time": "2019-12-08T05:33:40.090467Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "no_skill = len(y_testing[y_testing==1]) / len(y_testing)\n",
    "fig = plt.figure()\n",
    "plt.step(recall, precision, color='b', alpha=0.2, where='post')\n",
    "plt.fill_between(recall, precision, step='post', alpha=0.2, color='b',\\\n",
    "                 label='DNN')\n",
    "plt.plot([0, 1], [no_skill, no_skill], color='r', linestyle='--', label='No Skill:'+' %0.3f' % no_skill)\n",
    "plt.plot([0, 1], [precision[m_idx],precision[m_idx]], color='k', alpha=0.4, linestyle=':')\n",
    "plt.plot([recall[m_idx],recall[m_idx]],[0, 1], color='k', alpha=0.4,linestyle=':',\\\n",
    "         label='Prec/Rec @ Max F1')\n",
    "plt.text(recall[m_idx], f1[m_idx]+0.01, 'Max F1={0:0.3f}'.format(f1[m_idx]))\n",
    "plt.plot(recall,f1,color='k',label='F1')\n",
    "plt.xlabel('Recall', fontsize=14)\n",
    "plt.ylabel('Precision or F1 Score', fontsize=14)\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.legend()\n",
    "title=target+' DNN\\n Precision-Recall curve: AP={0:0.2f}'.format(\n",
    "          average_precision)\n",
    "plt.title(title, fontsize=16);\n",
    "# fig.savefig('./reports/figures/'+target+'_DNN_PrecisionRecallCurve.svg',\\\n",
    "#             format='svg', dpi=1200, transparent=True, bbox_inches = \"tight\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-08T05:33:41.519272Z",
     "start_time": "2019-12-08T05:33:41.507004Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.15368421052631578, 1.0, 0.04490542411804199)\n",
      "(0.1518987341772152, 0.9863013698630136, 0.045489758253097534)\n",
      "(0.1522198731501057, 0.9863013698630136, 0.045714378356933594)\n",
      "(0.15254237288135594, 0.9863013698630136, 0.04712679982185364)\n",
      "(0.15286624203821655, 0.9863013698630136, 0.04766783118247986)\n",
      "(0.15319148936170213, 0.9863013698630136, 0.047692179679870605)\n",
      "(0.1535181236673774, 0.9863013698630136, 0.04854416847229004)\n",
      "(0.15384615384615385, 0.9863013698630136, 0.04902520775794983)\n",
      "(0.15417558886509636, 0.9863013698630136, 0.04943159222602844)\n",
      "(0.15450643776824036, 0.9863013698630136, 0.05018043518066406)\n",
      "(0.15483870967741936, 0.9863013698630136, 0.050295889377593994)\n",
      "(0.15517241379310345, 0.9863013698630136, 0.053382664918899536)\n",
      "(0.15550755939524838, 0.9863013698630136, 0.053609758615493774)\n",
      "(0.15584415584415584, 0.9863013698630136, 0.05404910445213318)\n",
      "(0.1561822125813449, 0.9863013698630136, 0.05496317148208618)\n",
      "(0.1565217391304348, 0.9863013698630136, 0.05689054727554321)\n",
      "(0.1568627450980392, 0.9863013698630136, 0.057233959436416626)\n",
      "(0.1572052401746725, 0.9863013698630136, 0.057458460330963135)\n",
      "(0.1575492341356674, 0.9863013698630136, 0.058954060077667236)\n",
      "(0.15789473684210525, 0.9863013698630136, 0.059755682945251465)\n",
      "(0.15824175824175823, 0.9863013698630136, 0.061173439025878906)\n",
      "(0.15859030837004406, 0.9863013698630136, 0.06212562322616577)\n",
      "(0.15894039735099338, 0.9863013698630136, 0.06222006678581238)\n",
      "(0.1592920353982301, 0.9863013698630136, 0.06334149837493896)\n",
      "(0.15964523281596452, 0.9863013698630136, 0.06650403141975403)\n",
      "(0.16, 0.9863013698630136, 0.06712400913238525)\n",
      "(0.1603563474387528, 0.9863013698630136, 0.06719464063644409)\n",
      "(0.16071428571428573, 0.9863013698630136, 0.06861421465873718)\n",
      "(0.1610738255033557, 0.9863013698630136, 0.06898745894432068)\n",
      "(0.16143497757847533, 0.9863013698630136, 0.06955099105834961)\n",
      "(0.16179775280898875, 0.9863013698630136, 0.06991225481033325)\n",
      "(0.16216216216216217, 0.9863013698630136, 0.07133087515830994)\n",
      "(0.16252821670428894, 0.9863013698630136, 0.07268384099006653)\n",
      "(0.16289592760180996, 0.9863013698630136, 0.07661658525466919)\n",
      "(0.16326530612244897, 0.9863013698630136, 0.07667195796966553)\n",
      "(0.16363636363636364, 0.9863013698630136, 0.0768747627735138)\n",
      "(0.16400911161731208, 0.9863013698630136, 0.08123224973678589)\n",
      "(0.1643835616438356, 0.9863013698630136, 0.083285391330719)\n",
      "(0.16475972540045766, 0.9863013698630136, 0.08439299464225769)\n",
      "(0.1651376146788991, 0.9863013698630136, 0.0848478376865387)\n",
      "(0.1632183908045977, 0.9726027397260274, 0.08491796255111694)\n",
      "(0.16359447004608296, 0.9726027397260274, 0.08801665902137756)\n",
      "(0.16397228637413394, 0.9726027397260274, 0.08878284692764282)\n",
      "(0.16435185185185186, 0.9726027397260274, 0.0889827311038971)\n",
      "(0.16473317865429235, 0.9726027397260274, 0.09008798003196716)\n",
      "(0.16511627906976745, 0.9726027397260274, 0.09264189004898071)\n",
      "(0.1655011655011655, 0.9726027397260274, 0.0944695770740509)\n",
      "(0.1658878504672897, 0.9726027397260274, 0.0961102545261383)\n",
      "(0.16627634660421545, 0.9726027397260274, 0.09999546408653259)\n",
      "(0.16666666666666666, 0.9726027397260274, 0.10004815459251404)\n",
      "(0.16705882352941176, 0.9726027397260274, 0.1042872965335846)\n",
      "(0.16745283018867924, 0.9726027397260274, 0.1049027144908905)\n",
      "(0.16784869976359337, 0.9726027397260274, 0.10795605182647705)\n",
      "(0.16904761904761906, 0.9726027397260274, 0.11379793286323547)\n",
      "(0.16945107398568018, 0.9726027397260274, 0.11443361639976501)\n",
      "(0.16985645933014354, 0.9726027397260274, 0.1148797869682312)\n",
      "(0.17026378896882494, 0.9726027397260274, 0.11584910750389099)\n",
      "(0.17067307692307693, 0.9726027397260274, 0.11629915237426758)\n",
      "(0.1710843373493976, 0.9726027397260274, 0.11921098828315735)\n",
      "(0.17149758454106281, 0.9726027397260274, 0.12049174308776855)\n",
      "(0.17191283292978207, 0.9726027397260274, 0.12120988965034485)\n",
      "(0.17233009708737865, 0.9726027397260274, 0.1229921281337738)\n",
      "(0.17274939172749393, 0.9726027397260274, 0.12610885500907898)\n",
      "(0.17317073170731706, 0.9726027397260274, 0.12693458795547485)\n",
      "(0.17359413202933985, 0.9726027397260274, 0.1274263560771942)\n",
      "(0.17401960784313725, 0.9726027397260274, 0.13274481892585754)\n",
      "(0.17444717444717445, 0.9726027397260274, 0.1356182098388672)\n",
      "(0.1748768472906404, 0.9726027397260274, 0.1457826793193817)\n",
      "(0.17530864197530865, 0.9726027397260274, 0.14656072854995728)\n",
      "(0.17574257425742573, 0.9726027397260274, 0.14720341563224792)\n",
      "(0.1761786600496278, 0.9726027397260274, 0.14743566513061523)\n",
      "(0.17661691542288557, 0.9726027397260274, 0.1479237675666809)\n",
      "(0.1770573566084788, 0.9726027397260274, 0.1479872465133667)\n",
      "(0.1775, 0.9726027397260274, 0.14993047714233398)\n",
      "(0.17794486215538846, 0.9726027397260274, 0.15235358476638794)\n",
      "(0.17839195979899497, 0.9726027397260274, 0.1524592638015747)\n",
      "(0.17884130982367757, 0.9726027397260274, 0.15277957916259766)\n",
      "(0.17676767676767677, 0.958904109589041, 0.1554047167301178)\n",
      "(0.17721518987341772, 0.958904109589041, 0.15722793340682983)\n",
      "(0.17766497461928935, 0.958904109589041, 0.15983477234840393)\n",
      "(0.178117048346056, 0.958904109589041, 0.16027727723121643)\n",
      "(0.1760204081632653, 0.9452054794520548, 0.1698664128780365)\n",
      "(0.17647058823529413, 0.9452054794520548, 0.17402568459510803)\n",
      "(0.17692307692307693, 0.9452054794520548, 0.17584601044654846)\n",
      "(0.17737789203084833, 0.9452054794520548, 0.18598294258117676)\n",
      "(0.17783505154639176, 0.9452054794520548, 0.18779227137565613)\n",
      "(0.17829457364341086, 0.9452054794520548, 0.18868836760520935)\n",
      "(0.17875647668393782, 0.9452054794520548, 0.18960338830947876)\n",
      "(0.17922077922077922, 0.9452054794520548, 0.19077670574188232)\n",
      "(0.1796875, 0.9452054794520548, 0.19171792268753052)\n",
      "(0.1801566579634465, 0.9452054794520548, 0.1922684609889984)\n",
      "(0.1806282722513089, 0.9452054794520548, 0.19330385327339172)\n",
      "(0.18110236220472442, 0.9452054794520548, 0.19446000456809998)\n",
      "(0.18157894736842106, 0.9452054794520548, 0.20466294884681702)\n",
      "(0.1820580474934037, 0.9452054794520548, 0.2081601321697235)\n",
      "(0.18253968253968253, 0.9452054794520548, 0.2218191921710968)\n",
      "(0.1830238726790451, 0.9452054794520548, 0.22318345308303833)\n",
      "(0.18351063829787234, 0.9452054794520548, 0.22612595558166504)\n",
      "(0.184, 0.9452054794520548, 0.2279050052165985)\n",
      "(0.18449197860962566, 0.9452054794520548, 0.22964102029800415)\n",
      "(0.18498659517426275, 0.9452054794520548, 0.2382817566394806)\n",
      "(0.18548387096774194, 0.9452054794520548, 0.24593105912208557)\n",
      "(0.18598382749326145, 0.9452054794520548, 0.2460276484489441)\n",
      "(0.1864864864864865, 0.9452054794520548, 0.2464137077331543)\n",
      "(0.18699186991869918, 0.9452054794520548, 0.25326013565063477)\n",
      "(0.1885245901639344, 0.9452054794520548, 0.2548845410346985)\n",
      "(0.18904109589041096, 0.9452054794520548, 0.25571614503860474)\n",
      "(0.18956043956043955, 0.9452054794520548, 0.2592087388038635)\n",
      "(0.19008264462809918, 0.9452054794520548, 0.2593734860420227)\n",
      "(0.19113573407202217, 0.9452054794520548, 0.25968754291534424)\n",
      "(0.19166666666666668, 0.9452054794520548, 0.2607216238975525)\n",
      "(0.19220055710306408, 0.9452054794520548, 0.2630266547203064)\n",
      "(0.19273743016759776, 0.9452054794520548, 0.2651798725128174)\n",
      "(0.19327731092436976, 0.9452054794520548, 0.27025479078292847)\n",
      "(0.19382022471910113, 0.9452054794520548, 0.27032727003097534)\n",
      "(0.1954674220963173, 0.9452054794520548, 0.27621954679489136)\n",
      "(0.19602272727272727, 0.9452054794520548, 0.27654725313186646)\n",
      "(0.19658119658119658, 0.9452054794520548, 0.27812445163726807)\n",
      "(0.19714285714285715, 0.9452054794520548, 0.2801768183708191)\n",
      "(0.1977077363896848, 0.9452054794520548, 0.28493934869766235)\n",
      "(0.19827586206896552, 0.9452054794520548, 0.28735965490341187)\n",
      "(0.1988472622478386, 0.9452054794520548, 0.291779100894928)\n",
      "(0.1994219653179191, 0.9452054794520548, 0.29433998465538025)\n",
      "(0.2, 0.9452054794520548, 0.29566577076911926)\n",
      "(0.2005813953488372, 0.9452054794520548, 0.29644158482551575)\n",
      "(0.20116618075801748, 0.9452054794520548, 0.298957884311676)\n",
      "(0.20175438596491227, 0.9452054794520548, 0.2990800738334656)\n",
      "(0.20234604105571846, 0.9452054794520548, 0.29947710037231445)\n",
      "(0.20294117647058824, 0.9452054794520548, 0.3011409044265747)\n",
      "(0.20353982300884957, 0.9452054794520548, 0.30528950691223145)\n",
      "(0.20414201183431951, 0.9452054794520548, 0.30650660395622253)\n",
      "(0.20535714285714285, 0.9452054794520548, 0.31301432847976685)\n",
      "(0.20597014925373133, 0.9452054794520548, 0.31519418954849243)\n",
      "(0.20658682634730538, 0.9452054794520548, 0.31571319699287415)\n",
      "(0.2072072072072072, 0.9452054794520548, 0.31610962748527527)\n",
      "(0.20783132530120482, 0.9452054794520548, 0.32183828949928284)\n",
      "(0.2084592145015106, 0.9452054794520548, 0.32559964060783386)\n",
      "(0.20909090909090908, 0.9452054794520548, 0.32816824316978455)\n",
      "(0.2066869300911854, 0.9315068493150684, 0.32953137159347534)\n",
      "(0.2073170731707317, 0.9315068493150684, 0.33335763216018677)\n",
      "(0.20795107033639143, 0.9315068493150684, 0.3374309539794922)\n",
      "(0.2085889570552147, 0.9315068493150684, 0.3394615054130554)\n",
      "(0.20615384615384616, 0.9178082191780822, 0.3441324830055237)\n",
      "(0.20679012345679013, 0.9178082191780822, 0.3444404602050781)\n",
      "(0.20743034055727555, 0.9178082191780822, 0.3483821153640747)\n",
      "(0.2080745341614907, 0.9178082191780822, 0.3513973355293274)\n",
      "(0.2087227414330218, 0.9178082191780822, 0.3523899018764496)\n",
      "(0.209375, 0.9178082191780822, 0.35491958260536194)\n",
      "(0.21003134796238246, 0.9178082191780822, 0.35566356778144836)\n",
      "(0.21069182389937108, 0.9178082191780822, 0.35703206062316895)\n",
      "(0.2113564668769716, 0.9178082191780822, 0.36099886894226074)\n",
      "(0.2120253164556962, 0.9178082191780822, 0.3626645803451538)\n",
      "(0.2126984126984127, 0.9178082191780822, 0.36417508125305176)\n",
      "(0.21337579617834396, 0.9178082191780822, 0.3689899444580078)\n",
      "(0.2161290322580645, 0.9178082191780822, 0.36952656507492065)\n",
      "(0.2168284789644013, 0.9178082191780822, 0.37049680948257446)\n",
      "(0.21172638436482086, 0.8904109589041096, 0.371654212474823)\n",
      "(0.21311475409836064, 0.8904109589041096, 0.3723633289337158)\n",
      "(0.21885521885521886, 0.8904109589041096, 0.37655922770500183)\n",
      "(0.22033898305084745, 0.8904109589041096, 0.37680432200431824)\n",
      "(0.22108843537414966, 0.8904109589041096, 0.3779428005218506)\n",
      "(0.22336769759450173, 0.8904109589041096, 0.3800082206726074)\n",
      "(0.22569444444444445, 0.8904109589041096, 0.3801257610321045)\n",
      "(0.2264808362369338, 0.8904109589041096, 0.3806285262107849)\n",
      "(0.22727272727272727, 0.8904109589041096, 0.38088056445121765)\n",
      "(0.22807017543859648, 0.8904109589041096, 0.38190755248069763)\n",
      "(0.22887323943661972, 0.8904109589041096, 0.38440775871276855)\n",
      "(0.22968197879858657, 0.8904109589041096, 0.38497698307037354)\n",
      "(0.23049645390070922, 0.8904109589041096, 0.3873814344406128)\n",
      "(0.2313167259786477, 0.8904109589041096, 0.3878899812698364)\n",
      "(0.23214285714285715, 0.8904109589041096, 0.3883744478225708)\n",
      "(0.23297491039426524, 0.8904109589041096, 0.3883848488330841)\n",
      "(0.23381294964028776, 0.8904109589041096, 0.3886972963809967)\n",
      "(0.23465703971119134, 0.8904109589041096, 0.3891494870185852)\n",
      "(0.23550724637681159, 0.8904109589041096, 0.39034175872802734)\n",
      "(0.23636363636363636, 0.8904109589041096, 0.3909063935279846)\n",
      "(0.23722627737226276, 0.8904109589041096, 0.3911956548690796)\n",
      "(0.23897058823529413, 0.8904109589041096, 0.394798219203949)\n",
      "(0.23985239852398524, 0.8904109589041096, 0.39572086930274963)\n",
      "(0.24074074074074073, 0.8904109589041096, 0.39927276968955994)\n",
      "(0.241635687732342, 0.8904109589041096, 0.40050584077835083)\n",
      "(0.24253731343283583, 0.8904109589041096, 0.4007510542869568)\n",
      "(0.24436090225563908, 0.8904109589041096, 0.40075287222862244)\n",
      "(0.24528301886792453, 0.8904109589041096, 0.40153759717941284)\n",
      "(0.24714828897338403, 0.8904109589041096, 0.4058988690376282)\n",
      "(0.24427480916030533, 0.8767123287671232, 0.4061775505542755)\n",
      "(0.24521072796934865, 0.8767123287671232, 0.4100812077522278)\n",
      "(0.24806201550387597, 0.8767123287671232, 0.41059839725494385)\n",
      "(0.2490272373540856, 0.8767123287671232, 0.41139623522758484)\n",
      "(0.24609375, 0.863013698630137, 0.4121297001838684)\n",
      "(0.24803149606299213, 0.863013698630137, 0.4131685495376587)\n",
      "(0.2490118577075099, 0.863013698630137, 0.41373059153556824)\n",
      "(0.25, 0.863013698630137, 0.41690608859062195)\n",
      "(0.2550607287449393, 0.863013698630137, 0.4176948666572571)\n",
      "(0.25609756097560976, 0.863013698630137, 0.42004263401031494)\n",
      "(0.2571428571428571, 0.863013698630137, 0.421369731426239)\n",
      "(0.2581967213114754, 0.863013698630137, 0.42329350113868713)\n",
      "(0.25925925925925924, 0.863013698630137, 0.4255886971950531)\n",
      "(0.26141078838174275, 0.863013698630137, 0.4265802204608917)\n",
      "(0.2625, 0.863013698630137, 0.42699339985847473)\n",
      "(0.26359832635983266, 0.863013698630137, 0.4284731149673462)\n",
      "(0.2669491525423729, 0.863013698630137, 0.4294170141220093)\n",
      "(0.2680851063829787, 0.863013698630137, 0.43006157875061035)\n",
      "(0.2692307692307692, 0.863013698630137, 0.4311012625694275)\n",
      "(0.2703862660944206, 0.863013698630137, 0.43149587512016296)\n",
      "(0.27155172413793105, 0.863013698630137, 0.43207070231437683)\n",
      "(0.2727272727272727, 0.863013698630137, 0.4323614835739136)\n",
      "(0.27391304347826084, 0.863013698630137, 0.43918460607528687)\n",
      "(0.27510917030567683, 0.863013698630137, 0.43971532583236694)\n",
      "(0.29245283018867924, 0.8493150684931506, 0.44049307703971863)\n",
      "(0.2938388625592417, 0.8493150684931506, 0.4425850212574005)\n",
      "(0.29523809523809524, 0.8493150684931506, 0.44555237889289856)\n",
      "(0.2966507177033493, 0.8493150684931506, 0.4458658993244171)\n",
      "(0.2980769230769231, 0.8493150684931506, 0.446331262588501)\n",
      "(0.2995169082125604, 0.8493150684931506, 0.45078977942466736)\n",
      "(0.2975609756097561, 0.8356164383561644, 0.4509440064430237)\n",
      "(0.29901960784313725, 0.8356164383561644, 0.4512977600097656)\n",
      "(0.3, 0.821917808219178, 0.4513276517391205)\n",
      "(0.30456852791878175, 0.821917808219178, 0.4515097141265869)\n",
      "(0.30612244897959184, 0.821917808219178, 0.45228731632232666)\n",
      "(0.31088082901554404, 0.821917808219178, 0.45281320810317993)\n",
      "(0.31413612565445026, 0.821917808219178, 0.4538661241531372)\n",
      "(0.32085561497326204, 0.821917808219178, 0.453866571187973)\n",
      "(0.3225806451612903, 0.821917808219178, 0.45612359046936035)\n",
      "(0.32432432432432434, 0.821917808219178, 0.4565807580947876)\n",
      "(0.32967032967032966, 0.821917808219178, 0.456631600856781)\n",
      "(0.3314917127071823, 0.821917808219178, 0.4572427272796631)\n",
      "(0.33519553072625696, 0.821917808219178, 0.45739319920539856)\n",
      "(0.33707865168539325, 0.821917808219178, 0.4583677053451538)\n",
      "(0.3389830508474576, 0.821917808219178, 0.46099796891212463)\n",
      "(0.3409090909090909, 0.821917808219178, 0.4624391198158264)\n",
      "(0.34285714285714286, 0.821917808219178, 0.46252986788749695)\n",
      "(0.3390804597701149, 0.8082191780821918, 0.4660303592681885)\n",
      "(0.34104046242774566, 0.8082191780821918, 0.46634694933891296)\n",
      "(0.3372093023255814, 0.7945205479452054, 0.4676017463207245)\n",
      "(0.3391812865497076, 0.7945205479452054, 0.4676986634731293)\n",
      "(0.3411764705882353, 0.7945205479452054, 0.4758879840373993)\n",
      "(0.3431952662721893, 0.7945205479452054, 0.4778735935688019)\n",
      "(0.34523809523809523, 0.7945205479452054, 0.47849538922309875)\n",
      "(0.3473053892215569, 0.7945205479452054, 0.4797886312007904)\n",
      "(0.3493975903614458, 0.7945205479452054, 0.48001188039779663)\n",
      "(0.35403726708074534, 0.7808219178082192, 0.48103752732276917)\n",
      "(0.35, 0.7671232876712328, 0.48104149103164673)\n",
      "(0.3522012578616352, 0.7671232876712328, 0.48173362016677856)\n",
      "(0.35443037974683544, 0.7671232876712328, 0.48202112317085266)\n",
      "(0.35668789808917195, 0.7671232876712328, 0.48454749584198)\n",
      "(0.358974358974359, 0.7671232876712328, 0.4849645495414734)\n",
      "(0.36129032258064514, 0.7671232876712328, 0.4881163537502289)\n",
      "(0.36363636363636365, 0.7671232876712328, 0.4920010566711426)\n",
      "(0.3691275167785235, 0.7534246575342466, 0.4966689646244049)\n",
      "(0.36486486486486486, 0.7397260273972602, 0.5009897351264954)\n",
      "(0.36054421768707484, 0.726027397260274, 0.5013149976730347)\n",
      "(0.363013698630137, 0.726027397260274, 0.5043995976448059)\n",
      "(0.36551724137931035, 0.726027397260274, 0.507084846496582)\n",
      "(0.36428571428571427, 0.6986301369863014, 0.510388195514679)\n",
      "(0.3669064748201439, 0.6986301369863014, 0.5113679766654968)\n",
      "(0.3695652173913043, 0.6986301369863014, 0.5158885717391968)\n",
      "(0.3722627737226277, 0.6986301369863014, 0.5211424827575684)\n",
      "(0.375, 0.6986301369863014, 0.5304339528083801)\n",
      "(0.37777777777777777, 0.6986301369863014, 0.5331457257270813)\n",
      "(0.3805970149253731, 0.6986301369863014, 0.536141037940979)\n",
      "(0.38345864661654133, 0.6986301369863014, 0.5365080833435059)\n",
      "(0.38636363636363635, 0.6986301369863014, 0.53658127784729)\n",
      "(0.3893129770992366, 0.6986301369863014, 0.5408362150192261)\n",
      "(0.3923076923076923, 0.6986301369863014, 0.5421110391616821)\n",
      "(0.3953488372093023, 0.6986301369863014, 0.5424515008926392)\n",
      "(0.3984375, 0.6986301369863014, 0.5432254076004028)\n",
      "(0.4015748031496063, 0.6986301369863014, 0.5514640808105469)\n",
      "(0.40476190476190477, 0.6986301369863014, 0.5514824390411377)\n",
      "(0.408, 0.6986301369863014, 0.5518971681594849)\n",
      "(0.4032258064516129, 0.684931506849315, 0.5519930720329285)\n",
      "(0.4065040650406504, 0.684931506849315, 0.5602468252182007)\n",
      "(0.4098360655737705, 0.684931506849315, 0.5615075826644897)\n",
      "(0.4049586776859504, 0.6712328767123288, 0.5640390515327454)\n",
      "(0.4083333333333333, 0.6712328767123288, 0.5640461444854736)\n",
      "(0.40336134453781514, 0.6575342465753424, 0.5651372075080872)\n",
      "(0.4067796610169492, 0.6575342465753424, 0.5679108500480652)\n",
      "(0.41025641025641024, 0.6575342465753424, 0.5684789419174194)\n",
      "(0.4051724137931034, 0.6438356164383562, 0.5764385461807251)\n",
      "(0.4, 0.6301369863013698, 0.5772256851196289)\n",
      "(0.40350877192982454, 0.6301369863013698, 0.5807936191558838)\n",
      "(0.40707964601769914, 0.6301369863013698, 0.5850045084953308)\n",
      "(0.4107142857142857, 0.6301369863013698, 0.5911409258842468)\n",
      "(0.4144144144144144, 0.6301369863013698, 0.5915424227714539)\n",
      "(0.41818181818181815, 0.6301369863013698, 0.592665433883667)\n",
      "(0.42201834862385323, 0.6301369863013698, 0.5989089012145996)\n",
      "(0.42592592592592593, 0.6301369863013698, 0.6045659780502319)\n",
      "(0.42990654205607476, 0.6301369863013698, 0.6064467430114746)\n",
      "(0.4339622641509434, 0.6301369863013698, 0.6102601885795593)\n",
      "(0.4380952380952381, 0.6301369863013698, 0.6152163743972778)\n",
      "(0.4423076923076923, 0.6301369863013698, 0.6153562068939209)\n",
      "(0.44660194174757284, 0.6301369863013698, 0.6193393468856812)\n",
      "(0.4411764705882353, 0.6164383561643836, 0.6264550685882568)\n",
      "(0.44554455445544555, 0.6164383561643836, 0.6308777332305908)\n",
      "(0.45, 0.6164383561643836, 0.6314778327941895)\n",
      "(0.45454545454545453, 0.6164383561643836, 0.6327101588249207)\n",
      "(0.45918367346938777, 0.6164383561643836, 0.6357414722442627)\n",
      "(0.4536082474226804, 0.6027397260273972, 0.6469142436981201)\n",
      "(0.4479166666666667, 0.589041095890411, 0.6506783366203308)\n",
      "(0.45263157894736844, 0.589041095890411, 0.6533422470092773)\n",
      "(0.44680851063829785, 0.5753424657534246, 0.6538443565368652)\n",
      "(0.45161290322580644, 0.5753424657534246, 0.6550498604774475)\n",
      "(0.44565217391304346, 0.5616438356164384, 0.6587207317352295)\n",
      "(0.45054945054945056, 0.5616438356164384, 0.6601157784461975)\n",
      "(0.45555555555555555, 0.5616438356164384, 0.6643352508544922)\n",
      "(0.4606741573033708, 0.5616438356164384, 0.6670020818710327)\n",
      "(0.4659090909090909, 0.5616438356164384, 0.6739242672920227)\n",
      "(0.47126436781609193, 0.5616438356164384, 0.6766772270202637)\n",
      "(0.47674418604651164, 0.5616438356164384, 0.6778513789176941)\n",
      "(0.47058823529411764, 0.547945205479452, 0.6785597205162048)\n",
      "(0.47619047619047616, 0.547945205479452, 0.681110143661499)\n",
      "(0.4819277108433735, 0.547945205479452, 0.6814991235733032)\n",
      "(0.4878048780487805, 0.547945205479452, 0.6840857267379761)\n",
      "(0.49382716049382713, 0.547945205479452, 0.6843616962432861)\n",
      "(0.5, 0.547945205479452, 0.6884187459945679)\n",
      "(0.5063291139240507, 0.547945205479452, 0.6914979219436646)\n",
      "(0.5, 0.5342465753424658, 0.6915061473846436)\n",
      "(0.5064935064935064, 0.5342465753424658, 0.6940404772758484)\n",
      "(0.5131578947368421, 0.5342465753424658, 0.6976384520530701)\n",
      "(0.52, 0.5342465753424658, 0.7068884372711182)\n",
      "(0.527027027027027, 0.5342465753424658, 0.7070654630661011)\n",
      "(0.5342465753424658, 0.5342465753424658, 0.7137702107429504)\n",
      "(0.5416666666666666, 0.5342465753424658, 0.7168376445770264)\n",
      "(0.5492957746478874, 0.5342465753424658, 0.717110276222229)\n",
      "(0.5571428571428572, 0.5342465753424658, 0.7214062213897705)\n",
      "(0.5652173913043478, 0.5342465753424658, 0.7282474040985107)\n",
      "(0.5588235294117647, 0.5205479452054794, 0.7312098741531372)\n",
      "(0.5671641791044776, 0.5205479452054794, 0.7386695146560669)\n",
      "(0.5757575757575758, 0.5205479452054794, 0.7451967000961304)\n",
      "(0.5846153846153846, 0.5205479452054794, 0.765274167060852)\n",
      "(0.59375, 0.5205479452054794, 0.7658594846725464)\n",
      "(0.6031746031746031, 0.5205479452054794, 0.7735157012939453)\n",
      "(0.5967741935483871, 0.5068493150684932, 0.7766470909118652)\n",
      "(0.6065573770491803, 0.5068493150684932, 0.7774405479431152)\n",
      "(0.6, 0.4931506849315068, 0.7788974642753601)\n",
      "(0.6101694915254238, 0.4931506849315068, 0.7792001962661743)\n",
      "(0.6206896551724138, 0.4931506849315068, 0.780947744846344)\n",
      "(0.631578947368421, 0.4931506849315068, 0.7813196182250977)\n",
      "(0.625, 0.4794520547945205, 0.7830116748809814)\n",
      "(0.6181818181818182, 0.4657534246575342, 0.7892935276031494)\n",
      "(0.6111111111111112, 0.4520547945205479, 0.796334445476532)\n",
      "(0.6226415094339622, 0.4520547945205479, 0.7987356185913086)\n",
      "(0.6153846153846154, 0.4383561643835616, 0.8083239793777466)\n",
      "(0.6078431372549019, 0.4246575342465753, 0.8096351623535156)\n",
      "(0.6, 0.410958904109589, 0.8163020610809326)\n",
      "(0.6122448979591837, 0.410958904109589, 0.8193998336791992)\n",
      "(0.6041666666666666, 0.3972602739726027, 0.8195630311965942)\n",
      "(0.6170212765957447, 0.3972602739726027, 0.821904182434082)\n",
      "(0.6304347826086957, 0.3972602739726027, 0.8219916820526123)\n",
      "(0.6222222222222222, 0.3835616438356164, 0.822324275970459)\n",
      "(0.6136363636363636, 0.3698630136986301, 0.822806179523468)\n",
      "(0.6046511627906976, 0.3561643835616438, 0.8244763612747192)\n",
      "(0.6190476190476191, 0.3561643835616438, 0.8246634602546692)\n",
      "(0.6341463414634146, 0.3561643835616438, 0.8263704776763916)\n",
      "(0.625, 0.3424657534246575, 0.8280989527702332)\n",
      "(0.6153846153846154, 0.3287671232876712, 0.8310939073562622)\n",
      "(0.631578947368421, 0.3287671232876712, 0.8349303007125854)\n",
      "(0.6486486486486487, 0.3287671232876712, 0.8394303321838379)\n",
      "(0.6388888888888888, 0.3150684931506849, 0.8400290012359619)\n",
      "(0.6285714285714286, 0.3013698630136986, 0.8404654264450073)\n",
      "(0.6470588235294118, 0.3013698630136986, 0.8425318002700806)\n",
      "(0.6666666666666666, 0.3013698630136986, 0.8435350656509399)\n",
      "(0.65625, 0.2876712328767123, 0.845990777015686)\n",
      "(0.6451612903225806, 0.273972602739726, 0.8502142429351807)\n",
      "(0.6333333333333333, 0.2602739726027397, 0.8509646058082581)\n",
      "(0.6551724137931034, 0.2602739726027397, 0.8529031276702881)\n",
      "(0.6785714285714286, 0.2602739726027397, 0.8529170751571655)\n",
      "(0.7037037037037037, 0.2602739726027397, 0.8540318012237549)\n",
      "(0.7307692307692307, 0.2602739726027397, 0.8548195362091064)\n",
      "(0.72, 0.2465753424657534, 0.8601921796798706)\n",
      "(0.7083333333333334, 0.2328767123287671, 0.8609507083892822)\n",
      "(0.6956521739130435, 0.2191780821917808, 0.8612474799156189)\n",
      "(0.6818181818181818, 0.2054794520547945, 0.8634878396987915)\n",
      "(0.6666666666666666, 0.1917808219178082, 0.8651964068412781)\n",
      "(0.65, 0.1780821917808219, 0.8675111532211304)\n",
      "(0.6842105263157895, 0.1780821917808219, 0.8684133887290955)\n",
      "(0.6666666666666666, 0.1643835616438356, 0.8713773488998413)\n",
      "(0.6470588235294118, 0.1506849315068493, 0.8730831742286682)\n",
      "(0.625, 0.136986301369863, 0.8732292652130127)\n",
      "(0.6, 0.1232876712328767, 0.87529456615448)\n",
      "(0.6428571428571429, 0.1232876712328767, 0.8799703121185303)\n",
      "(0.6153846153846154, 0.1095890410958904, 0.880195140838623)\n",
      "(0.6666666666666666, 0.1095890410958904, 0.8855925798416138)\n",
      "(0.6363636363636364, 0.0958904109589041, 0.8856944441795349)\n",
      "(0.7, 0.0958904109589041, 0.8869450092315674)\n",
      "(0.7777777777777778, 0.0958904109589041, 0.8883732557296753)\n",
      "(0.75, 0.0821917808219178, 0.8884657025337219)\n",
      "(0.8571428571428571, 0.0821917808219178, 0.8885682821273804)\n",
      "(0.8333333333333334, 0.0684931506849315, 0.888595461845398)\n",
      "(0.8, 0.0547945205479452, 0.8911207914352417)\n",
      "(0.75, 0.0410958904109589, 0.9007571935653687)\n",
      "(1.0, 0.0410958904109589, 0.9012261629104614)\n",
      "(1.0, 0.0273972602739726, 0.9034098386764526)\n",
      "(1.0, 0.0136986301369863, 0.9253634810447693)\n",
      "(1.0, 0.0, 1.0)\n"
     ]
    }
   ],
   "source": [
    "for i in zip(precision,recall,np.append(thresholds,1)):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-06T19:15:05.178430Z",
     "start_time": "2019-12-06T19:15:05.169778Z"
    }
   },
   "source": [
    "## Compare Predictions with Actually Toxic Compounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-06T19:25:28.507083Z",
     "start_time": "2019-12-06T19:25:28.492575Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NR.PPAR.gamma</th>\n",
       "      <th>DNN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>NCGC00261662-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>NCGC00261924-01</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>NCGC00261052-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>NCGC00261683-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>NCGC00261058-01</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>NCGC00261215-01</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>NCGC00261367-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>NCGC00261392-01</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>NCGC00260696-01</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>NCGC00261071-01</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>NCGC00261764-01</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>NCGC00260795-01</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>NCGC00261332-01</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>NCGC00261840-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>NCGC00260979-01</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>NCGC00260993-01</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>NCGC00261065-01</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>NCGC00260875-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>NCGC00260947-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>NCGC00261155-01</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>NCGC00261643-01</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>NCGC00261737-01</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>NCGC00261583-01</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>NCGC00356984-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>NCGC00357046-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>NCGC00356974-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>NCGC00357007-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>NCGC00357094-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>NCGC00356956-01</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>NCGC00357111-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>NCGC00357051-01</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 NR.PPAR.gamma  DNN\n",
       "NCGC00261662-01              1    0\n",
       "NCGC00261924-01              1    1\n",
       "NCGC00261052-01              1    0\n",
       "NCGC00261683-01              1    0\n",
       "NCGC00261058-01              1    1\n",
       "NCGC00261215-01              1    1\n",
       "NCGC00261367-01              1    0\n",
       "NCGC00261392-01              1    1\n",
       "NCGC00260696-01              1    1\n",
       "NCGC00261071-01              1    1\n",
       "NCGC00261764-01              1    1\n",
       "NCGC00260795-01              1    1\n",
       "NCGC00261332-01              1    1\n",
       "NCGC00261840-01              1    0\n",
       "NCGC00260979-01              1    1\n",
       "NCGC00260993-01              1    1\n",
       "NCGC00261065-01              1    1\n",
       "NCGC00260875-01              1    0\n",
       "NCGC00260947-01              1    0\n",
       "NCGC00261155-01              1    1\n",
       "NCGC00261643-01              1    1\n",
       "NCGC00261737-01              1    1\n",
       "NCGC00261583-01              1    1\n",
       "NCGC00356984-01              1    0\n",
       "NCGC00357046-01              1    0\n",
       "NCGC00356974-01              1    0\n",
       "NCGC00357007-01              1    0\n",
       "NCGC00357094-01              1    0\n",
       "NCGC00356956-01              1    1\n",
       "NCGC00357111-01              1    0\n",
       "NCGC00357051-01              1    1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat=pd.DataFrame(y_hat_testing,columns=['DNN'],index=y_te[rows_te].index)\n",
    "compare_TP = pd.concat([y_te[target][rows_te].astype('int'), y_hat],axis=1)\n",
    "compare_TP[compare_TP[target]==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:metis] *",
   "language": "python",
   "name": "conda-env-metis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
