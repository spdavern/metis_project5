{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T22:37:44.745457Z",
     "start_time": "2019-12-09T22:37:44.738229Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import io\n",
    "# from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, f1_score,\\\n",
    "                            precision_score, recall_score, accuracy_score,\\\n",
    "                            average_precision_score, precision_recall_curve,\\\n",
    "                            matthews_corrcoef, roc_curve\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from src.helper_functions import load_data, get_model_perfs, init_model_perfs,\\\n",
    "                                 save_model, save_model_perfs, check_is_best,\\\n",
    "                                 read_model, evaluate_model_predictions,\\\n",
    "                                 update_model_perfs, check_and_save,\\\n",
    "                                 adjusted_classes, mcc\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T22:37:46.021815Z",
     "start_time": "2019-12-09T22:37:46.015410Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.5'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "from keras.callbacks import Callback\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T22:38:05.147586Z",
     "start_time": "2019-12-09T22:37:49.832296Z"
    }
   },
   "outputs": [],
   "source": [
    "x_tr, y_tr, x_te, y_te = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Neural Network\n",
    "\n",
    "Following the desciption in section 2.2.4 of the [DeepTox article](https://www.frontiersin.org/articles/10.3389/fenvs.2015.00080/full), I tried to use intermediate values in [Table 2](https://www.frontiersin.org/articles/10.3389/fenvs.2015.00080/full#T2) to build the neural network:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following [this question/answer](https://datascience.stackexchange.com/questions/45165/how-to-get-accuracy-f1-precision-and-recall-for-a-keras-model) and [this question/answer](https://stackoverflow.com/questions/54065733/how-to-employ-the-scikit-learn-evaluation-metrics-functions-with-keras-in-python) to implement usage of recall in model training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T00:31:25.480771Z",
     "start_time": "2019-12-03T00:31:25.327888Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/sean/anaconda3/envs/metis/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Users/sean/anaconda3/envs/metis/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "h0_sigmoid_activation (Dense (None, 1024)              1684480   \n",
      "_________________________________________________________________\n",
      "Dropout0 (Dropout)           (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "h1_sigmoid_activation (Dense (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "Dropout1 (Dropout)           (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "h2_sigmoid_activation (Dense (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "Dropout2 (Dropout)           (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 1025      \n",
      "=================================================================\n",
      "Total params: 3,784,705\n",
      "Trainable params: 3,784,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "drop_out=0.5    # DeepTox range: 0.5, 0.2, 0\n",
    "L2_reg = 0.0001 # Default = 0.01\n",
    "layers = 3      # DeepTox range: 1, 2, 3, 4\n",
    "act = 'sigmoid' # Consider sigmoid and tanh\n",
    "neurons = 1024  # DeepTox range: 1024, 2048, 4096, 8192, 16384\n",
    "# Info on decay: https://datascience.stackexchange.com/questions/26112/decay-parameter-in-keras-optimizers\n",
    "decay = 0       # DeepTox range: 10^-4, 10^-5, 10^-6\n",
    "learn_rate = 0.1  #Research appropriate range\n",
    "DNN = keras.Sequential()\n",
    "DNN.add(Dense(neurons, activation=act,input_shape=x_tr.shape[1:],name='h0_'+act+'_activation'))\n",
    "DNN.add(Dropout(rate=drop_out,name='Dropout0'))\n",
    "for i in range(1,layers):\n",
    "    DNN.add(Dense(units=neurons, activation=act,\\\n",
    "                  name='h'+str(i)+'_'+act+'_activation',\\\n",
    "                  kernel_regularizer=keras.regularizers.l2(L2_reg)))\n",
    "    DNN.add(Dropout(rate=drop_out,name='Dropout'+str(i)))\n",
    "DNN.add(Dense(units=1, activation='sigmoid'))\n",
    "keras.optimizers.Adam(lr=learn_rate, beta_1=0.9,\\\n",
    "                      beta_2=0.999, decay=decay, amsgrad=False)\n",
    "DNN.compile(optimizer='adam', loss='binary_crossentropy',\\\n",
    "            metrics=['accuracy'])\n",
    "DNN.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Train DNN's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T00:45:37.293279Z",
     "start_time": "2019-12-03T00:31:53.696343Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/sean/anaconda3/envs/metis/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 11934 samples, validate on 1689 samples\n",
      "Epoch 1/100\n",
      "11934/11934 [==============================] - 3s 227us/step - loss: 1.1133 - acc: 0.5215 - val_loss: 0.7070 - val_acc: 0.8840\n",
      "Epoch 2/100\n",
      "11934/11934 [==============================] - 2s 175us/step - loss: 0.8746 - acc: 0.5747 - val_loss: 0.8319 - val_acc: 0.4938\n",
      "Epoch 3/100\n",
      "11934/11934 [==============================] - 2s 181us/step - loss: 0.7788 - acc: 0.6629 - val_loss: 0.7846 - val_acc: 0.5595\n",
      "Epoch 4/100\n",
      "11934/11934 [==============================] - 3s 210us/step - loss: 0.7400 - acc: 0.6865 - val_loss: 0.6602 - val_acc: 0.7105\n",
      "Epoch 5/100\n",
      "11934/11934 [==============================] - 3s 210us/step - loss: 0.7156 - acc: 0.6886 - val_loss: 0.5235 - val_acc: 0.8502\n",
      "Epoch 6/100\n",
      "11934/11934 [==============================] - 2s 200us/step - loss: 0.6984 - acc: 0.7056 - val_loss: 0.8266 - val_acc: 0.5317\n",
      "Epoch 7/100\n",
      "11934/11934 [==============================] - 2s 201us/step - loss: 0.6794 - acc: 0.7106 - val_loss: 0.5944 - val_acc: 0.7821\n",
      "Epoch 8/100\n",
      "11934/11934 [==============================] - 2s 201us/step - loss: 0.6682 - acc: 0.7123 - val_loss: 0.6837 - val_acc: 0.6092\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 9/100\n",
      "11934/11934 [==============================] - 2s 203us/step - loss: 0.6513 - acc: 0.7200 - val_loss: 0.7557 - val_acc: 0.5921\n",
      "Epoch 10/100\n",
      "11934/11934 [==============================] - 2s 205us/step - loss: 0.6357 - acc: 0.7319 - val_loss: 0.6479 - val_acc: 0.6750\n",
      "Epoch 11/100\n",
      "11934/11934 [==============================] - 2s 205us/step - loss: 0.6234 - acc: 0.7360 - val_loss: 0.5659 - val_acc: 0.8040\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 12/100\n",
      "11934/11934 [==============================] - 2s 205us/step - loss: 0.6146 - acc: 0.7387 - val_loss: 0.5735 - val_acc: 0.7916\n",
      "Epoch 13/100\n",
      "11934/11934 [==============================] - 2s 203us/step - loss: 0.6016 - acc: 0.7479 - val_loss: 0.6328 - val_acc: 0.6809\n",
      "Epoch 14/100\n",
      "11934/11934 [==============================] - 2s 206us/step - loss: 0.5965 - acc: 0.7479 - val_loss: 0.5107 - val_acc: 0.8378\n",
      "Epoch 15/100\n",
      "11934/11934 [==============================] - 3s 214us/step - loss: 0.5918 - acc: 0.7517 - val_loss: 0.5822 - val_acc: 0.7762\n",
      "Epoch 16/100\n",
      "11934/11934 [==============================] - 3s 210us/step - loss: 0.5871 - acc: 0.7556 - val_loss: 0.6022 - val_acc: 0.7010\n",
      "Epoch 17/100\n",
      "11934/11934 [==============================] - 2s 208us/step - loss: 0.5775 - acc: 0.7600 - val_loss: 0.5301 - val_acc: 0.8188\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 18/100\n",
      "11934/11934 [==============================] - 2s 207us/step - loss: 0.5706 - acc: 0.7588 - val_loss: 0.5502 - val_acc: 0.7833\n",
      "Epoch 19/100\n",
      "11934/11934 [==============================] - 3s 211us/step - loss: 0.5669 - acc: 0.7599 - val_loss: 0.5472 - val_acc: 0.7709\n",
      "Epoch 20/100\n",
      "11934/11934 [==============================] - 3s 212us/step - loss: 0.5605 - acc: 0.7637 - val_loss: 0.5162 - val_acc: 0.8265\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 21/100\n",
      "11934/11934 [==============================] - 2s 209us/step - loss: 0.5540 - acc: 0.7702 - val_loss: 0.5447 - val_acc: 0.7910\n",
      "Epoch 22/100\n",
      "11934/11934 [==============================] - 3s 215us/step - loss: 0.5547 - acc: 0.7665 - val_loss: 0.5377 - val_acc: 0.7845\n",
      "Epoch 23/100\n",
      "11934/11934 [==============================] - 3s 215us/step - loss: 0.5521 - acc: 0.7691 - val_loss: 0.5375 - val_acc: 0.7904\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 24/100\n",
      "11934/11934 [==============================] - 3s 210us/step - loss: 0.5515 - acc: 0.7733 - val_loss: 0.5496 - val_acc: 0.7815\n",
      "Epoch 25/100\n",
      "11934/11934 [==============================] - 3s 212us/step - loss: 0.5456 - acc: 0.7753 - val_loss: 0.5423 - val_acc: 0.7975\n",
      "Epoch 26/100\n",
      "11934/11934 [==============================] - 3s 210us/step - loss: 0.5473 - acc: 0.7756 - val_loss: 0.5340 - val_acc: 0.8011\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "Epoch 27/100\n",
      "11934/11934 [==============================] - 3s 214us/step - loss: 0.5476 - acc: 0.7745 - val_loss: 0.5473 - val_acc: 0.7869\n",
      "Epoch 28/100\n",
      "11934/11934 [==============================] - 3s 215us/step - loss: 0.5416 - acc: 0.7775 - val_loss: 0.5436 - val_acc: 0.7946\n",
      "Epoch 29/100\n",
      "11934/11934 [==============================] - 2s 208us/step - loss: 0.5424 - acc: 0.7764 - val_loss: 0.5403 - val_acc: 0.8005\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "Epoch 30/100\n",
      "11934/11934 [==============================] - 2s 205us/step - loss: 0.5402 - acc: 0.7745 - val_loss: 0.5377 - val_acc: 0.8034\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00030: early stopping\n",
      "         NR.AhR: 0.86472\n",
      "Model saved and metrics table updated.\n",
      "Model saved and metrics table updated.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>threshold</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>auc_roc</th>\n",
       "      <th>avg_precision</th>\n",
       "      <th>confusion_matrix</th>\n",
       "      <th>model_filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.911475</td>\n",
       "      <td>0.756757</td>\n",
       "      <td>0.383562</td>\n",
       "      <td>0.509091</td>\n",
       "      <td>0.905028</td>\n",
       "      <td>0.602102</td>\n",
       "      <td>[[528, 9], [45, 28]]</td>\n",
       "      <td>RF0.joblib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>RF_modT</td>\n",
       "      <td>0.235000</td>\n",
       "      <td>0.842623</td>\n",
       "      <td>0.424837</td>\n",
       "      <td>0.890411</td>\n",
       "      <td>0.575221</td>\n",
       "      <td>0.905028</td>\n",
       "      <td>0.602102</td>\n",
       "      <td>[[449, 88], [8, 65]]</td>\n",
       "      <td>RF_modT0.joblib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>DNN</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.814754</td>\n",
       "      <td>0.364865</td>\n",
       "      <td>0.739726</td>\n",
       "      <td>0.488688</td>\n",
       "      <td>0.864723</td>\n",
       "      <td>0.524576</td>\n",
       "      <td>[[443, 94], [19, 54]]</td>\n",
       "      <td>DNN0.h5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>DNN_modT</td>\n",
       "      <td>0.773516</td>\n",
       "      <td>0.901639</td>\n",
       "      <td>0.603175</td>\n",
       "      <td>0.520548</td>\n",
       "      <td>0.558824</td>\n",
       "      <td>0.864723</td>\n",
       "      <td>0.524576</td>\n",
       "      <td>[[512, 25], [35, 38]]</td>\n",
       "      <td>DNN_modT0.h5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      model  threshold  accuracy  precision    recall        f1   auc_roc  \\\n",
       "0        RF   0.500000  0.911475   0.756757  0.383562  0.509091  0.905028   \n",
       "1   RF_modT   0.235000  0.842623   0.424837  0.890411  0.575221  0.905028   \n",
       "2       DNN   0.500000  0.814754   0.364865  0.739726  0.488688  0.864723   \n",
       "3  DNN_modT   0.773516  0.901639   0.603175  0.520548  0.558824  0.864723   \n",
       "\n",
       "   avg_precision       confusion_matrix   model_filename  \n",
       "0       0.602102   [[528, 9], [45, 28]]       RF0.joblib  \n",
       "1       0.602102   [[449, 88], [8, 65]]  RF_modT0.joblib  \n",
       "2       0.524576  [[443, 94], [19, 54]]          DNN0.h5  \n",
       "3       0.524576  [[512, 25], [35, 38]]     DNN_modT0.h5  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14834 samples, validate on 1931 samples\n",
      "Epoch 1/100\n",
      "14834/14834 [==============================] - 3s 221us/step - loss: 1.0468 - acc: 0.4847 - val_loss: 0.9818 - val_acc: 0.4443\n",
      "Epoch 2/100\n",
      "14834/14834 [==============================] - 3s 221us/step - loss: 0.9139 - acc: 0.5450 - val_loss: 0.9631 - val_acc: 0.4459\n",
      "Epoch 3/100\n",
      "14834/14834 [==============================] - 3s 215us/step - loss: 0.8575 - acc: 0.5810 - val_loss: 0.8899 - val_acc: 0.4858\n",
      "Epoch 4/100\n",
      "14834/14834 [==============================] - 3s 217us/step - loss: 0.8338 - acc: 0.5866 - val_loss: 0.8453 - val_acc: 0.5049\n",
      "Epoch 5/100\n",
      "14834/14834 [==============================] - 3s 217us/step - loss: 0.8115 - acc: 0.6066 - val_loss: 0.8269 - val_acc: 0.5308\n",
      "Epoch 6/100\n",
      "14834/14834 [==============================] - 3s 209us/step - loss: 0.7909 - acc: 0.6201 - val_loss: 0.8087 - val_acc: 0.5526\n",
      "Epoch 7/100\n",
      "14834/14834 [==============================] - 3s 212us/step - loss: 0.7809 - acc: 0.6283 - val_loss: 0.7948 - val_acc: 0.5769\n",
      "Epoch 8/100\n",
      "14834/14834 [==============================] - 3s 207us/step - loss: 0.7699 - acc: 0.6393 - val_loss: 0.7865 - val_acc: 0.6100\n",
      "Epoch 9/100\n",
      "14834/14834 [==============================] - 3s 209us/step - loss: 0.7588 - acc: 0.6507 - val_loss: 0.7722 - val_acc: 0.6277\n",
      "Epoch 10/100\n",
      "14834/14834 [==============================] - 3s 212us/step - loss: 0.7521 - acc: 0.6536 - val_loss: 0.7701 - val_acc: 0.6282\n",
      "Epoch 11/100\n",
      "14834/14834 [==============================] - 3s 209us/step - loss: 0.7520 - acc: 0.6553 - val_loss: 0.7625 - val_acc: 0.6308\n",
      "Epoch 12/100\n",
      "14834/14834 [==============================] - 3s 207us/step - loss: 0.7426 - acc: 0.6642 - val_loss: 0.7514 - val_acc: 0.6349\n",
      "Epoch 13/100\n",
      "14834/14834 [==============================] - 3s 208us/step - loss: 0.7341 - acc: 0.6741 - val_loss: 0.7471 - val_acc: 0.6359\n",
      "Epoch 14/100\n",
      "14834/14834 [==============================] - 3s 211us/step - loss: 0.7322 - acc: 0.6757 - val_loss: 0.7426 - val_acc: 0.6401\n",
      "Epoch 15/100\n",
      "14834/14834 [==============================] - 3s 205us/step - loss: 0.7234 - acc: 0.6853 - val_loss: 0.7338 - val_acc: 0.6432\n",
      "Epoch 16/100\n",
      "14834/14834 [==============================] - 3s 203us/step - loss: 0.7211 - acc: 0.6891 - val_loss: 0.7311 - val_acc: 0.6432\n",
      "Epoch 17/100\n",
      "14834/14834 [==============================] - 3s 208us/step - loss: 0.7185 - acc: 0.6904 - val_loss: 0.7301 - val_acc: 0.6484\n",
      "Epoch 18/100\n",
      "14834/14834 [==============================] - 3s 204us/step - loss: 0.7134 - acc: 0.6927 - val_loss: 0.7207 - val_acc: 0.6530\n",
      "Epoch 19/100\n",
      "14834/14834 [==============================] - 3s 204us/step - loss: 0.7119 - acc: 0.6976 - val_loss: 0.7194 - val_acc: 0.6546\n",
      "Epoch 20/100\n",
      "14834/14834 [==============================] - 3s 205us/step - loss: 0.7103 - acc: 0.6972 - val_loss: 0.7152 - val_acc: 0.6587\n",
      "Epoch 21/100\n",
      "14834/14834 [==============================] - 3s 201us/step - loss: 0.7053 - acc: 0.7008 - val_loss: 0.7118 - val_acc: 0.6598\n",
      "Epoch 22/100\n",
      "14834/14834 [==============================] - 3s 219us/step - loss: 0.7015 - acc: 0.7063 - val_loss: 0.7116 - val_acc: 0.6608\n",
      "Epoch 23/100\n",
      "14834/14834 [==============================] - 3s 205us/step - loss: 0.7019 - acc: 0.7053 - val_loss: 0.7125 - val_acc: 0.6613\n",
      "Epoch 24/100\n",
      "14834/14834 [==============================] - 3s 212us/step - loss: 0.6991 - acc: 0.7087 - val_loss: 0.7069 - val_acc: 0.6639\n",
      "Epoch 25/100\n",
      "14834/14834 [==============================] - 3s 202us/step - loss: 0.6962 - acc: 0.7134 - val_loss: 0.7000 - val_acc: 0.6665\n",
      "Epoch 26/100\n",
      "14834/14834 [==============================] - 3s 204us/step - loss: 0.6957 - acc: 0.7093 - val_loss: 0.7000 - val_acc: 0.6655\n",
      "Epoch 27/100\n",
      "14834/14834 [==============================] - 3s 200us/step - loss: 0.6924 - acc: 0.7108 - val_loss: 0.6999 - val_acc: 0.6649\n",
      "Epoch 28/100\n",
      "14834/14834 [==============================] - 3s 207us/step - loss: 0.6893 - acc: 0.7152 - val_loss: 0.6927 - val_acc: 0.6696\n",
      "Epoch 29/100\n",
      "14834/14834 [==============================] - 3s 212us/step - loss: 0.6886 - acc: 0.7161 - val_loss: 0.6924 - val_acc: 0.6706\n",
      "Epoch 30/100\n",
      "14834/14834 [==============================] - 3s 203us/step - loss: 0.6873 - acc: 0.7136 - val_loss: 0.6918 - val_acc: 0.6712\n",
      "Epoch 31/100\n",
      "14834/14834 [==============================] - 3s 202us/step - loss: 0.6852 - acc: 0.7179 - val_loss: 0.6865 - val_acc: 0.6743\n",
      "Epoch 32/100\n",
      "14834/14834 [==============================] - 3s 201us/step - loss: 0.6816 - acc: 0.7201 - val_loss: 0.6897 - val_acc: 0.6727\n",
      "Epoch 33/100\n",
      "14834/14834 [==============================] - 3s 211us/step - loss: 0.6827 - acc: 0.7188 - val_loss: 0.6831 - val_acc: 0.6774\n",
      "Epoch 34/100\n",
      "14834/14834 [==============================] - 3s 218us/step - loss: 0.6815 - acc: 0.7245 - val_loss: 0.6882 - val_acc: 0.6753\n",
      "Epoch 35/100\n",
      "14834/14834 [==============================] - 3s 200us/step - loss: 0.6784 - acc: 0.7219 - val_loss: 0.6830 - val_acc: 0.6789\n",
      "Epoch 36/100\n",
      "14834/14834 [==============================] - 3s 205us/step - loss: 0.6777 - acc: 0.7226 - val_loss: 0.6808 - val_acc: 0.6800\n",
      "Epoch 37/100\n",
      "14834/14834 [==============================] - 3s 201us/step - loss: 0.6747 - acc: 0.7241 - val_loss: 0.6827 - val_acc: 0.6794\n",
      "Epoch 38/100\n",
      "14834/14834 [==============================] - 3s 202us/step - loss: 0.6711 - acc: 0.7267 - val_loss: 0.6767 - val_acc: 0.6815\n",
      "Epoch 39/100\n",
      "14834/14834 [==============================] - 3s 211us/step - loss: 0.6739 - acc: 0.7264 - val_loss: 0.6728 - val_acc: 0.6836\n",
      "Epoch 40/100\n",
      "14834/14834 [==============================] - 3s 218us/step - loss: 0.6725 - acc: 0.7226 - val_loss: 0.6704 - val_acc: 0.6846\n",
      "Epoch 41/100\n",
      "14834/14834 [==============================] - 3s 221us/step - loss: 0.6710 - acc: 0.7265 - val_loss: 0.6765 - val_acc: 0.6831\n",
      "Epoch 42/100\n",
      "14834/14834 [==============================] - 3s 209us/step - loss: 0.6683 - acc: 0.7299 - val_loss: 0.6699 - val_acc: 0.6882\n",
      "Epoch 43/100\n",
      "14834/14834 [==============================] - 3s 214us/step - loss: 0.6661 - acc: 0.7270 - val_loss: 0.6664 - val_acc: 0.6898\n",
      "Epoch 44/100\n",
      "14834/14834 [==============================] - 3s 200us/step - loss: 0.6609 - acc: 0.7363 - val_loss: 0.6657 - val_acc: 0.6919\n",
      "Epoch 45/100\n",
      "14834/14834 [==============================] - 3s 206us/step - loss: 0.6661 - acc: 0.7308 - val_loss: 0.6673 - val_acc: 0.6914\n",
      "Epoch 46/100\n",
      "14834/14834 [==============================] - 3s 205us/step - loss: 0.6634 - acc: 0.7304 - val_loss: 0.6627 - val_acc: 0.6929\n",
      "Epoch 47/100\n",
      "14834/14834 [==============================] - 3s 202us/step - loss: 0.6588 - acc: 0.7340 - val_loss: 0.6579 - val_acc: 0.6950\n",
      "Epoch 48/100\n",
      "14834/14834 [==============================] - 3s 207us/step - loss: 0.6595 - acc: 0.7340 - val_loss: 0.6645 - val_acc: 0.6929\n",
      "Epoch 49/100\n",
      "14834/14834 [==============================] - 3s 211us/step - loss: 0.6568 - acc: 0.7337 - val_loss: 0.6578 - val_acc: 0.6965\n",
      "Epoch 50/100\n",
      "14834/14834 [==============================] - 3s 207us/step - loss: 0.6545 - acc: 0.7350 - val_loss: 0.6500 - val_acc: 0.6996\n",
      "Epoch 51/100\n",
      "14834/14834 [==============================] - 3s 206us/step - loss: 0.6552 - acc: 0.7355 - val_loss: 0.6539 - val_acc: 0.6981\n",
      "Epoch 52/100\n",
      "14834/14834 [==============================] - 3s 206us/step - loss: 0.6546 - acc: 0.7355 - val_loss: 0.6441 - val_acc: 0.7033\n",
      "Epoch 53/100\n",
      "14834/14834 [==============================] - 3s 204us/step - loss: 0.6525 - acc: 0.7329 - val_loss: 0.6566 - val_acc: 0.6991\n",
      "Epoch 54/100\n",
      "14834/14834 [==============================] - 3s 201us/step - loss: 0.6486 - acc: 0.7361 - val_loss: 0.6474 - val_acc: 0.7027\n",
      "Epoch 55/100\n",
      "14834/14834 [==============================] - 3s 213us/step - loss: 0.6481 - acc: 0.7407 - val_loss: 0.6474 - val_acc: 0.7043\n",
      "\n",
      "Epoch 00055: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "Epoch 56/100\n",
      "14834/14834 [==============================] - 3s 202us/step - loss: 0.6482 - acc: 0.7383 - val_loss: 0.6472 - val_acc: 0.7038\n",
      "Epoch 57/100\n",
      "14834/14834 [==============================] - 3s 204us/step - loss: 0.6509 - acc: 0.7384 - val_loss: 0.6416 - val_acc: 0.7074\n",
      "Epoch 58/100\n",
      "14834/14834 [==============================] - 3s 202us/step - loss: 0.6473 - acc: 0.7371 - val_loss: 0.6424 - val_acc: 0.7090\n",
      "Epoch 59/100\n",
      "14834/14834 [==============================] - 3s 220us/step - loss: 0.6479 - acc: 0.7403 - val_loss: 0.6392 - val_acc: 0.7095\n",
      "Epoch 60/100\n",
      "14834/14834 [==============================] - 3s 203us/step - loss: 0.6470 - acc: 0.7397 - val_loss: 0.6413 - val_acc: 0.7084\n",
      "Epoch 61/100\n",
      "14834/14834 [==============================] - 3s 203us/step - loss: 0.6461 - acc: 0.7375 - val_loss: 0.6384 - val_acc: 0.7100\n",
      "Epoch 62/100\n",
      "14834/14834 [==============================] - 3s 205us/step - loss: 0.6441 - acc: 0.7381 - val_loss: 0.6373 - val_acc: 0.7100\n",
      "Epoch 63/100\n",
      "14834/14834 [==============================] - 3s 214us/step - loss: 0.6458 - acc: 0.7405 - val_loss: 0.6407 - val_acc: 0.7090\n",
      "Epoch 64/100\n",
      "14834/14834 [==============================] - 3s 209us/step - loss: 0.6437 - acc: 0.7392 - val_loss: 0.6404 - val_acc: 0.7084\n",
      "Epoch 65/100\n",
      "14834/14834 [==============================] - 3s 207us/step - loss: 0.6416 - acc: 0.7410 - val_loss: 0.6358 - val_acc: 0.7141\n",
      "Epoch 66/100\n",
      "14834/14834 [==============================] - 3s 208us/step - loss: 0.6403 - acc: 0.7414 - val_loss: 0.6347 - val_acc: 0.7147\n",
      "Epoch 67/100\n",
      "14834/14834 [==============================] - 3s 203us/step - loss: 0.6412 - acc: 0.7417 - val_loss: 0.6335 - val_acc: 0.7147\n",
      "Epoch 68/100\n",
      "14834/14834 [==============================] - 3s 203us/step - loss: 0.6412 - acc: 0.7392 - val_loss: 0.6373 - val_acc: 0.7136\n",
      "Epoch 69/100\n",
      "14834/14834 [==============================] - 3s 201us/step - loss: 0.6404 - acc: 0.7396 - val_loss: 0.6307 - val_acc: 0.7157\n",
      "Epoch 70/100\n",
      "14834/14834 [==============================] - 3s 206us/step - loss: 0.6370 - acc: 0.7425 - val_loss: 0.6339 - val_acc: 0.7141\n",
      "Epoch 71/100\n",
      "14834/14834 [==============================] - 3s 204us/step - loss: 0.6425 - acc: 0.7370 - val_loss: 0.6329 - val_acc: 0.7141\n",
      "Epoch 72/100\n",
      "14834/14834 [==============================] - 3s 207us/step - loss: 0.6358 - acc: 0.7437 - val_loss: 0.6319 - val_acc: 0.7162\n",
      "\n",
      "Epoch 00072: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "Epoch 73/100\n",
      "14834/14834 [==============================] - 3s 200us/step - loss: 0.6385 - acc: 0.7396 - val_loss: 0.6294 - val_acc: 0.7172\n",
      "Epoch 74/100\n",
      "14834/14834 [==============================] - 3s 204us/step - loss: 0.6360 - acc: 0.7432 - val_loss: 0.6295 - val_acc: 0.7172\n",
      "Epoch 75/100\n",
      "14834/14834 [==============================] - 3s 202us/step - loss: 0.6371 - acc: 0.7459 - val_loss: 0.6288 - val_acc: 0.7178\n",
      "Epoch 76/100\n",
      "14834/14834 [==============================] - 3s 210us/step - loss: 0.6363 - acc: 0.7440 - val_loss: 0.6299 - val_acc: 0.7178\n",
      "Epoch 77/100\n",
      "14834/14834 [==============================] - 3s 204us/step - loss: 0.6368 - acc: 0.7387 - val_loss: 0.6263 - val_acc: 0.7188\n",
      "Epoch 78/100\n",
      "14834/14834 [==============================] - 3s 205us/step - loss: 0.6379 - acc: 0.7449 - val_loss: 0.6283 - val_acc: 0.7183\n",
      "Epoch 79/100\n",
      "14834/14834 [==============================] - 3s 207us/step - loss: 0.6361 - acc: 0.7454 - val_loss: 0.6271 - val_acc: 0.7193\n",
      "Epoch 80/100\n",
      "14834/14834 [==============================] - 3s 207us/step - loss: 0.6364 - acc: 0.7444 - val_loss: 0.6230 - val_acc: 0.7214\n",
      "Epoch 81/100\n",
      "14834/14834 [==============================] - 3s 201us/step - loss: 0.6374 - acc: 0.7439 - val_loss: 0.6251 - val_acc: 0.7204\n",
      "Epoch 82/100\n",
      "14834/14834 [==============================] - 3s 205us/step - loss: 0.6359 - acc: 0.7435 - val_loss: 0.6254 - val_acc: 0.7198\n",
      "Epoch 83/100\n",
      "14834/14834 [==============================] - 3s 203us/step - loss: 0.6355 - acc: 0.7449 - val_loss: 0.6255 - val_acc: 0.7204\n",
      "\n",
      "Epoch 00083: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
      "Epoch 84/100\n",
      "14834/14834 [==============================] - 3s 204us/step - loss: 0.6337 - acc: 0.7447 - val_loss: 0.6267 - val_acc: 0.7193\n",
      "Epoch 85/100\n",
      "14834/14834 [==============================] - 3s 205us/step - loss: 0.6345 - acc: 0.7437 - val_loss: 0.6248 - val_acc: 0.7209\n",
      "Epoch 86/100\n",
      "14834/14834 [==============================] - 3s 206us/step - loss: 0.6341 - acc: 0.7436 - val_loss: 0.6255 - val_acc: 0.7204\n",
      "\n",
      "Epoch 00086: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
      "Epoch 87/100\n",
      "14834/14834 [==============================] - 3s 202us/step - loss: 0.6353 - acc: 0.7441 - val_loss: 0.6252 - val_acc: 0.7209\n",
      "Epoch 88/100\n",
      "14834/14834 [==============================] - 3s 208us/step - loss: 0.6361 - acc: 0.7421 - val_loss: 0.6250 - val_acc: 0.7214\n",
      "Epoch 89/100\n",
      "14834/14834 [==============================] - 3s 208us/step - loss: 0.6325 - acc: 0.7479 - val_loss: 0.6248 - val_acc: 0.7214\n",
      "\n",
      "Epoch 00089: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
      "Epoch 90/100\n",
      "14834/14834 [==============================] - 3s 204us/step - loss: 0.6316 - acc: 0.7439 - val_loss: 0.6247 - val_acc: 0.7214\n",
      "Epoch 91/100\n",
      "14834/14834 [==============================] - 3s 203us/step - loss: 0.6362 - acc: 0.7445 - val_loss: 0.6245 - val_acc: 0.7219\n",
      "Epoch 92/100\n",
      "14834/14834 [==============================] - 3s 210us/step - loss: 0.6372 - acc: 0.7421 - val_loss: 0.6245 - val_acc: 0.7219\n",
      "\n",
      "Epoch 00092: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.\n",
      "Epoch 93/100\n",
      "14834/14834 [==============================] - 3s 205us/step - loss: 0.6326 - acc: 0.7457 - val_loss: 0.6245 - val_acc: 0.7214\n",
      "Epoch 94/100\n",
      "14834/14834 [==============================] - 3s 207us/step - loss: 0.6328 - acc: 0.7454 - val_loss: 0.6247 - val_acc: 0.7214\n",
      "Epoch 95/100\n",
      "14834/14834 [==============================] - 3s 208us/step - loss: 0.6340 - acc: 0.7419 - val_loss: 0.6246 - val_acc: 0.7214\n",
      "\n",
      "Epoch 00095: ReduceLROnPlateau reducing learning rate to 6.103515914901436e-08.\n",
      "Epoch 96/100\n",
      "14834/14834 [==============================] - 3s 207us/step - loss: 0.6356 - acc: 0.7445 - val_loss: 0.6246 - val_acc: 0.7219\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00096: early stopping\n",
      "          NR.AR: 0.74151\n",
      "Model saved and metrics table updated.\n",
      "Model saved and metrics table updated.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>threshold</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>auc_roc</th>\n",
       "      <th>avg_precision</th>\n",
       "      <th>confusion_matrix</th>\n",
       "      <th>model_filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.981229</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.678934</td>\n",
       "      <td>0.232723</td>\n",
       "      <td>[[573, 1], [10, 2]]</td>\n",
       "      <td>RF0.joblib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>RF_modT</td>\n",
       "      <td>0.730000</td>\n",
       "      <td>0.982935</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.678934</td>\n",
       "      <td>0.232723</td>\n",
       "      <td>[[574, 0], [10, 2]]</td>\n",
       "      <td>RF_modT0.joblib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>DNN</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.721843</td>\n",
       "      <td>0.042424</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.079096</td>\n",
       "      <td>0.741507</td>\n",
       "      <td>0.168663</td>\n",
       "      <td>[[416, 158], [5, 7]]</td>\n",
       "      <td>DNN0.h5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>DNN_modT</td>\n",
       "      <td>0.792447</td>\n",
       "      <td>0.965870</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.741507</td>\n",
       "      <td>0.168663</td>\n",
       "      <td>[[562, 12], [8, 4]]</td>\n",
       "      <td>DNN_modT0.h5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      model  threshold  accuracy  precision    recall        f1   auc_roc  \\\n",
       "0        RF   0.500000  0.981229   0.666667  0.166667  0.266667  0.678934   \n",
       "1   RF_modT   0.730000  0.982935   1.000000  0.166667  0.285714  0.678934   \n",
       "2       DNN   0.500000  0.721843   0.042424  0.583333  0.079096  0.741507   \n",
       "3  DNN_modT   0.792447  0.965870   0.250000  0.333333  0.285714  0.741507   \n",
       "\n",
       "   avg_precision      confusion_matrix   model_filename  \n",
       "0       0.232723   [[573, 1], [10, 2]]       RF0.joblib  \n",
       "1       0.232723   [[574, 0], [10, 2]]  RF_modT0.joblib  \n",
       "2       0.168663  [[416, 158], [5, 7]]          DNN0.h5  \n",
       "3       0.168663   [[562, 12], [8, 4]]     DNN_modT0.h5  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13670 samples, validate on 1771 samples\n",
      "Epoch 1/100\n",
      "13670/13670 [==============================] - 3s 207us/step - loss: 0.6926 - acc: 0.7134 - val_loss: 0.5940 - val_acc: 0.7493\n",
      "Epoch 2/100\n",
      "13670/13670 [==============================] - 3s 214us/step - loss: 0.6916 - acc: 0.7143 - val_loss: 0.5947 - val_acc: 0.7493\n",
      "Epoch 3/100\n",
      "13670/13670 [==============================] - 3s 212us/step - loss: 0.6977 - acc: 0.7115 - val_loss: 0.5955 - val_acc: 0.7487\n",
      "Epoch 4/100\n",
      "13670/13670 [==============================] - 3s 211us/step - loss: 0.6946 - acc: 0.7108 - val_loss: 0.5961 - val_acc: 0.7487\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 3.051757957450718e-08.\n",
      "Epoch 5/100\n",
      "13670/13670 [==============================] - 3s 210us/step - loss: 0.6950 - acc: 0.7102 - val_loss: 0.5964 - val_acc: 0.7487\n",
      "Epoch 6/100\n",
      "13670/13670 [==============================] - 3s 212us/step - loss: 0.6970 - acc: 0.7123 - val_loss: 0.5967 - val_acc: 0.7487\n",
      "Epoch 7/100\n",
      "13670/13670 [==============================] - 3s 208us/step - loss: 0.6924 - acc: 0.7127 - val_loss: 0.5971 - val_acc: 0.7487\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 1.525878978725359e-08.\n",
      "Epoch 8/100\n",
      "13670/13670 [==============================] - 3s 214us/step - loss: 0.6927 - acc: 0.7119 - val_loss: 0.5972 - val_acc: 0.7487\n",
      "Epoch 9/100\n",
      "13670/13670 [==============================] - 3s 212us/step - loss: 0.6910 - acc: 0.7113 - val_loss: 0.5974 - val_acc: 0.7487\n",
      "Epoch 10/100\n",
      "13670/13670 [==============================] - 3s 212us/step - loss: 0.6926 - acc: 0.7102 - val_loss: 0.5976 - val_acc: 0.7487\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 7.629394893626795e-09.\n",
      "Epoch 11/100\n",
      "13670/13670 [==============================] - 3s 209us/step - loss: 0.6916 - acc: 0.7091 - val_loss: 0.5976 - val_acc: 0.7487\n",
      "Epoch 12/100\n",
      "13670/13670 [==============================] - 3s 208us/step - loss: 0.6926 - acc: 0.7125 - val_loss: 0.5977 - val_acc: 0.7487\n",
      "Epoch 13/100\n",
      "13670/13670 [==============================] - 3s 204us/step - loss: 0.6938 - acc: 0.7119 - val_loss: 0.5978 - val_acc: 0.7487\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 3.814697446813398e-09.\n",
      "Epoch 14/100\n",
      "13670/13670 [==============================] - 3s 206us/step - loss: 0.6929 - acc: 0.7125 - val_loss: 0.5978 - val_acc: 0.7487\n",
      "Epoch 15/100\n",
      "13670/13670 [==============================] - 3s 212us/step - loss: 0.6896 - acc: 0.7121 - val_loss: 0.5979 - val_acc: 0.7487\n",
      "Epoch 16/100\n",
      "13670/13670 [==============================] - 3s 209us/step - loss: 0.6905 - acc: 0.7138 - val_loss: 0.5979 - val_acc: 0.7487\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.907348723406699e-09.\n",
      "Epoch 17/100\n",
      "13670/13670 [==============================] - 3s 207us/step - loss: 0.6933 - acc: 0.7110 - val_loss: 0.5979 - val_acc: 0.7487\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00017: early stopping\n",
      "      NR.AR.LBD: 0.63513\n",
      "Model saved and metrics table updated.\n",
      "Model saved and metrics table updated.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>threshold</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>auc_roc</th>\n",
       "      <th>avg_precision</th>\n",
       "      <th>confusion_matrix</th>\n",
       "      <th>model_filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.982818</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.763284</td>\n",
       "      <td>0.060344</td>\n",
       "      <td>[[572, 2], [8, 0]]</td>\n",
       "      <td>RF0.joblib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>RF_modT</td>\n",
       "      <td>0.270000</td>\n",
       "      <td>0.979381</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.763284</td>\n",
       "      <td>0.060344</td>\n",
       "      <td>[[569, 5], [7, 1]]</td>\n",
       "      <td>RF_modT0.joblib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>DNN</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.738832</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.037975</td>\n",
       "      <td>0.635126</td>\n",
       "      <td>0.030153</td>\n",
       "      <td>[[427, 147], [5, 3]]</td>\n",
       "      <td>DNN0.h5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>DNN_modT</td>\n",
       "      <td>0.762549</td>\n",
       "      <td>0.929553</td>\n",
       "      <td>0.054054</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.088889</td>\n",
       "      <td>0.635126</td>\n",
       "      <td>0.030153</td>\n",
       "      <td>[[539, 35], [6, 2]]</td>\n",
       "      <td>DNN_modT0.h5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      model  threshold  accuracy  precision  recall        f1   auc_roc  \\\n",
       "0        RF   0.500000  0.982818   0.000000   0.000  0.000000  0.763284   \n",
       "1   RF_modT   0.270000  0.979381   0.166667   0.125  0.142857  0.763284   \n",
       "2       DNN   0.500000  0.738832   0.020000   0.375  0.037975  0.635126   \n",
       "3  DNN_modT   0.762549  0.929553   0.054054   0.250  0.088889  0.635126   \n",
       "\n",
       "   avg_precision      confusion_matrix   model_filename  \n",
       "0       0.060344    [[572, 2], [8, 0]]       RF0.joblib  \n",
       "1       0.060344    [[569, 5], [7, 1]]  RF_modT0.joblib  \n",
       "2       0.030153  [[427, 147], [5, 3]]          DNN0.h5  \n",
       "3       0.030153   [[539, 35], [6, 2]]     DNN_modT0.h5  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11300 samples, validate on 1488 samples\n",
      "Epoch 1/100\n",
      "11300/11300 [==============================] - 2s 201us/step - loss: 0.7350 - acc: 0.6379 - val_loss: 0.5618 - val_acc: 0.7708\n",
      "Epoch 2/100\n",
      "11300/11300 [==============================] - 2s 209us/step - loss: 0.7377 - acc: 0.6377 - val_loss: 0.5619 - val_acc: 0.7708\n",
      "Epoch 3/100\n",
      "11300/11300 [==============================] - 2s 214us/step - loss: 0.7319 - acc: 0.6388 - val_loss: 0.5619 - val_acc: 0.7708\n",
      "Epoch 4/100\n",
      "11300/11300 [==============================] - 2s 210us/step - loss: 0.7355 - acc: 0.6370 - val_loss: 0.5620 - val_acc: 0.7708\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 9.536743617033494e-10.\n",
      "Epoch 5/100\n",
      "11300/11300 [==============================] - 2s 213us/step - loss: 0.7381 - acc: 0.6356 - val_loss: 0.5620 - val_acc: 0.7708\n",
      "Epoch 6/100\n",
      "11300/11300 [==============================] - 2s 214us/step - loss: 0.7360 - acc: 0.6382 - val_loss: 0.5620 - val_acc: 0.7708\n",
      "Epoch 7/100\n",
      "11300/11300 [==============================] - 2s 215us/step - loss: 0.7324 - acc: 0.6381 - val_loss: 0.5621 - val_acc: 0.7702\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 4.768371808516747e-10.\n",
      "Epoch 8/100\n",
      "11300/11300 [==============================] - 2s 215us/step - loss: 0.7318 - acc: 0.6371 - val_loss: 0.5621 - val_acc: 0.7702\n",
      "Epoch 9/100\n",
      "11300/11300 [==============================] - 2s 215us/step - loss: 0.7365 - acc: 0.6379 - val_loss: 0.5621 - val_acc: 0.7702\n",
      "Epoch 10/100\n",
      "11300/11300 [==============================] - 2s 212us/step - loss: 0.7357 - acc: 0.6411 - val_loss: 0.5621 - val_acc: 0.7702\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 2.3841859042583735e-10.\n",
      "Epoch 11/100\n",
      "11300/11300 [==============================] - 3s 224us/step - loss: 0.7339 - acc: 0.6389 - val_loss: 0.5621 - val_acc: 0.7702\n",
      "Epoch 12/100\n",
      "11300/11300 [==============================] - 2s 219us/step - loss: 0.7361 - acc: 0.6339 - val_loss: 0.5621 - val_acc: 0.7702\n",
      "Epoch 13/100\n",
      "11300/11300 [==============================] - 2s 215us/step - loss: 0.7371 - acc: 0.6361 - val_loss: 0.5621 - val_acc: 0.7702\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 1.1920929521291868e-10.\n",
      "Epoch 14/100\n",
      "11300/11300 [==============================] - 2s 214us/step - loss: 0.7354 - acc: 0.6356 - val_loss: 0.5621 - val_acc: 0.7702\n",
      "Epoch 15/100\n",
      "11300/11300 [==============================] - 2s 214us/step - loss: 0.7362 - acc: 0.6336 - val_loss: 0.5621 - val_acc: 0.7702\n",
      "Epoch 16/100\n",
      "11300/11300 [==============================] - 2s 204us/step - loss: 0.7342 - acc: 0.6396 - val_loss: 0.5621 - val_acc: 0.7702\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 5.960464760645934e-11.\n",
      "Epoch 17/100\n",
      "11300/11300 [==============================] - 2s 210us/step - loss: 0.7347 - acc: 0.6350 - val_loss: 0.5621 - val_acc: 0.7702\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00017: early stopping\n",
      "   NR.Aromatase: 0.71816\n",
      "Model saved and metrics table updated.\n",
      "Model saved and metrics table updated.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>threshold</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>auc_roc</th>\n",
       "      <th>avg_precision</th>\n",
       "      <th>confusion_matrix</th>\n",
       "      <th>model_filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.928030</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.771931</td>\n",
       "      <td>0.311884</td>\n",
       "      <td>[[489, 0], [38, 1]]</td>\n",
       "      <td>RF0.joblib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>RF_modT</td>\n",
       "      <td>0.130000</td>\n",
       "      <td>0.840909</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.512821</td>\n",
       "      <td>0.322581</td>\n",
       "      <td>0.771931</td>\n",
       "      <td>0.311884</td>\n",
       "      <td>[[424, 65], [19, 20]]</td>\n",
       "      <td>RF_modT0.joblib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>DNN</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.772727</td>\n",
       "      <td>0.159664</td>\n",
       "      <td>0.487179</td>\n",
       "      <td>0.240506</td>\n",
       "      <td>0.718158</td>\n",
       "      <td>0.143086</td>\n",
       "      <td>[[389, 100], [20, 19]]</td>\n",
       "      <td>DNN0.h5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>DNN_modT</td>\n",
       "      <td>0.475931</td>\n",
       "      <td>0.767045</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.254545</td>\n",
       "      <td>0.718158</td>\n",
       "      <td>0.143086</td>\n",
       "      <td>[[384, 105], [18, 21]]</td>\n",
       "      <td>DNN_modT0.h5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      model  threshold  accuracy  precision    recall        f1   auc_roc  \\\n",
       "0        RF   0.500000  0.928030   1.000000  0.025641  0.050000  0.771931   \n",
       "1   RF_modT   0.130000  0.840909   0.235294  0.512821  0.322581  0.771931   \n",
       "2       DNN   0.500000  0.772727   0.159664  0.487179  0.240506  0.718158   \n",
       "3  DNN_modT   0.475931  0.767045   0.166667  0.538462  0.254545  0.718158   \n",
       "\n",
       "   avg_precision        confusion_matrix   model_filename  \n",
       "0       0.311884     [[489, 0], [38, 1]]       RF0.joblib  \n",
       "1       0.311884   [[424, 65], [19, 20]]  RF_modT0.joblib  \n",
       "2       0.143086  [[389, 100], [20, 19]]          DNN0.h5  \n",
       "3       0.143086  [[384, 105], [18, 21]]     DNN_modT0.h5  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11196 samples, validate on 1593 samples\n",
      "Epoch 1/100\n",
      "11196/11196 [==============================] - 2s 200us/step - loss: 0.7824 - acc: 0.6451 - val_loss: 0.5880 - val_acc: 0.7589\n",
      "Epoch 2/100\n",
      "11196/11196 [==============================] - 2s 217us/step - loss: 0.7853 - acc: 0.6436 - val_loss: 0.5880 - val_acc: 0.7589\n",
      "Epoch 3/100\n",
      "11196/11196 [==============================] - 2s 214us/step - loss: 0.7875 - acc: 0.6437 - val_loss: 0.5880 - val_acc: 0.7589\n",
      "Epoch 4/100\n",
      "11196/11196 [==============================] - 2s 209us/step - loss: 0.7822 - acc: 0.6476 - val_loss: 0.5880 - val_acc: 0.7589\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 2.980232380322967e-11.\n",
      "Epoch 5/100\n",
      "11196/11196 [==============================] - 2s 208us/step - loss: 0.7862 - acc: 0.6417 - val_loss: 0.5880 - val_acc: 0.7589\n",
      "Epoch 6/100\n",
      "11196/11196 [==============================] - 2s 212us/step - loss: 0.7820 - acc: 0.6442 - val_loss: 0.5880 - val_acc: 0.7589\n",
      "Epoch 7/100\n",
      "11196/11196 [==============================] - 2s 214us/step - loss: 0.7882 - acc: 0.6391 - val_loss: 0.5880 - val_acc: 0.7589\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 1.4901161901614834e-11.\n",
      "Epoch 8/100\n",
      "11196/11196 [==============================] - 2s 211us/step - loss: 0.7848 - acc: 0.6426 - val_loss: 0.5880 - val_acc: 0.7589\n",
      "Epoch 9/100\n",
      "11196/11196 [==============================] - 2s 208us/step - loss: 0.7884 - acc: 0.6458 - val_loss: 0.5880 - val_acc: 0.7589\n",
      "Epoch 10/100\n",
      "11196/11196 [==============================] - 2s 209us/step - loss: 0.7862 - acc: 0.6430 - val_loss: 0.5880 - val_acc: 0.7589\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 7.450580950807417e-12.\n",
      "Epoch 11/100\n",
      "11196/11196 [==============================] - 2s 215us/step - loss: 0.7821 - acc: 0.6445 - val_loss: 0.5880 - val_acc: 0.7589\n",
      "Epoch 12/100\n",
      "11196/11196 [==============================] - 2s 209us/step - loss: 0.7886 - acc: 0.6399 - val_loss: 0.5880 - val_acc: 0.7589\n",
      "Epoch 13/100\n",
      "11196/11196 [==============================] - 2s 209us/step - loss: 0.7858 - acc: 0.6439 - val_loss: 0.5880 - val_acc: 0.7589\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 3.725290475403709e-12.\n",
      "Epoch 14/100\n",
      "11196/11196 [==============================] - 2s 209us/step - loss: 0.7855 - acc: 0.6454 - val_loss: 0.5880 - val_acc: 0.7589\n",
      "Epoch 15/100\n",
      "11196/11196 [==============================] - 2s 203us/step - loss: 0.7872 - acc: 0.6462 - val_loss: 0.5880 - val_acc: 0.7589\n",
      "Epoch 16/100\n",
      "11196/11196 [==============================] - 2s 208us/step - loss: 0.7828 - acc: 0.6470 - val_loss: 0.5880 - val_acc: 0.7589\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.8626452377018543e-12.\n",
      "Epoch 17/100\n",
      "11196/11196 [==============================] - 2s 199us/step - loss: 0.7822 - acc: 0.6457 - val_loss: 0.5880 - val_acc: 0.7589\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00017: early stopping\n",
      "          NR.ER: 0.73637\n",
      "Model saved and metrics table updated.\n",
      "Model saved and metrics table updated.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>threshold</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>auc_roc</th>\n",
       "      <th>avg_precision</th>\n",
       "      <th>confusion_matrix</th>\n",
       "      <th>model_filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.215686</td>\n",
       "      <td>0.338462</td>\n",
       "      <td>0.769007</td>\n",
       "      <td>0.445565</td>\n",
       "      <td>[[462, 3], [40, 11]]</td>\n",
       "      <td>RF0.joblib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>RF_modT</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.912791</td>\n",
       "      <td>0.607143</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.430380</td>\n",
       "      <td>0.769007</td>\n",
       "      <td>0.445565</td>\n",
       "      <td>[[454, 11], [34, 17]]</td>\n",
       "      <td>RF_modT0.joblib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>DNN</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.748062</td>\n",
       "      <td>0.203008</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.293478</td>\n",
       "      <td>0.736369</td>\n",
       "      <td>0.280064</td>\n",
       "      <td>[[359, 106], [24, 27]]</td>\n",
       "      <td>DNN0.h5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>DNN_modT</td>\n",
       "      <td>0.625295</td>\n",
       "      <td>0.815891</td>\n",
       "      <td>0.270833</td>\n",
       "      <td>0.509804</td>\n",
       "      <td>0.353741</td>\n",
       "      <td>0.736369</td>\n",
       "      <td>0.280064</td>\n",
       "      <td>[[395, 70], [25, 26]]</td>\n",
       "      <td>DNN_modT0.h5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      model  threshold  accuracy  precision    recall        f1   auc_roc  \\\n",
       "0        RF   0.500000  0.916667   0.785714  0.215686  0.338462  0.769007   \n",
       "1   RF_modT   0.400000  0.912791   0.607143  0.333333  0.430380  0.769007   \n",
       "2       DNN   0.500000  0.748062   0.203008  0.529412  0.293478  0.736369   \n",
       "3  DNN_modT   0.625295  0.815891   0.270833  0.509804  0.353741  0.736369   \n",
       "\n",
       "   avg_precision        confusion_matrix   model_filename  \n",
       "0       0.445565    [[462, 3], [40, 11]]       RF0.joblib  \n",
       "1       0.445565   [[454, 11], [34, 17]]  RF_modT0.joblib  \n",
       "2       0.280064  [[359, 106], [24, 27]]          DNN0.h5  \n",
       "3       0.280064   [[395, 70], [25, 26]]     DNN_modT0.h5  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13734 samples, validate on 1808 samples\n",
      "Epoch 1/100\n",
      "13734/13734 [==============================] - 3s 200us/step - loss: 0.7634 - acc: 0.6589 - val_loss: 0.6040 - val_acc: 0.7406\n",
      "Epoch 2/100\n",
      "13734/13734 [==============================] - 3s 211us/step - loss: 0.7638 - acc: 0.6589 - val_loss: 0.6040 - val_acc: 0.7406\n",
      "Epoch 3/100\n",
      "13734/13734 [==============================] - 3s 211us/step - loss: 0.7642 - acc: 0.6591 - val_loss: 0.6040 - val_acc: 0.7406\n",
      "Epoch 4/100\n",
      "13734/13734 [==============================] - 3s 208us/step - loss: 0.7649 - acc: 0.6557 - val_loss: 0.6040 - val_acc: 0.7406\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 9.313226188509272e-13.\n",
      "Epoch 5/100\n",
      "13734/13734 [==============================] - 3s 212us/step - loss: 0.7620 - acc: 0.6594 - val_loss: 0.6040 - val_acc: 0.7406\n",
      "Epoch 6/100\n",
      "13734/13734 [==============================] - 3s 210us/step - loss: 0.7610 - acc: 0.6589 - val_loss: 0.6040 - val_acc: 0.7406\n",
      "Epoch 7/100\n",
      "13734/13734 [==============================] - 3s 209us/step - loss: 0.7661 - acc: 0.6593 - val_loss: 0.6040 - val_acc: 0.7406\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 4.656613094254636e-13.\n",
      "Epoch 8/100\n",
      "13734/13734 [==============================] - 3s 213us/step - loss: 0.7624 - acc: 0.6576 - val_loss: 0.6040 - val_acc: 0.7406\n",
      "Epoch 9/100\n",
      "13734/13734 [==============================] - 3s 211us/step - loss: 0.7625 - acc: 0.6600 - val_loss: 0.6040 - val_acc: 0.7406\n",
      "Epoch 10/100\n",
      "13734/13734 [==============================] - 3s 209us/step - loss: 0.7645 - acc: 0.6574 - val_loss: 0.6040 - val_acc: 0.7406\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 2.328306547127318e-13.\n",
      "Epoch 11/100\n",
      "13734/13734 [==============================] - 3s 212us/step - loss: 0.7647 - acc: 0.6549 - val_loss: 0.6040 - val_acc: 0.7406\n",
      "Epoch 12/100\n",
      "13734/13734 [==============================] - 3s 205us/step - loss: 0.7610 - acc: 0.6553 - val_loss: 0.6040 - val_acc: 0.7406\n",
      "Epoch 13/100\n",
      "13734/13734 [==============================] - 3s 206us/step - loss: 0.7646 - acc: 0.6568 - val_loss: 0.6040 - val_acc: 0.7406\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 1.164153273563659e-13.\n",
      "Epoch 14/100\n",
      "13734/13734 [==============================] - 3s 201us/step - loss: 0.7617 - acc: 0.6571 - val_loss: 0.6040 - val_acc: 0.7406\n",
      "Epoch 15/100\n",
      "13734/13734 [==============================] - 3s 203us/step - loss: 0.7621 - acc: 0.6591 - val_loss: 0.6040 - val_acc: 0.7406\n",
      "Epoch 16/100\n",
      "13734/13734 [==============================] - 3s 207us/step - loss: 0.7641 - acc: 0.6596 - val_loss: 0.6040 - val_acc: 0.7406\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 5.820766367818295e-14.\n",
      "Epoch 17/100\n",
      "13734/13734 [==============================] - 3s 200us/step - loss: 0.7643 - acc: 0.6552 - val_loss: 0.6040 - val_acc: 0.7406\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00017: early stopping\n",
      "      NR.ER.LBD: 0.67832\n",
      "Model saved and metrics table updated.\n",
      "Model saved and metrics table updated.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>threshold</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>auc_roc</th>\n",
       "      <th>avg_precision</th>\n",
       "      <th>confusion_matrix</th>\n",
       "      <th>model_filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.50000</td>\n",
       "      <td>0.971667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.260870</td>\n",
       "      <td>0.750345</td>\n",
       "      <td>0.268778</td>\n",
       "      <td>[[580, 0], [17, 3]]</td>\n",
       "      <td>RF0.joblib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>RF_modT</td>\n",
       "      <td>0.30500</td>\n",
       "      <td>0.965000</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.750345</td>\n",
       "      <td>0.268778</td>\n",
       "      <td>[[573, 7], [14, 6]]</td>\n",
       "      <td>RF_modT0.joblib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>DNN</td>\n",
       "      <td>0.50000</td>\n",
       "      <td>0.725000</td>\n",
       "      <td>0.049689</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.088398</td>\n",
       "      <td>0.678319</td>\n",
       "      <td>0.069591</td>\n",
       "      <td>[[427, 153], [12, 8]]</td>\n",
       "      <td>DNN0.h5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>DNN_modT</td>\n",
       "      <td>0.74828</td>\n",
       "      <td>0.915000</td>\n",
       "      <td>0.102564</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.135593</td>\n",
       "      <td>0.678319</td>\n",
       "      <td>0.069591</td>\n",
       "      <td>[[545, 35], [16, 4]]</td>\n",
       "      <td>DNN_modT0.h5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      model  threshold  accuracy  precision  recall        f1   auc_roc  \\\n",
       "0        RF    0.50000  0.971667   1.000000    0.15  0.260870  0.750345   \n",
       "1   RF_modT    0.30500  0.965000   0.461538    0.30  0.363636  0.750345   \n",
       "2       DNN    0.50000  0.725000   0.049689    0.40  0.088398  0.678319   \n",
       "3  DNN_modT    0.74828  0.915000   0.102564    0.20  0.135593  0.678319   \n",
       "\n",
       "   avg_precision       confusion_matrix   model_filename  \n",
       "0       0.268778    [[580, 0], [17, 3]]       RF0.joblib  \n",
       "1       0.268778    [[573, 7], [14, 6]]  RF_modT0.joblib  \n",
       "2       0.069591  [[427, 153], [12, 8]]          DNN0.h5  \n",
       "3       0.069591   [[545, 35], [16, 4]]     DNN_modT0.h5  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13140 samples, validate on 1691 samples\n",
      "Epoch 1/100\n",
      "13140/13140 [==============================] - 3s 201us/step - loss: 0.8473 - acc: 0.6414 - val_loss: 0.5922 - val_acc: 0.7463\n",
      "Epoch 2/100\n",
      "13140/13140 [==============================] - 3s 213us/step - loss: 0.8494 - acc: 0.6419 - val_loss: 0.5922 - val_acc: 0.7463\n",
      "Epoch 3/100\n",
      "13140/13140 [==============================] - 3s 213us/step - loss: 0.8480 - acc: 0.6416 - val_loss: 0.5922 - val_acc: 0.7463\n",
      "Epoch 4/100\n",
      "13140/13140 [==============================] - 3s 211us/step - loss: 0.8501 - acc: 0.6409 - val_loss: 0.5922 - val_acc: 0.7463\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 2.9103831839091474e-14.\n",
      "Epoch 5/100\n",
      "13140/13140 [==============================] - 3s 210us/step - loss: 0.8500 - acc: 0.6400 - val_loss: 0.5922 - val_acc: 0.7463\n",
      "Epoch 6/100\n",
      "13140/13140 [==============================] - 3s 211us/step - loss: 0.8496 - acc: 0.6407 - val_loss: 0.5922 - val_acc: 0.7463\n",
      "Epoch 7/100\n",
      "13140/13140 [==============================] - 3s 207us/step - loss: 0.8504 - acc: 0.6414 - val_loss: 0.5922 - val_acc: 0.7463\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 1.4551915919545737e-14.\n",
      "Epoch 8/100\n",
      "13140/13140 [==============================] - 3s 211us/step - loss: 0.8466 - acc: 0.6377 - val_loss: 0.5922 - val_acc: 0.7463\n",
      "Epoch 9/100\n",
      "13140/13140 [==============================] - 3s 212us/step - loss: 0.8507 - acc: 0.6404 - val_loss: 0.5922 - val_acc: 0.7463\n",
      "Epoch 10/100\n",
      "13140/13140 [==============================] - 3s 210us/step - loss: 0.8489 - acc: 0.6435 - val_loss: 0.5922 - val_acc: 0.7463\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 7.275957959772868e-15.\n",
      "Epoch 11/100\n",
      "13140/13140 [==============================] - 3s 208us/step - loss: 0.8507 - acc: 0.6406 - val_loss: 0.5922 - val_acc: 0.7463\n",
      "Epoch 12/100\n",
      "13140/13140 [==============================] - 3s 209us/step - loss: 0.8512 - acc: 0.6407 - val_loss: 0.5922 - val_acc: 0.7463\n",
      "Epoch 13/100\n",
      "13140/13140 [==============================] - 3s 214us/step - loss: 0.8524 - acc: 0.6384 - val_loss: 0.5922 - val_acc: 0.7463\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 3.637978979886434e-15.\n",
      "Epoch 14/100\n",
      "13140/13140 [==============================] - 3s 200us/step - loss: 0.8479 - acc: 0.6425 - val_loss: 0.5922 - val_acc: 0.7463\n",
      "Epoch 15/100\n",
      "13140/13140 [==============================] - 3s 202us/step - loss: 0.8506 - acc: 0.6368 - val_loss: 0.5922 - val_acc: 0.7463\n",
      "Epoch 16/100\n",
      "13140/13140 [==============================] - 3s 203us/step - loss: 0.8475 - acc: 0.6420 - val_loss: 0.5922 - val_acc: 0.7463\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.818989489943217e-15.\n",
      "Epoch 17/100\n",
      "13140/13140 [==============================] - 3s 203us/step - loss: 0.8496 - acc: 0.6376 - val_loss: 0.5922 - val_acc: 0.7463\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00017: early stopping\n",
      "  NR.PPAR.gamma: 0.74039\n",
      "Model saved and metrics table updated.\n",
      "Model saved and metrics table updated.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>threshold</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>auc_roc</th>\n",
       "      <th>avg_precision</th>\n",
       "      <th>confusion_matrix</th>\n",
       "      <th>model_filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.947107</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.709846</td>\n",
       "      <td>0.129989</td>\n",
       "      <td>[[573, 1], [31, 0]]</td>\n",
       "      <td>RF0.joblib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>RF_modT</td>\n",
       "      <td>0.130000</td>\n",
       "      <td>0.890909</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.322581</td>\n",
       "      <td>0.232558</td>\n",
       "      <td>0.709846</td>\n",
       "      <td>0.129989</td>\n",
       "      <td>[[529, 45], [21, 10]]</td>\n",
       "      <td>RF_modT0.joblib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>DNN</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.750413</td>\n",
       "      <td>0.115385</td>\n",
       "      <td>0.580645</td>\n",
       "      <td>0.192513</td>\n",
       "      <td>0.740390</td>\n",
       "      <td>0.109550</td>\n",
       "      <td>[[436, 138], [13, 18]]</td>\n",
       "      <td>DNN0.h5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>DNN_modT</td>\n",
       "      <td>0.577705</td>\n",
       "      <td>0.798347</td>\n",
       "      <td>0.130081</td>\n",
       "      <td>0.516129</td>\n",
       "      <td>0.207792</td>\n",
       "      <td>0.740390</td>\n",
       "      <td>0.109550</td>\n",
       "      <td>[[467, 107], [15, 16]]</td>\n",
       "      <td>DNN_modT0.h5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      model  threshold  accuracy  precision    recall        f1   auc_roc  \\\n",
       "0        RF   0.500000  0.947107   0.000000  0.000000  0.000000  0.709846   \n",
       "1   RF_modT   0.130000  0.890909   0.181818  0.322581  0.232558  0.709846   \n",
       "2       DNN   0.500000  0.750413   0.115385  0.580645  0.192513  0.740390   \n",
       "3  DNN_modT   0.577705  0.798347   0.130081  0.516129  0.207792  0.740390   \n",
       "\n",
       "   avg_precision        confusion_matrix   model_filename  \n",
       "0       0.129989     [[573, 1], [31, 0]]       RF0.joblib  \n",
       "1       0.129989   [[529, 45], [21, 10]]  RF_modT0.joblib  \n",
       "2       0.109550  [[436, 138], [13, 18]]          DNN0.h5  \n",
       "3       0.109550  [[467, 107], [15, 16]]     DNN_modT0.h5  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10006 samples, validate on 1481 samples\n",
      "Epoch 1/100\n",
      "10006/10006 [==============================] - 2s 198us/step - loss: 0.8277 - acc: 0.6072 - val_loss: 0.6143 - val_acc: 0.7387\n",
      "Epoch 2/100\n",
      "10006/10006 [==============================] - 2s 209us/step - loss: 0.8290 - acc: 0.6072 - val_loss: 0.6143 - val_acc: 0.7387\n",
      "Epoch 3/100\n",
      "10006/10006 [==============================] - 2s 214us/step - loss: 0.8288 - acc: 0.6097 - val_loss: 0.6143 - val_acc: 0.7387\n",
      "Epoch 4/100\n",
      "10006/10006 [==============================] - 2s 217us/step - loss: 0.8261 - acc: 0.6060 - val_loss: 0.6143 - val_acc: 0.7387\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 9.094947449716085e-16.\n",
      "Epoch 5/100\n",
      "10006/10006 [==============================] - 2s 211us/step - loss: 0.8253 - acc: 0.6076 - val_loss: 0.6143 - val_acc: 0.7387\n",
      "Epoch 6/100\n",
      "10006/10006 [==============================] - 2s 213us/step - loss: 0.8243 - acc: 0.6088 - val_loss: 0.6143 - val_acc: 0.7387\n",
      "Epoch 7/100\n",
      "10006/10006 [==============================] - 2s 213us/step - loss: 0.8269 - acc: 0.6092 - val_loss: 0.6143 - val_acc: 0.7387\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 4.547473724858043e-16.\n",
      "Epoch 8/100\n",
      "10006/10006 [==============================] - 2s 211us/step - loss: 0.8283 - acc: 0.6095 - val_loss: 0.6143 - val_acc: 0.7387\n",
      "Epoch 9/100\n",
      "10006/10006 [==============================] - 2s 218us/step - loss: 0.8291 - acc: 0.6070 - val_loss: 0.6143 - val_acc: 0.7387\n",
      "Epoch 10/100\n",
      "10006/10006 [==============================] - 2s 212us/step - loss: 0.8243 - acc: 0.6079 - val_loss: 0.6143 - val_acc: 0.7387\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 2.2737368624290214e-16.\n",
      "Epoch 11/100\n",
      "10006/10006 [==============================] - 2s 208us/step - loss: 0.8267 - acc: 0.6096 - val_loss: 0.6143 - val_acc: 0.7387\n",
      "Epoch 12/100\n",
      "10006/10006 [==============================] - 2s 218us/step - loss: 0.8263 - acc: 0.6059 - val_loss: 0.6143 - val_acc: 0.7387\n",
      "Epoch 13/100\n",
      "10006/10006 [==============================] - 2s 211us/step - loss: 0.8271 - acc: 0.6047 - val_loss: 0.6143 - val_acc: 0.7387\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 1.1368684312145107e-16.\n",
      "Epoch 14/100\n",
      "10006/10006 [==============================] - 2s 214us/step - loss: 0.8230 - acc: 0.6084 - val_loss: 0.6143 - val_acc: 0.7387\n",
      "Epoch 15/100\n",
      "10006/10006 [==============================] - 2s 208us/step - loss: 0.8242 - acc: 0.6107 - val_loss: 0.6143 - val_acc: 0.7387\n",
      "Epoch 16/100\n",
      "10006/10006 [==============================] - 2s 212us/step - loss: 0.8264 - acc: 0.6108 - val_loss: 0.6143 - val_acc: 0.7387\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 5.684342156072553e-17.\n",
      "Epoch 17/100\n",
      "10006/10006 [==============================] - 2s 214us/step - loss: 0.8261 - acc: 0.6097 - val_loss: 0.6143 - val_acc: 0.7387\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00017: early stopping\n",
      "         SR.ARE: 0.70550\n",
      "Model saved and metrics table updated.\n",
      "Model saved and metrics table updated.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>threshold</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>auc_roc</th>\n",
       "      <th>avg_precision</th>\n",
       "      <th>confusion_matrix</th>\n",
       "      <th>model_filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.841441</td>\n",
       "      <td>0.592593</td>\n",
       "      <td>0.172043</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.780838</td>\n",
       "      <td>0.430711</td>\n",
       "      <td>[[451, 11], [77, 16]]</td>\n",
       "      <td>RF0.joblib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>RF_modT</td>\n",
       "      <td>0.340000</td>\n",
       "      <td>0.823423</td>\n",
       "      <td>0.476190</td>\n",
       "      <td>0.537634</td>\n",
       "      <td>0.505051</td>\n",
       "      <td>0.780838</td>\n",
       "      <td>0.430711</td>\n",
       "      <td>[[407, 55], [43, 50]]</td>\n",
       "      <td>RF_modT0.joblib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>DNN</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.762162</td>\n",
       "      <td>0.355556</td>\n",
       "      <td>0.516129</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>0.705500</td>\n",
       "      <td>0.347718</td>\n",
       "      <td>[[375, 87], [45, 48]]</td>\n",
       "      <td>DNN0.h5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>DNN_modT</td>\n",
       "      <td>0.457539</td>\n",
       "      <td>0.753153</td>\n",
       "      <td>0.353333</td>\n",
       "      <td>0.569892</td>\n",
       "      <td>0.436214</td>\n",
       "      <td>0.705500</td>\n",
       "      <td>0.347718</td>\n",
       "      <td>[[365, 97], [40, 53]]</td>\n",
       "      <td>DNN_modT0.h5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      model  threshold  accuracy  precision    recall        f1   auc_roc  \\\n",
       "0        RF   0.500000  0.841441   0.592593  0.172043  0.266667  0.780838   \n",
       "1   RF_modT   0.340000  0.823423   0.476190  0.537634  0.505051  0.780838   \n",
       "2       DNN   0.500000  0.762162   0.355556  0.516129  0.421053  0.705500   \n",
       "3  DNN_modT   0.457539  0.753153   0.353333  0.569892  0.436214  0.705500   \n",
       "\n",
       "   avg_precision       confusion_matrix   model_filename  \n",
       "0       0.430711  [[451, 11], [77, 16]]       RF0.joblib  \n",
       "1       0.430711  [[407, 55], [43, 50]]  RF_modT0.joblib  \n",
       "2       0.347718  [[375, 87], [45, 48]]          DNN0.h5  \n",
       "3       0.347718  [[365, 97], [40, 53]]     DNN_modT0.h5  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14400 samples, validate on 1873 samples\n",
      "Epoch 1/100\n",
      "14400/14400 [==============================] - 3s 207us/step - loss: 0.7856 - acc: 0.6460 - val_loss: 0.6163 - val_acc: 0.7277\n",
      "Epoch 2/100\n",
      "14400/14400 [==============================] - 3s 216us/step - loss: 0.7849 - acc: 0.6498 - val_loss: 0.6163 - val_acc: 0.7277\n",
      "Epoch 3/100\n",
      "14400/14400 [==============================] - 3s 213us/step - loss: 0.7839 - acc: 0.6488 - val_loss: 0.6163 - val_acc: 0.7277\n",
      "Epoch 4/100\n",
      "14400/14400 [==============================] - 3s 215us/step - loss: 0.7871 - acc: 0.6468 - val_loss: 0.6163 - val_acc: 0.7277\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 2.842171078036277e-17.\n",
      "Epoch 5/100\n",
      "14400/14400 [==============================] - 3s 210us/step - loss: 0.7841 - acc: 0.6508 - val_loss: 0.6163 - val_acc: 0.7277\n",
      "Epoch 6/100\n",
      "14400/14400 [==============================] - 3s 213us/step - loss: 0.7849 - acc: 0.6462 - val_loss: 0.6163 - val_acc: 0.7277\n",
      "Epoch 7/100\n",
      "14400/14400 [==============================] - 3s 212us/step - loss: 0.7881 - acc: 0.6475 - val_loss: 0.6163 - val_acc: 0.7277\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 1.4210855390181384e-17.\n",
      "Epoch 8/100\n",
      "14400/14400 [==============================] - 3s 212us/step - loss: 0.7859 - acc: 0.6503 - val_loss: 0.6163 - val_acc: 0.7277\n",
      "Epoch 9/100\n",
      "14400/14400 [==============================] - 3s 213us/step - loss: 0.7861 - acc: 0.6492 - val_loss: 0.6163 - val_acc: 0.7277\n",
      "Epoch 10/100\n",
      "14400/14400 [==============================] - 3s 212us/step - loss: 0.7865 - acc: 0.6467 - val_loss: 0.6163 - val_acc: 0.7277\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 7.105427695090692e-18.\n",
      "Epoch 11/100\n",
      "14400/14400 [==============================] - 3s 210us/step - loss: 0.7842 - acc: 0.6473 - val_loss: 0.6163 - val_acc: 0.7277\n",
      "Epoch 12/100\n",
      "14400/14400 [==============================] - 3s 214us/step - loss: 0.7872 - acc: 0.6451 - val_loss: 0.6163 - val_acc: 0.7277\n",
      "Epoch 13/100\n",
      "14400/14400 [==============================] - 3s 210us/step - loss: 0.7902 - acc: 0.6468 - val_loss: 0.6163 - val_acc: 0.7277\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 3.552713847545346e-18.\n",
      "Epoch 14/100\n",
      "14400/14400 [==============================] - 3s 214us/step - loss: 0.7862 - acc: 0.6485 - val_loss: 0.6163 - val_acc: 0.7277\n",
      "Epoch 15/100\n",
      "14400/14400 [==============================] - 3s 206us/step - loss: 0.7876 - acc: 0.6485 - val_loss: 0.6163 - val_acc: 0.7277\n",
      "Epoch 16/100\n",
      "14400/14400 [==============================] - 3s 201us/step - loss: 0.7894 - acc: 0.6469 - val_loss: 0.6163 - val_acc: 0.7277\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.776356923772673e-18.\n",
      "Epoch 17/100\n",
      "14400/14400 [==============================] - 3s 214us/step - loss: 0.7857 - acc: 0.6475 - val_loss: 0.6163 - val_acc: 0.7277\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00017: early stopping\n",
      "       SR.ATAD5: 0.71161\n",
      "Model saved and metrics table updated.\n",
      "Model saved and metrics table updated.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>threshold</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>auc_roc</th>\n",
       "      <th>avg_precision</th>\n",
       "      <th>confusion_matrix</th>\n",
       "      <th>model_filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.938907</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.795625</td>\n",
       "      <td>0.427828</td>\n",
       "      <td>[[584, 0], [38, 0]]</td>\n",
       "      <td>RF0.joblib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>RF_modT</td>\n",
       "      <td>0.255000</td>\n",
       "      <td>0.948553</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.795625</td>\n",
       "      <td>0.427828</td>\n",
       "      <td>[[576, 8], [24, 14]]</td>\n",
       "      <td>RF_modT0.joblib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>DNN</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.736334</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.552632</td>\n",
       "      <td>0.203883</td>\n",
       "      <td>0.711608</td>\n",
       "      <td>0.153415</td>\n",
       "      <td>[[437, 147], [17, 21]]</td>\n",
       "      <td>DNN0.h5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>DNN_modT</td>\n",
       "      <td>0.577705</td>\n",
       "      <td>0.789389</td>\n",
       "      <td>0.155556</td>\n",
       "      <td>0.552632</td>\n",
       "      <td>0.242775</td>\n",
       "      <td>0.711608</td>\n",
       "      <td>0.153415</td>\n",
       "      <td>[[470, 114], [17, 21]]</td>\n",
       "      <td>DNN_modT0.h5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      model  threshold  accuracy  precision    recall        f1   auc_roc  \\\n",
       "0        RF   0.500000  0.938907   0.000000  0.000000  0.000000  0.795625   \n",
       "1   RF_modT   0.255000  0.948553   0.636364  0.368421  0.466667  0.795625   \n",
       "2       DNN   0.500000  0.736334   0.125000  0.552632  0.203883  0.711608   \n",
       "3  DNN_modT   0.577705  0.789389   0.155556  0.552632  0.242775  0.711608   \n",
       "\n",
       "   avg_precision        confusion_matrix   model_filename  \n",
       "0       0.427828     [[584, 0], [38, 0]]       RF0.joblib  \n",
       "1       0.427828    [[576, 8], [24, 14]]  RF_modT0.joblib  \n",
       "2       0.153415  [[437, 147], [17, 21]]          DNN0.h5  \n",
       "3       0.153415  [[470, 114], [17, 21]]     DNN_modT0.h5  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12766 samples, validate on 1684 samples\n",
      "Epoch 1/100\n",
      "12766/12766 [==============================] - 3s 201us/step - loss: 0.8939 - acc: 0.5726 - val_loss: 0.6040 - val_acc: 0.7387\n",
      "Epoch 2/100\n",
      "12766/12766 [==============================] - 3s 216us/step - loss: 0.8961 - acc: 0.5725 - val_loss: 0.6040 - val_acc: 0.7387\n",
      "Epoch 3/100\n",
      "12766/12766 [==============================] - 3s 213us/step - loss: 0.8974 - acc: 0.5715 - val_loss: 0.6040 - val_acc: 0.7387\n",
      "Epoch 4/100\n",
      "12766/12766 [==============================] - 3s 213us/step - loss: 0.8990 - acc: 0.5734 - val_loss: 0.6040 - val_acc: 0.7387\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 8.881784618863365e-19.\n",
      "Epoch 5/100\n",
      "12766/12766 [==============================] - 3s 210us/step - loss: 0.8931 - acc: 0.5728 - val_loss: 0.6040 - val_acc: 0.7387\n",
      "Epoch 6/100\n",
      "12766/12766 [==============================] - 3s 209us/step - loss: 0.8964 - acc: 0.5714 - val_loss: 0.6040 - val_acc: 0.7387\n",
      "Epoch 7/100\n",
      "12766/12766 [==============================] - 3s 225us/step - loss: 0.8986 - acc: 0.5725 - val_loss: 0.6040 - val_acc: 0.7387\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 4.440892309431682e-19.\n",
      "Epoch 8/100\n",
      "12766/12766 [==============================] - 3s 214us/step - loss: 0.8973 - acc: 0.5717 - val_loss: 0.6040 - val_acc: 0.7387\n",
      "Epoch 9/100\n",
      "12766/12766 [==============================] - 3s 213us/step - loss: 0.8969 - acc: 0.5734 - val_loss: 0.6040 - val_acc: 0.7387\n",
      "Epoch 10/100\n",
      "12766/12766 [==============================] - 3s 207us/step - loss: 0.8970 - acc: 0.5706 - val_loss: 0.6040 - val_acc: 0.7387\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 2.220446154715841e-19.\n",
      "Epoch 11/100\n",
      "12766/12766 [==============================] - 3s 210us/step - loss: 0.8962 - acc: 0.5743 - val_loss: 0.6040 - val_acc: 0.7387\n",
      "Epoch 12/100\n",
      "12766/12766 [==============================] - 3s 204us/step - loss: 0.8947 - acc: 0.5739 - val_loss: 0.6040 - val_acc: 0.7387\n",
      "Epoch 13/100\n",
      "12766/12766 [==============================] - 3s 214us/step - loss: 0.8954 - acc: 0.5743 - val_loss: 0.6040 - val_acc: 0.7387\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 1.1102230773579206e-19.\n",
      "Epoch 14/100\n",
      "12766/12766 [==============================] - 3s 211us/step - loss: 0.8960 - acc: 0.5734 - val_loss: 0.6040 - val_acc: 0.7387\n",
      "Epoch 15/100\n",
      "12766/12766 [==============================] - 3s 202us/step - loss: 0.9010 - acc: 0.5701 - val_loss: 0.6040 - val_acc: 0.7387\n",
      "Epoch 16/100\n",
      "12766/12766 [==============================] - 3s 202us/step - loss: 0.8929 - acc: 0.5774 - val_loss: 0.6040 - val_acc: 0.7387\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 5.551115386789603e-20.\n",
      "Epoch 17/100\n",
      "12766/12766 [==============================] - 3s 204us/step - loss: 0.8962 - acc: 0.5713 - val_loss: 0.6040 - val_acc: 0.7387\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00017: early stopping\n",
      "         SR.HSE: 0.62674\n",
      "Model saved and metrics table updated.\n",
      "Model saved and metrics table updated.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>threshold</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>auc_roc</th>\n",
       "      <th>avg_precision</th>\n",
       "      <th>confusion_matrix</th>\n",
       "      <th>model_filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.965574</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.759586</td>\n",
       "      <td>0.239563</td>\n",
       "      <td>[[586, 2], [19, 3]]</td>\n",
       "      <td>RF0.joblib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>RF_modT</td>\n",
       "      <td>0.340000</td>\n",
       "      <td>0.965574</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.759586</td>\n",
       "      <td>0.239563</td>\n",
       "      <td>[[583, 5], [16, 6]]</td>\n",
       "      <td>RF_modT0.joblib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>DNN</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.726230</td>\n",
       "      <td>0.055215</td>\n",
       "      <td>0.409091</td>\n",
       "      <td>0.097297</td>\n",
       "      <td>0.626739</td>\n",
       "      <td>0.054621</td>\n",
       "      <td>[[434, 154], [13, 9]]</td>\n",
       "      <td>DNN0.h5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>DNN_modT</td>\n",
       "      <td>0.450345</td>\n",
       "      <td>0.708197</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.118812</td>\n",
       "      <td>0.626739</td>\n",
       "      <td>0.054621</td>\n",
       "      <td>[[420, 168], [10, 12]]</td>\n",
       "      <td>DNN_modT0.h5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      model  threshold  accuracy  precision    recall        f1   auc_roc  \\\n",
       "0        RF   0.500000  0.965574   0.600000  0.136364  0.222222  0.759586   \n",
       "1   RF_modT   0.340000  0.965574   0.545455  0.272727  0.363636  0.759586   \n",
       "2       DNN   0.500000  0.726230   0.055215  0.409091  0.097297  0.626739   \n",
       "3  DNN_modT   0.450345  0.708197   0.066667  0.545455  0.118812  0.626739   \n",
       "\n",
       "   avg_precision        confusion_matrix   model_filename  \n",
       "0       0.239563     [[586, 2], [19, 3]]       RF0.joblib  \n",
       "1       0.239563     [[583, 5], [16, 6]]  RF_modT0.joblib  \n",
       "2       0.054621   [[434, 154], [13, 9]]          DNN0.h5  \n",
       "3       0.054621  [[420, 168], [10, 12]]     DNN_modT0.h5  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10204 samples, validate on 1512 samples\n",
      "Epoch 1/100\n",
      "10204/10204 [==============================] - 2s 201us/step - loss: 0.7619 - acc: 0.6424 - val_loss: 0.6140 - val_acc: 0.7421\n",
      "Epoch 2/100\n",
      "10204/10204 [==============================] - 2s 218us/step - loss: 0.7654 - acc: 0.6400 - val_loss: 0.6140 - val_acc: 0.7421\n",
      "Epoch 3/100\n",
      "10204/10204 [==============================] - 2s 215us/step - loss: 0.7604 - acc: 0.6392 - val_loss: 0.6140 - val_acc: 0.7421\n",
      "Epoch 4/100\n",
      "10204/10204 [==============================] - 2s 219us/step - loss: 0.7587 - acc: 0.6449 - val_loss: 0.6140 - val_acc: 0.7421\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 2.7755576933948015e-20.\n",
      "Epoch 5/100\n",
      "10204/10204 [==============================] - 2s 214us/step - loss: 0.7617 - acc: 0.6446 - val_loss: 0.6140 - val_acc: 0.7421\n",
      "Epoch 6/100\n",
      "10204/10204 [==============================] - 2s 213us/step - loss: 0.7611 - acc: 0.6400 - val_loss: 0.6140 - val_acc: 0.7421\n",
      "Epoch 7/100\n",
      "10204/10204 [==============================] - 2s 212us/step - loss: 0.7633 - acc: 0.6404 - val_loss: 0.6140 - val_acc: 0.7421\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 1.3877788466974007e-20.\n",
      "Epoch 8/100\n",
      "10204/10204 [==============================] - 2s 216us/step - loss: 0.7594 - acc: 0.6465 - val_loss: 0.6140 - val_acc: 0.7421\n",
      "Epoch 9/100\n",
      "10204/10204 [==============================] - 2s 216us/step - loss: 0.7610 - acc: 0.6450 - val_loss: 0.6140 - val_acc: 0.7421\n",
      "Epoch 10/100\n",
      "10204/10204 [==============================] - 2s 215us/step - loss: 0.7598 - acc: 0.6421 - val_loss: 0.6140 - val_acc: 0.7421\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 6.938894233487004e-21.\n",
      "Epoch 11/100\n",
      "10204/10204 [==============================] - 2s 221us/step - loss: 0.7635 - acc: 0.6415 - val_loss: 0.6140 - val_acc: 0.7421\n",
      "Epoch 12/100\n",
      "10204/10204 [==============================] - 2s 213us/step - loss: 0.7603 - acc: 0.6439 - val_loss: 0.6140 - val_acc: 0.7421\n",
      "Epoch 13/100\n",
      "10204/10204 [==============================] - 2s 216us/step - loss: 0.7582 - acc: 0.6419 - val_loss: 0.6140 - val_acc: 0.7421\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 3.469447116743502e-21.\n",
      "Epoch 14/100\n",
      "10204/10204 [==============================] - 2s 216us/step - loss: 0.7601 - acc: 0.6458 - val_loss: 0.6140 - val_acc: 0.7421\n",
      "Epoch 15/100\n",
      "10204/10204 [==============================] - 2s 215us/step - loss: 0.7570 - acc: 0.6437 - val_loss: 0.6140 - val_acc: 0.7421\n",
      "Epoch 16/100\n",
      "10204/10204 [==============================] - 2s 213us/step - loss: 0.7629 - acc: 0.6403 - val_loss: 0.6140 - val_acc: 0.7421\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.734723558371751e-21.\n",
      "Epoch 17/100\n",
      "10204/10204 [==============================] - 2s 209us/step - loss: 0.7598 - acc: 0.6424 - val_loss: 0.6140 - val_acc: 0.7421\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00017: early stopping\n",
      "         SR.MMP: 0.80388\n",
      "Model saved and metrics table updated.\n",
      "Model saved and metrics table updated.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>threshold</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>auc_roc</th>\n",
       "      <th>avg_precision</th>\n",
       "      <th>confusion_matrix</th>\n",
       "      <th>model_filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.906077</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.484848</td>\n",
       "      <td>0.930331</td>\n",
       "      <td>0.560184</td>\n",
       "      <td>[[468, 15], [36, 24]]</td>\n",
       "      <td>RF0.joblib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>RF_modT</td>\n",
       "      <td>0.382500</td>\n",
       "      <td>0.913444</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.624000</td>\n",
       "      <td>0.930331</td>\n",
       "      <td>0.560184</td>\n",
       "      <td>[[457, 26], [21, 39]]</td>\n",
       "      <td>RF_modT0.joblib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>DNN</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.815838</td>\n",
       "      <td>0.330508</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.438202</td>\n",
       "      <td>0.803882</td>\n",
       "      <td>0.296565</td>\n",
       "      <td>[[404, 79], [21, 39]]</td>\n",
       "      <td>DNN0.h5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>DNN_modT</td>\n",
       "      <td>0.505391</td>\n",
       "      <td>0.817680</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.440678</td>\n",
       "      <td>0.803882</td>\n",
       "      <td>0.296565</td>\n",
       "      <td>[[405, 78], [21, 39]]</td>\n",
       "      <td>DNN_modT0.h5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      model  threshold  accuracy  precision  recall        f1   auc_roc  \\\n",
       "0        RF   0.500000  0.906077   0.615385    0.40  0.484848  0.930331   \n",
       "1   RF_modT   0.382500  0.913444   0.600000    0.65  0.624000  0.930331   \n",
       "2       DNN   0.500000  0.815838   0.330508    0.65  0.438202  0.803882   \n",
       "3  DNN_modT   0.505391  0.817680   0.333333    0.65  0.440678  0.803882   \n",
       "\n",
       "   avg_precision       confusion_matrix   model_filename  \n",
       "0       0.560184  [[468, 15], [36, 24]]       RF0.joblib  \n",
       "1       0.560184  [[457, 26], [21, 39]]  RF_modT0.joblib  \n",
       "2       0.296565  [[404, 79], [21, 39]]          DNN0.h5  \n",
       "3       0.296565  [[405, 78], [21, 39]]     DNN_modT0.h5  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13340 samples, validate on 1781 samples\n",
      "Epoch 1/100\n",
      "13340/13340 [==============================] - 3s 206us/step - loss: 0.7926 - acc: 0.6395 - val_loss: 0.6058 - val_acc: 0.7344\n",
      "Epoch 2/100\n",
      "13340/13340 [==============================] - 3s 218us/step - loss: 0.7896 - acc: 0.6397 - val_loss: 0.6058 - val_acc: 0.7344\n",
      "Epoch 3/100\n",
      "13340/13340 [==============================] - 3s 215us/step - loss: 0.7886 - acc: 0.6389 - val_loss: 0.6058 - val_acc: 0.7344\n",
      "Epoch 4/100\n",
      "13340/13340 [==============================] - 3s 214us/step - loss: 0.7935 - acc: 0.6389 - val_loss: 0.6058 - val_acc: 0.7344\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 8.673617791858755e-22.\n",
      "Epoch 5/100\n",
      "13340/13340 [==============================] - 3s 213us/step - loss: 0.7917 - acc: 0.6403 - val_loss: 0.6058 - val_acc: 0.7344\n",
      "Epoch 6/100\n",
      "13340/13340 [==============================] - 3s 212us/step - loss: 0.7906 - acc: 0.6421 - val_loss: 0.6058 - val_acc: 0.7344\n",
      "Epoch 7/100\n",
      "13340/13340 [==============================] - 3s 215us/step - loss: 0.7909 - acc: 0.6407 - val_loss: 0.6058 - val_acc: 0.7344\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 4.336808895929377e-22.\n",
      "Epoch 8/100\n",
      "13340/13340 [==============================] - 3s 219us/step - loss: 0.7873 - acc: 0.6408 - val_loss: 0.6058 - val_acc: 0.7344\n",
      "Epoch 9/100\n",
      "13340/13340 [==============================] - 3s 224us/step - loss: 0.7938 - acc: 0.6364 - val_loss: 0.6058 - val_acc: 0.7344\n",
      "Epoch 10/100\n",
      "13340/13340 [==============================] - 3s 225us/step - loss: 0.7931 - acc: 0.6383 - val_loss: 0.6058 - val_acc: 0.7344\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 2.1684044479646887e-22.\n",
      "Epoch 11/100\n",
      "13340/13340 [==============================] - 3s 221us/step - loss: 0.7897 - acc: 0.6435 - val_loss: 0.6058 - val_acc: 0.7344\n",
      "Epoch 12/100\n",
      "13340/13340 [==============================] - 3s 211us/step - loss: 0.7910 - acc: 0.6357 - val_loss: 0.6058 - val_acc: 0.7344\n",
      "Epoch 13/100\n",
      "13340/13340 [==============================] - 3s 217us/step - loss: 0.7885 - acc: 0.6378 - val_loss: 0.6058 - val_acc: 0.7344\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 1.0842022239823443e-22.\n",
      "Epoch 14/100\n",
      "13340/13340 [==============================] - 3s 209us/step - loss: 0.7954 - acc: 0.6359 - val_loss: 0.6058 - val_acc: 0.7344\n",
      "Epoch 15/100\n",
      "13340/13340 [==============================] - 3s 208us/step - loss: 0.7901 - acc: 0.6382 - val_loss: 0.6058 - val_acc: 0.7344\n",
      "Epoch 16/100\n",
      "13340/13340 [==============================] - 3s 213us/step - loss: 0.7933 - acc: 0.6373 - val_loss: 0.6058 - val_acc: 0.7344\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 5.421011119911722e-23.\n",
      "Epoch 17/100\n",
      "13340/13340 [==============================] - 3s 207us/step - loss: 0.7926 - acc: 0.6397 - val_loss: 0.6058 - val_acc: 0.7344\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00017: early stopping\n",
      "         SR.p53: 0.76870\n",
      "Model saved and metrics table updated.\n",
      "Model saved and metrics table updated.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>threshold</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>auc_roc</th>\n",
       "      <th>avg_precision</th>\n",
       "      <th>confusion_matrix</th>\n",
       "      <th>model_filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.933442</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.024390</td>\n",
       "      <td>0.046512</td>\n",
       "      <td>0.806702</td>\n",
       "      <td>0.221990</td>\n",
       "      <td>[[574, 1], [40, 1]]</td>\n",
       "      <td>RF0.joblib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>RF_modT</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>0.886364</td>\n",
       "      <td>0.254237</td>\n",
       "      <td>0.365854</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.806702</td>\n",
       "      <td>0.221990</td>\n",
       "      <td>[[531, 44], [26, 15]]</td>\n",
       "      <td>RF_modT0.joblib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>DNN</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.753247</td>\n",
       "      <td>0.150943</td>\n",
       "      <td>0.585366</td>\n",
       "      <td>0.240000</td>\n",
       "      <td>0.768696</td>\n",
       "      <td>0.168234</td>\n",
       "      <td>[[440, 135], [17, 24]]</td>\n",
       "      <td>DNN0.h5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>DNN_modT</td>\n",
       "      <td>0.540362</td>\n",
       "      <td>0.780844</td>\n",
       "      <td>0.169014</td>\n",
       "      <td>0.585366</td>\n",
       "      <td>0.262295</td>\n",
       "      <td>0.768696</td>\n",
       "      <td>0.168234</td>\n",
       "      <td>[[457, 118], [17, 24]]</td>\n",
       "      <td>DNN_modT0.h5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      model  threshold  accuracy  precision    recall        f1   auc_roc  \\\n",
       "0        RF   0.500000  0.933442   0.500000  0.024390  0.046512  0.806702   \n",
       "1   RF_modT   0.220000  0.886364   0.254237  0.365854  0.300000  0.806702   \n",
       "2       DNN   0.500000  0.753247   0.150943  0.585366  0.240000  0.768696   \n",
       "3  DNN_modT   0.540362  0.780844   0.169014  0.585366  0.262295  0.768696   \n",
       "\n",
       "   avg_precision        confusion_matrix   model_filename  \n",
       "0       0.221990     [[574, 1], [40, 1]]       RF0.joblib  \n",
       "1       0.221990   [[531, 44], [26, 15]]  RF_modT0.joblib  \n",
       "2       0.168234  [[440, 135], [17, 24]]          DNN0.h5  \n",
       "3       0.168234  [[457, 118], [17, 24]]     DNN_modT0.h5  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for target in y_tr.columns:\n",
    "    # Determine rows with available data\n",
    "    rows_tr = np.isfinite(y_tr[target]).values\n",
    "    rows_te = np.isfinite(y_te[target]).values\n",
    "    x,y = x_tr[rows_tr], y_tr[target][rows_tr]\n",
    "    \n",
    "    # Address Class Imbalance\n",
    "    x_train, x_val, y_train, y_val = train_test_split(x, y, stratify=y, \\\n",
    "                                                      test_size=0.2, random_state=42)\n",
    "    ros = RandomOverSampler(random_state=0)\n",
    "    x_resampled, y_resampled = ros.fit_sample(x_train,y_train)\n",
    "    \n",
    "    # Train the DNN\n",
    "    DNN.fit(\n",
    "        x_resampled, y_resampled, batch_size=512, epochs=100,\\\n",
    "        validation_data=(x_val,y_val), verbose=1,\n",
    "        callbacks=[\n",
    "            keras.callbacks.EarlyStopping(patience=16,verbose=1,\\\n",
    "                                          restore_best_weights=True),\n",
    "            keras.callbacks.ReduceLROnPlateau(factor=0.5,patience=3,verbose=1)\n",
    "        ])\n",
    "    \n",
    "    # Get predictions, calculate model performance and save info\n",
    "    p_te = DNN.predict(x_te[rows_te])[:,0]\n",
    "    y_testing=y_te[target][rows_te]\n",
    "    auc_te = roc_auc_score(y_testing, p_te)\n",
    "    print(\"%15s: %3.5f\" % (target, auc_te))\n",
    "\n",
    "    y_hat_testing=DNN.predict_classes(x_te[rows_te])\n",
    "    average_precision=average_precision_score(y_testing,p_te)\n",
    "    mv=evaluate_model_predictions(target,'DNN',0.5,y_testing,y_hat_testing,\\\n",
    "                                  auc_te,average_precision)\n",
    "    filename = check_and_save(target,mv,DNN,True)\n",
    "    \n",
    "    # Find max F1 varying probability threshold, calculate modified performance, save\n",
    "    precision, recall, thresholds = precision_recall_curve(y_testing,p_te)\n",
    "    # f1 = 2*precision*recall/(precision+recall)  # Sometimes precision=recall=0!\n",
    "    p,r,t = zip(*[i for i in zip(precision,recall,np.append(thresholds,1)) if i[0:2]!=(0,0)])\n",
    "    p,r,t = np.array(p),np.array(r),np.array(t)\n",
    "    f1 = 2*p*r/(p+r)\n",
    "    m_idx = np.argmax(f1)\n",
    "    m_thresh = thresholds[m_idx]\n",
    "    y_hat_testing_adj=adjusted_classes(p_te,m_thresh)\n",
    "    mv=evaluate_model_predictions(target,'DNN_modT',m_thresh,y_testing,y_hat_testing_adj,\\\n",
    "                                  auc_te,average_precision)\n",
    "    if filename is None:\n",
    "        check_and_save(target,mv,DNN,True)\n",
    "    else:\n",
    "        check_and_save(target,mv,filename,True)\n",
    "    display(get_model_perfs(target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Performance Metrics Saving Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T05:43:49.580236Z",
     "start_time": "2019-12-02T05:43:49.383833Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "p_te = DNN.predict(x_te[rows_te])[:,0]\n",
    "y_testing=y_te[target][rows_te]\n",
    "auc_te = roc_auc_score(y_testing, p_te)\n",
    "print(\"%15s: %3.5f\" % (target, auc_te))\n",
    "\n",
    "y_hat_testing=DNN.predict_classes(x_te[rows_te])\n",
    "average_precision=average_precision_score(y_testing,p_te)\n",
    "mv=evaluate_model_predictions(target,'DNN',0.5,y_testing,y_hat_testing,\\\n",
    "                              auc_te,average_precision)\n",
    "#     check_and_save(target,mv,DNN,True)\n",
    "display(mv)\n",
    "precision, recall, thresholds = precision_recall_curve(y_testing,p_te)\n",
    "# f1 = 2*precision*recall/(precision+recall)\n",
    "p,r,t = zip(*[i for i in zip(precision,recall,np.append(thresholds,1)) if i[0:2]!=(0,0)])\n",
    "p,r,t = np.array(p),np.array(r),np.array(t)\n",
    "f1 = 2*p*r/(p+r)\n",
    "m_idx = np.argmax(f1)\n",
    "m_thresh = thresholds[m_idx]\n",
    "y_hat_testing_adj=adjusted_classes(p_te,m_thresh)\n",
    "mv=evaluate_model_predictions(target,'DNN_modT',m_thresh,y_testing,y_hat_testing_adj,\\\n",
    "                              auc_te,average_precision)\n",
    "#     check_and_save(target,mv,DNN,True)\n",
    "# display(get_model_perfs(target))\n",
    "display(mv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Troubleshooting Precision-Recall Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T05:08:08.069861Z",
     "start_time": "2019-12-02T05:08:08.008719Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "f1 = 2*precision*recall/(precision+recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T05:52:54.150467Z",
     "start_time": "2019-12-02T05:52:54.089918Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "a, b = zip(*zip(precision,recall))\n",
    "type(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T05:41:49.450684Z",
     "start_time": "2019-12-02T05:41:49.389635Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "p,r,t = zip(*[i for i in zip(precision,recall,np.append(thresholds,1)) if i[0:2]!=(0,0)])\n",
    "p,r,t = np.array(p),np.array(r),np.array(t)\n",
    "f1 = 2*p*r/(p+r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Testing DNN Model Saving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T00:48:15.368581Z",
     "start_time": "2019-12-03T00:48:15.274930Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DNN_test0.h5'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = save_model(target,'DNN_test',DNN,True)\n",
    "filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T00:48:29.774885Z",
     "start_time": "2019-12-03T00:48:28.661557Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.engine.sequential.Sequential at 0x1276ace10>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_model(target,filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T00:48:35.160766Z",
     "start_time": "2019-12-03T00:48:35.156689Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.5'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Saved DNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T22:38:21.204512Z",
     "start_time": "2019-12-09T22:38:20.549086Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/sean/anaconda3/envs/metis/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Users/sean/anaconda3/envs/metis/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /Users/sean/anaconda3/envs/metis/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "target = 'NR.PPAR.gamma'\n",
    "target = 'NR.AhR'\n",
    "DNN = read_model(target,'DNN_modT0.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T22:38:22.389131Z",
     "start_time": "2019-12-09T22:38:22.384199Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_144\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "h0_sigmoid_activation (Dense (None, 64)                105280    \n",
      "_________________________________________________________________\n",
      "Dropout0 (Dropout)           (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_144 (Dense)            (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 105,345\n",
      "Trainable params: 105,345\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "DNN.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance of Read DNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T22:38:24.577337Z",
     "start_time": "2019-12-09T22:38:24.503168Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         NR.AhR: 0.85306\n"
     ]
    }
   ],
   "source": [
    "rows_tr = np.isfinite(y_tr[target]).values\n",
    "rows_te = np.isfinite(y_te[target]).values\n",
    "y_testing=y_te[target][rows_te]\n",
    "p_te = DNN.predict(x_te[rows_te])[:,0]\n",
    "auc_te = roc_auc_score(y_te[target][rows_te], p_te)\n",
    "average_precision=average_precision_score(y_testing, p_te)\n",
    "print(\"%15s: %3.5f\" % (target, auc_te))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T22:38:25.835325Z",
     "start_time": "2019-12-09T22:38:25.798899Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model                                  DNN\n",
       "threshold                              0.5\n",
       "accuracy                               0.8\n",
       "precision                         0.347826\n",
       "recall                            0.767123\n",
       "f1                                0.478632\n",
       "auc_roc                           0.853065\n",
       "mcc                               0.420919\n",
       "avg_precision                      0.49692\n",
       "confusion_matrix    [[432, 105], [17, 56]]\n",
       "dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat_testing=DNN.predict_classes(x_te[rows_te])\n",
    "evaluate_model_predictions(target, 'DNN', 0.5, y_testing, y_hat_testing, auc_te, average_precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T22:38:28.371093Z",
     "start_time": "2019-12-09T22:38:28.367947Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T22:38:29.682198Z",
     "start_time": "2019-12-09T22:38:29.487150Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_te[target][rows_te], p_te)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "         lw=2, label='ROC curve (area = %0.3f)' % auc_te)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title(target+' Receiver operating characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix at Alternate Decision Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T22:38:31.413530Z",
     "start_time": "2019-12-09T22:38:31.390724Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision-Recall Curve \"Average Precision\": 0.4969\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "model                                 DNN\n",
       "threshold                          0.7035\n",
       "accuracy                         0.893443\n",
       "precision                        0.547619\n",
       "recall                           0.630137\n",
       "f1                               0.585987\n",
       "auc_roc                          0.853065\n",
       "mcc                              0.526888\n",
       "avg_precision                     0.49692\n",
       "confusion_matrix    [[499, 38], [27, 46]]\n",
       "dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decision_threshold = 0.7035\n",
    "y_testing=y_te[target][rows_te]\n",
    "y_hat_testing_adj=adjusted_classes(p_te,decision_threshold)\n",
    "print('Precision-Recall Curve \"Average Precision\": %0.4f' %average_precision)\n",
    "mv=evaluate_model_predictions(target,'DNN',decision_threshold,y_testing,y_hat_testing_adj,\\\n",
    "                              auc_te,average_precision)\n",
    "mv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision-Recall Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T22:38:33.545991Z",
     "start_time": "2019-12-09T22:38:33.535620Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max F1: 0.59494, threshold prob: 0.70347\n",
      "Max F1 Precision: 0.55294, Recall: 0.64384\n"
     ]
    }
   ],
   "source": [
    "precision, recall, thresholds = precision_recall_curve(y_testing,p_te)\n",
    "precision, recall, thresholds = zip(*[i for i in zip(precision,recall,np.append(thresholds,1)) if i[0:2]!=(0,0)])\n",
    "precision, recall, thresholds = np.array(precision),np.array(recall),np.array(thresholds)\n",
    "f1 = 2*precision*recall/(precision+recall)\n",
    "m_idx = np.argmax(f1)\n",
    "m_thresh = thresholds[m_idx]\n",
    "print('Max F1: %0.5f, threshold prob: %0.5f' % (f1[m_idx], m_thresh))\n",
    "print('Max F1 Precision: %0.5f, Recall: %0.5f' % (precision[m_idx],recall[m_idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T22:38:37.195218Z",
     "start_time": "2019-12-09T22:38:37.190641Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17328140139579773"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thresholds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T22:38:39.229275Z",
     "start_time": "2019-12-09T22:38:38.238256Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sean/anaconda3/envs/metis/lib/python3.7/site-packages/sklearn/metrics/classification.py:872: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
     ]
    }
   ],
   "source": [
    "mccs = []\n",
    "for th in thresholds:\n",
    "    y_hat_testing_adj=adjusted_classes(p_te,th)\n",
    "    mccs.append(matthews_corrcoef(y_testing,y_hat_testing_adj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T23:52:41.144276Z",
     "start_time": "2019-12-09T23:52:40.663604Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "no_skill = len(y_testing[y_testing==1]) / len(y_testing)\n",
    "fig = plt.figure(figsize=[8,8*4.8/6.4])\n",
    "plt.step(recall, precision, color='crimson', alpha=0.3, where='post')\n",
    "plt.fill_between(recall, precision, step='post', alpha=0.3, color='crimson',\\\n",
    "                 label='DNN')\n",
    "plt.plot([0, 1], [no_skill, no_skill], color='r', linestyle='--', label='No Skill:'+' %0.3f' % no_skill)\n",
    "plt.plot([0, 1], [precision[m_idx],precision[m_idx]], color='k', alpha=0.4, linestyle=':')\n",
    "plt.plot([recall[m_idx],recall[m_idx]],[0, 1], color='k', alpha=0.4,linestyle=':',\\\n",
    "         label='Prec/Rec @ Max F1')\n",
    "plt.text(recall[m_idx], f1[m_idx]+0.01, 'Max F1={0:0.3f}'.format(f1[m_idx]))\n",
    "plt.plot(recall,f1,color='k',label='F1')\n",
    "# plt.plot(recall,mccs,label=\"Matthew's Corr Coef\")\n",
    "plt.xlabel('Recall', fontsize=14)\n",
    "plt.ylabel('Precision or F1 Score', fontsize=14)\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.legend()\n",
    "title=target+' Neural Network\\n Precision-Recall curve: AP={0:0.2f}'.format(\n",
    "          average_precision)\n",
    "plt.title(title, fontsize=16);\n",
    "fig.savefig('./reports/figures/'+target+'_DNN_PrecisionRecallCurve2.svg',\\\n",
    "            format='svg', dpi=1200, transparent=True, bbox_inches = \"tight\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T09:09:41.402783Z",
     "start_time": "2019-12-09T09:09:41.392980Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.16704805491990846, 1.0, 0.28627450980392155, 0.23198324564311515, 0.17328140139579773)\n",
      "(0.1651376146788991, 0.9863013698630136, 0.2829076620825148, 0.22173369633779677, 0.17465224862098694)\n",
      "(0.16551724137931034, 0.9863013698630136, 0.28346456692913385, 0.2226895755682889, 0.1813332438468933)\n",
      "(0.16589861751152074, 0.9863013698630136, 0.28402366863905326, 0.22364576427189672, 0.1842179298400879)\n",
      "(0.16628175519630484, 0.9863013698630136, 0.2845849802371542, 0.2246022885079155, 0.18537762761116028)\n",
      "(0.16511627906976745, 0.9726027397260274, 0.2823061630218688, 0.216399972952515, 0.18541648983955383)\n",
      "(0.1655011655011655, 0.9726027397260274, 0.28286852589641437, 0.2173758670266673, 0.18947604298591614)\n",
      "(0.1658878504672897, 0.9726027397260274, 0.2834331337325349, 0.2183519976700012, 0.1915532648563385)\n",
      "(0.16627634660421545, 0.9726027397260274, 0.284, 0.21932839276507035, 0.19245806336402893)\n",
      "(0.16666666666666666, 0.9726027397260274, 0.2845691382765531, 0.22030508001372517, 0.19489645957946777)\n",
      "(0.16705882352941176, 0.9726027397260274, 0.285140562248996, 0.22128208694377893, 0.19898191094398499)\n",
      "(0.16745283018867924, 0.9726027397260274, 0.2857142857142857, 0.2222594409155313, 0.20502963662147522)\n",
      "(0.16784869976359337, 0.9726027397260274, 0.28629032258064513, 0.22323716912815372, 0.21005719900131226)\n",
      "(0.16587677725118483, 0.958904109589041, 0.2828282828282828, 0.21327709227902755, 0.21884813904762268)\n",
      "(0.166270783847981, 0.958904109589041, 0.2834008097165992, 0.21427167674288639, 0.21893590688705444)\n",
      "(0.16666666666666666, 0.958904109589041, 0.2839756592292089, 0.21526650922957605, 0.22046548128128052)\n",
      "(0.16467780429594273, 0.9452054794520548, 0.2804878048780488, 0.20537087420303693, 0.22505328059196472)\n",
      "(0.16507177033492823, 0.9452054794520548, 0.28105906313645623, 0.2063817021262738, 0.22538992762565613)\n",
      "(0.16546762589928057, 0.9452054794520548, 0.2816326530612245, 0.20739266406115034, 0.22658228874206543)\n",
      "(0.1658653846153846, 0.9452054794520548, 0.2822085889570552, 0.20840379054879396, 0.22743678092956543)\n",
      "(0.16626506024096385, 0.9452054794520548, 0.2827868852459016, 0.20941511193011525, 0.2280375361442566)\n",
      "(0.16666666666666666, 0.9452054794520548, 0.28336755646817247, 0.21042665835305346, 0.2308761477470398)\n",
      "(0.16707021791767554, 0.9452054794520548, 0.2839506172839506, 0.21143845977967277, 0.230894535779953)\n",
      "(0.16747572815533981, 0.9452054794520548, 0.2845360824742268, 0.21245054599311503, 0.23091742396354675)\n",
      "(0.1678832116788321, 0.9452054794520548, 0.28512396694214875, 0.21346294660441392, 0.23093682527542114)\n",
      "(0.16829268292682928, 0.9452054794520548, 0.28571428571428575, 0.2144756910591756, 0.23449993133544922)\n",
      "(0.1687041564792176, 0.9452054794520548, 0.28630705394190875, 0.21548880864413, 0.2361835241317749)\n",
      "(0.16911764705882354, 0.9452054794520548, 0.28690228690228686, 0.2165023284935588, 0.24313059449195862)\n",
      "(0.16953316953316952, 0.9452054794520548, 0.2875, 0.2175162795956027, 0.24752092361450195)\n",
      "(0.16995073891625614, 0.9452054794520548, 0.2881002087682672, 0.21853069079845414, 0.2511894702911377)\n",
      "(0.17037037037037037, 0.9452054794520548, 0.2887029288702929, 0.21954559081643807, 0.2520563304424286)\n",
      "(0.1707920792079208, 0.9452054794520548, 0.289308176100629, 0.2205610082359862, 0.25265181064605713)\n",
      "(0.17121588089330025, 0.9452054794520548, 0.28991596638655465, 0.2215769715215076, 0.25850510597229004)\n",
      "(0.17164179104477612, 0.9452054794520548, 0.2905263157894737, 0.2225935090211599, 0.26115885376930237)\n",
      "(0.17336683417085427, 0.9452054794520548, 0.2929936305732484, 0.22666596437071188, 0.2611609101295471)\n",
      "(0.17380352644836272, 0.9452054794520548, 0.29361702127659567, 0.2276857944868091, 0.26182693243026733)\n",
      "(0.17424242424242425, 0.9452054794520548, 0.2942430703624734, 0.22870636677627437, 0.2626449763774872)\n",
      "(0.17468354430379746, 0.9452054794520548, 0.2948717948717948, 0.22972770892749622, 0.26353734731674194)\n",
      "(0.1751269035532995, 0.9452054794520548, 0.29550321199143476, 0.23074984855562236, 0.2664661407470703)\n",
      "(0.17557251908396945, 0.9452054794520548, 0.296137339055794, 0.23177281320758947, 0.2691808342933655)\n",
      "(0.1760204081632653, 0.9452054794520548, 0.29677419354838713, 0.23279663036708545, 0.2701396942138672)\n",
      "(0.17647058823529413, 0.9452054794520548, 0.2974137931034483, 0.23382132745944795, 0.27947187423706055)\n",
      "(0.17692307692307693, 0.9452054794520548, 0.2980561555075594, 0.2348469318565005, 0.28066617250442505)\n",
      "(0.17737789203084833, 0.9452054794520548, 0.2987012987012987, 0.2358734708813301, 0.2826305031776428)\n",
      "(0.17783505154639176, 0.9452054794520548, 0.299349240780911, 0.23690097181300804, 0.2844478189945221)\n",
      "(0.17829457364341086, 0.9452054794520548, 0.3, 0.2379294618912565, 0.2878362834453583)\n",
      "(0.17922077922077922, 0.9452054794520548, 0.3013100436681223, 0.23998951827724943, 0.28785502910614014)\n",
      "(0.1796875, 0.9452054794520548, 0.30196936542669583, 0.24102113890898402, 0.28961360454559326)\n",
      "(0.18110236220472442, 0.9452054794520548, 0.3039647577092512, 0.24412269605809403, 0.29591673612594604)\n",
      "(0.18157894736842106, 0.9452054794520548, 0.30463576158940403, 0.24515887052643942, 0.29658931493759155)\n",
      "(0.1820580474934037, 0.9452054794520548, 0.30530973451327437, 0.2461962511865791, 0.3028804063796997)\n",
      "(0.18253968253968253, 0.9452054794520548, 0.30598669623059865, 0.24723486512631915, 0.30307096242904663)\n",
      "(0.1830238726790451, 0.9452054794520548, 0.30666666666666664, 0.24827473943831163, 0.30376410484313965)\n",
      "(0.18351063829787234, 0.9452054794520548, 0.30734966592427615, 0.24931590122427322, 0.30378958582878113)\n",
      "(0.184, 0.9452054794520548, 0.3080357142857143, 0.2503583775991756, 0.31390249729156494)\n",
      "(0.18449197860962566, 0.9452054794520548, 0.30872483221476504, 0.25140219569540917, 0.3148218095302582)\n",
      "(0.18498659517426275, 0.9452054794520548, 0.3094170403587444, 0.2524473826669214, 0.3149172365665436)\n",
      "(0.18548387096774194, 0.9452054794520548, 0.3101123595505618, 0.25349396569333293, 0.32115602493286133)\n",
      "(0.18598382749326145, 0.9452054794520548, 0.3108108108108108, 0.25454197198403106, 0.3213396668434143)\n",
      "(0.1837837837837838, 0.9315068493150684, 0.30699774266365687, 0.2452525182015308, 0.3221105933189392)\n",
      "(0.1842818428184282, 0.9315068493150684, 0.30769230769230776, 0.24631095429795052, 0.32274746894836426)\n",
      "(0.18478260869565216, 0.9315068493150684, 0.30839002267573695, 0.24737076327577912, 0.3255764842033386)\n",
      "(0.19101123595505617, 0.9315068493150684, 0.317016317016317, 0.2602058511245016, 0.327288955450058)\n",
      "(0.192090395480226, 0.9315068493150684, 0.3185011709601874, 0.2623680951392541, 0.3273174464702606)\n",
      "(0.19263456090651557, 0.9315068493150684, 0.31924882629107976, 0.2634519038366168, 0.3282669484615326)\n",
      "(0.19318181818181818, 0.9315068493150684, 0.32, 0.26453754243433975, 0.335477352142334)\n",
      "(0.19428571428571428, 0.9315068493150684, 0.3215130023640662, 0.26671442622519625, 0.3356223702430725)\n",
      "(0.19484240687679083, 0.9315068493150684, 0.3222748815165877, 0.2678057301189512, 0.33631378412246704)\n",
      "(0.19653179190751446, 0.9315068493150684, 0.324582338902148, 0.27109144426703724, 0.3374090790748596)\n",
      "(0.19710144927536233, 0.9315068493150684, 0.3253588516746411, 0.2721907156066147, 0.3415788412094116)\n",
      "(0.19767441860465115, 0.9315068493150684, 0.32613908872901676, 0.2732920534625334, 0.34907305240631104)\n",
      "(0.19825072886297376, 0.9315068493150684, 0.3269230769230769, 0.2743954879488086, 0.35145437717437744)\n",
      "(0.19883040935672514, 0.9315068493150684, 0.32771084337349393, 0.2755010493129492, 0.3523406684398651)\n",
      "(0.19941348973607037, 0.9315068493150684, 0.32850241545893716, 0.27660876794025185, 0.3578851521015167)\n",
      "(0.2, 0.9315068493150684, 0.3292978208232446, 0.2777186743581163, 0.35950174927711487)\n",
      "(0.20058997050147492, 0.9315068493150684, 0.3300970873786408, 0.27883079924038284, 0.3601006865501404)\n",
      "(0.20118343195266272, 0.9315068493150684, 0.3309002433090025, 0.2799451734116934, 0.3634076714515686)\n",
      "(0.20178041543026706, 0.9315068493150684, 0.3317073170731707, 0.2810618278518785, 0.36704957485198975)\n",
      "(0.22388059701492538, 0.821917808219178, 0.3519061583577713, 0.2842095116928305, 0.3719310760498047)\n",
      "(0.2247191011235955, 0.821917808219178, 0.35294117647058826, 0.28554421284083487, 0.3727127015590668)\n",
      "(0.22556390977443608, 0.821917808219178, 0.3539823008849557, 0.28688319052989053, 0.3776320517063141)\n",
      "(0.22641509433962265, 0.821917808219178, 0.35502958579881655, 0.2882264987905576, 0.37996989488601685)\n",
      "(0.22727272727272727, 0.821917808219178, 0.3560830860534125, 0.2895741922240921, 0.381315678358078)\n",
      "(0.22813688212927757, 0.821917808219178, 0.35714285714285715, 0.2909263260137553, 0.3820660412311554)\n",
      "(0.22900763358778625, 0.821917808219178, 0.3582089552238806, 0.2922829559363349, 0.38274821639060974)\n",
      "(0.22988505747126436, 0.821917808219178, 0.35928143712574856, 0.2936441383738822, 0.38496947288513184)\n",
      "(0.23076923076923078, 0.821917808219178, 0.36036036036036034, 0.2950099303256722, 0.3851257860660553)\n",
      "(0.23166023166023167, 0.821917808219178, 0.3614457831325301, 0.29638038942039235, 0.385318398475647)\n",
      "(0.23255813953488372, 0.821917808219178, 0.36253776435045315, 0.29775557392856566, 0.3880103528499603)\n",
      "(0.22957198443579765, 0.8082191780821918, 0.35757575757575755, 0.2889066801095456, 0.3955206274986267)\n",
      "(0.23046875, 0.8082191780821918, 0.3586626139817629, 0.2902860200856487, 0.39750370383262634)\n",
      "(0.23137254901960785, 0.8082191780821918, 0.35975609756097565, 0.29167014235656474, 0.40052223205566406)\n",
      "(0.23228346456692914, 0.8082191780821918, 0.3608562691131499, 0.2930591073083078, 0.40716466307640076)\n",
      "(0.233201581027668, 0.8082191780821918, 0.36196319018404904, 0.29445297602014026, 0.40972521901130676)\n",
      "(0.23412698412698413, 0.8082191780821918, 0.3630769230769231, 0.2958518102783651, 0.4136120080947876)\n",
      "(0.2350597609561753, 0.8082191780821918, 0.3641975308641976, 0.29725567259039654, 0.41694873571395874)\n",
      "(0.236, 0.8082191780821918, 0.3653250773993808, 0.29866462619911543, 0.4180801510810852)\n",
      "(0.23694779116465864, 0.8082191780821918, 0.36645962732919257, 0.3000787350975182, 0.41919639706611633)\n",
      "(0.23577235772357724, 0.7945205479452054, 0.3636363636363637, 0.2940567758257069, 0.42404794692993164)\n",
      "(0.23265306122448978, 0.7808219178082192, 0.3584905660377359, 0.2851825957360278, 0.43023058772087097)\n",
      "(0.296875, 0.7808219178082192, 0.43018867924528303, 0.37001087127925764, 0.4336148798465729)\n",
      "(0.29842931937172773, 0.7808219178082192, 0.43181818181818177, 0.3718385809811919, 0.43379145860671997)\n",
      "(0.3, 0.7808219178082192, 0.4334600760456273, 0.37367691386197166, 0.43380308151245117)\n",
      "(0.30158730158730157, 0.7808219178082192, 0.43511450381679384, 0.37552601958122805, 0.4338268041610718)\n",
      "(0.30319148936170215, 0.7808219178082192, 0.43678160919540227, 0.37738605045438156, 0.44090986251831055)\n",
      "(0.3048128342245989, 0.7808219178082192, 0.4384615384615385, 0.379257161516972, 0.44210466742515564)\n",
      "(0.3064516129032258, 0.7808219178082192, 0.44015444015444016, 0.3811395105908674, 0.44248050451278687)\n",
      "(0.3081081081081081, 0.7808219178082192, 0.44186046511627913, 0.38303325835241836, 0.4425780475139618)\n",
      "(0.30978260869565216, 0.7808219178082192, 0.443579766536965, 0.3849385684026259, 0.44821488857269287)\n",
      "(0.3131868131868132, 0.7808219178082192, 0.4470588235294118, 0.3887845448319498, 0.4482244551181793)\n",
      "(0.3149171270718232, 0.7808219178082192, 0.44881889763779526, 0.39072555369747913, 0.44839003682136536)\n",
      "(0.31666666666666665, 0.7808219178082192, 0.450592885375494, 0.392678809980109, 0.44890159368515015)\n",
      "(0.31843575418994413, 0.7808219178082192, 0.4523809523809524, 0.3946444930322696, 0.44973453879356384)\n",
      "(0.3202247191011236, 0.7808219178082192, 0.4541832669322709, 0.3966227855985539, 0.450889527797699)\n",
      "(0.3220338983050847, 0.7808219178082192, 0.45599999999999996, 0.39861387390215625, 0.45164141058921814)\n",
      "(0.32386363636363635, 0.7808219178082192, 0.45783132530120485, 0.4006179477339859, 0.45217472314834595)\n",
      "(0.32571428571428573, 0.7808219178082192, 0.45967741935483875, 0.4026352005445552, 0.45397713780403137)\n",
      "(0.3275862068965517, 0.7808219178082192, 0.46153846153846156, 0.4046658295387445, 0.45853912830352783)\n",
      "(0.32947976878612717, 0.7808219178082192, 0.46341463414634143, 0.4067100357735539, 0.4633287787437439)\n",
      "(0.3313953488372093, 0.7808219178082192, 0.46530612244897956, 0.4087680242589512, 0.4638950824737549)\n",
      "(0.3333333333333333, 0.7808219178082192, 0.4672131147540984, 0.41084000406193644, 0.46691304445266724)\n",
      "(0.3352941176470588, 0.7808219178082192, 0.4691358024691358, 0.41292618841394346, 0.47025978565216064)\n",
      "(0.33727810650887574, 0.7808219178082192, 0.47107438016528924, 0.4150267948217078, 0.47523051500320435)\n",
      "(0.3413173652694611, 0.7808219178082192, 0.4750000000000001, 0.41927216589849775, 0.4752364754676819)\n",
      "(0.3433734939759036, 0.7808219178082192, 0.4769874476987447, 0.42141738800654555, 0.4776079058647156)\n",
      "(0.34545454545454546, 0.7808219178082192, 0.4789915966386555, 0.4235779472966141, 0.47870761156082153)\n",
      "(0.3475609756097561, 0.7808219178082192, 0.48101265822784806, 0.42575408444596025, 0.4796389937400818)\n",
      "(0.34355828220858897, 0.7671232876712328, 0.47457627118644063, 0.4165321547440876, 0.4827778935432434)\n",
      "(0.345679012345679, 0.7671232876712328, 0.4765957446808511, 0.4187178011201899, 0.4845709800720215)\n",
      "(0.34782608695652173, 0.7671232876712328, 0.4786324786324786, 0.42091948729564005, 0.5028659105300903)\n",
      "(0.35, 0.7671232876712328, 0.48068669527896984, 0.4231374707334297, 0.5120404958724976)\n",
      "(0.3522012578616352, 0.7671232876712328, 0.48275862068965514, 0.42537201440845973, 0.5121070742607117)\n",
      "(0.35443037974683544, 0.7671232876712328, 0.48484848484848486, 0.4276233869641479, 0.5136734843254089)\n",
      "(0.35668789808917195, 0.7671232876712328, 0.48695652173913045, 0.4298918628744842, 0.5142155885696411)\n",
      "(0.358974358974359, 0.7671232876712328, 0.48908296943231433, 0.4321777226117572, 0.5142219662666321)\n",
      "(0.36129032258064514, 0.7671232876712328, 0.4912280701754385, 0.4344812528201906, 0.5179610252380371)\n",
      "(0.36363636363636365, 0.7671232876712328, 0.49339207048458145, 0.43680274649573786, 0.5200366973876953)\n",
      "(0.3660130718954248, 0.7671232876712328, 0.49557522123893805, 0.439142503172294, 0.5200368165969849)\n",
      "(0.3618421052631579, 0.7534246575342466, 0.48888888888888893, 0.42982395148019603, 0.5264448523521423)\n",
      "(0.36423841059602646, 0.7534246575342466, 0.49107142857142855, 0.4321753274482815, 0.5319575071334839)\n",
      "(0.36666666666666664, 0.7534246575342466, 0.4932735426008969, 0.4345455640235713, 0.5340980291366577)\n",
      "(0.3691275167785235, 0.7534246575342466, 0.4954954954954955, 0.4369349835255295, 0.5355691313743591)\n",
      "(0.3716216216216216, 0.7534246575342466, 0.4977375565610859, 0.43934391571820525, 0.5400340557098389)\n",
      "(0.3741496598639456, 0.7534246575342466, 0.5, 0.4417726980372222, 0.5410366654396057)\n",
      "(0.3698630136986301, 0.7397260273972602, 0.4931506849315068, 0.43238456145109644, 0.5411461591720581)\n",
      "(0.3724137931034483, 0.7397260273972602, 0.4954128440366973, 0.43482611946858185, 0.5431700944900513)\n",
      "(0.375, 0.7397260273972602, 0.4976958525345623, 0.43728821191848627, 0.5432511568069458)\n",
      "(0.3776223776223776, 0.7397260273972602, 0.5, 0.4397712031544869, 0.5433512330055237)\n",
      "(0.38028169014084506, 0.7397260273972602, 0.5023255813953488, 0.44227546631508113, 0.5433924794197083)\n",
      "(0.3829787234042553, 0.7397260273972602, 0.5046728971962616, 0.4448013836026574, 0.5435643196105957)\n",
      "(0.38571428571428573, 0.7397260273972602, 0.5070422535211268, 0.4473493465734601, 0.5491951704025269)\n",
      "(0.38848920863309355, 0.7397260273972602, 0.5094339622641509, 0.4499197564389544, 0.5498624444007874)\n",
      "(0.391304347826087, 0.7397260273972602, 0.5118483412322274, 0.4525130243791265, 0.5549741387367249)\n",
      "(0.39416058394160586, 0.7397260273972602, 0.5142857142857143, 0.4551295718682841, 0.5551242828369141)\n",
      "(0.39705882352941174, 0.7397260273972602, 0.5167464114832535, 0.45776983101394964, 0.5551245808601379)\n",
      "(0.4, 0.7397260273972602, 0.5192307692307693, 0.46043424490947676, 0.5551498532295227)\n",
      "(0.40298507462686567, 0.7397260273972602, 0.5217391304347826, 0.46312326800105114, 0.558928906917572)\n",
      "(0.40601503759398494, 0.7397260273972602, 0.5242718446601942, 0.46583736646977547, 0.5596955418586731)\n",
      "(0.4015151515151515, 0.726027397260274, 0.5170731707317072, 0.4563116572596843, 0.5599369406700134)\n",
      "(0.40458015267175573, 0.726027397260274, 0.5196078431372549, 0.45904348719619065, 0.5644659399986267)\n",
      "(0.4, 0.7123287671232876, 0.5123152709359605, 0.4494678111186665, 0.5647518038749695)\n",
      "(0.40310077519379844, 0.7123287671232876, 0.5148514851485148, 0.452217451469057, 0.5698819756507874)\n",
      "(0.40625, 0.7123287671232876, 0.5174129353233831, 0.4549936928788712, 0.5821796655654907)\n",
      "(0.4094488188976378, 0.7123287671232876, 0.52, 0.4577970619991535, 0.5833263397216797)\n",
      "(0.4126984126984127, 0.7123287671232876, 0.5226130653266331, 0.46062809984755, 0.5848428606987)\n",
      "(0.416, 0.7123287671232876, 0.5252525252525253, 0.46348736232246696, 0.5879695415496826)\n",
      "(0.41935483870967744, 0.7123287671232876, 0.5279187817258884, 0.46637542073989785, 0.5905325412750244)\n",
      "(0.42276422764227645, 0.7123287671232876, 0.5306122448979592, 0.4692928623941122, 0.5988749265670776)\n",
      "(0.4180327868852459, 0.6986301369863014, 0.5230769230769231, 0.45961354539097243, 0.5992032885551453)\n",
      "(0.4214876033057851, 0.6986301369863014, 0.5257731958762886, 0.4625524836528188, 0.6000864505767822)\n",
      "(0.425, 0.6986301369863014, 0.5284974093264249, 0.4655220873514139, 0.6016438603401184)\n",
      "(0.42857142857142855, 0.6986301369863014, 0.53125, 0.4685230019448543, 0.605911910533905)\n",
      "(0.4322033898305085, 0.6986301369863014, 0.5340314136125655, 0.47155589172162327, 0.607475757598877)\n",
      "(0.4358974358974359, 0.6986301369863014, 0.5368421052631579, 0.4746214405202779, 0.6074758172035217)\n",
      "(0.4396551724137931, 0.6986301369863014, 0.5396825396825397, 0.4777203524830585, 0.607756495475769)\n",
      "(0.4434782608695652, 0.6986301369863014, 0.5425531914893617, 0.48085335284532826, 0.6141749620437622)\n",
      "(0.4473684210526316, 0.6986301369863014, 0.5454545454545455, 0.48402118876287875, 0.6158145666122437)\n",
      "(0.45132743362831856, 0.6986301369863014, 0.5483870967741936, 0.4872246301792675, 0.615862250328064)\n",
      "(0.44642857142857145, 0.684931506849315, 0.5405405405405406, 0.4774190653483623, 0.6209880709648132)\n",
      "(0.44144144144144143, 0.6712328767123288, 0.532608695652174, 0.4675597290529613, 0.6261598467826843)\n",
      "(0.44545454545454544, 0.6712328767123288, 0.5355191256830601, 0.47078242319401425, 0.6312482357025146)\n",
      "(0.44954128440366975, 0.6712328767123288, 0.5384615384615385, 0.47404258074605204, 0.6399688720703125)\n",
      "(0.4537037037037037, 0.6712328767123288, 0.5414364640883979, 0.4773410668274493, 0.6504337191581726)\n",
      "(0.45794392523364486, 0.6712328767123288, 0.5444444444444444, 0.48067877443095997, 0.6526235342025757)\n",
      "(0.46226415094339623, 0.6712328767123288, 0.5474860335195532, 0.48405662559904206, 0.6549820899963379)\n",
      "(0.4666666666666667, 0.6712328767123288, 0.550561797752809, 0.4874755726603849, 0.6602199077606201)\n",
      "(0.47115384615384615, 0.6712328767123288, 0.5536723163841807, 0.49093659953144553, 0.663005530834198)\n",
      "(0.47572815533980584, 0.6712328767123288, 0.5568181818181819, 0.49444072308707887, 0.6661838293075562)\n",
      "(0.4803921568627451, 0.6712328767123288, 0.5599999999999999, 0.4979889946046414, 0.6677386164665222)\n",
      "(0.48514851485148514, 0.6712328767123288, 0.5632183908045977, 0.5015825012862766, 0.6680070161819458)\n",
      "(0.49, 0.6712328767123288, 0.5664739884393063, 0.5052223678644394, 0.6685200333595276)\n",
      "(0.494949494949495, 0.6712328767123288, 0.569767441860465, 0.5089097582960982, 0.6685491800308228)\n",
      "(0.5, 0.6712328767123288, 0.5730994152046783, 0.5126458775514693, 0.6696735620498657)\n",
      "(0.5051546391752577, 0.6712328767123288, 0.5764705882352941, 0.5164319735035874, 0.67081618309021)\n",
      "(0.5104166666666666, 0.6712328767123288, 0.5798816568047337, 0.5202693389255078, 0.6714624166488647)\n",
      "(0.5157894736842106, 0.6712328767123288, 0.5833333333333334, 0.5241593136024689, 0.6749551296234131)\n",
      "(0.5212765957446809, 0.6712328767123288, 0.5868263473053892, 0.5281032865669272, 0.6763989925384521)\n",
      "(0.5268817204301075, 0.6712328767123288, 0.5903614457831325, 0.5321026984650106, 0.6812368631362915)\n",
      "(0.5217391304347826, 0.6575342465753424, 0.5818181818181818, 0.5220459424463529, 0.6812376976013184)\n",
      "(0.5274725274725275, 0.6575342465753424, 0.5853658536585367, 0.5260971181786139, 0.6918179988861084)\n",
      "(0.5333333333333333, 0.6575342465753424, 0.588957055214724, 0.5302072168374239, 0.6931205987930298)\n",
      "(0.5393258426966292, 0.6575342465753424, 0.5925925925925926, 0.5343778794974818, 0.6961268782615662)\n",
      "(0.5340909090909091, 0.6438356164383562, 0.5838509316770186, 0.5242359169169968, 0.699643611907959)\n",
      "(0.5402298850574713, 0.6438356164383562, 0.5875, 0.5284643417793875, 0.7009394764900208)\n",
      "(0.5465116279069767, 0.6438356164383562, 0.5911949685534591, 0.5327573419224498, 0.7015763521194458)\n",
      "(0.5529411764705883, 0.6438356164383562, 0.5949367088607594, 0.5371167987126113, 0.703474760055542)\n",
      "(0.5476190476190477, 0.6301369863013698, 0.5859872611464968, 0.5268875474612481, 0.7047172784805298)\n",
      "(0.5421686746987951, 0.6164383561643836, 0.576923076923077, 0.5165806816519808, 0.7116811871528625)\n",
      "(0.5365853658536586, 0.6027397260273972, 0.567741935483871, 0.5061939017981382, 0.7117826342582703)\n",
      "(0.5308641975308642, 0.589041095890411, 0.5584415584415585, 0.4957248182676853, 0.7145785093307495)\n",
      "(0.5375, 0.589041095890411, 0.5620915032679739, 0.5001332455961321, 0.7175037860870361)\n",
      "(0.5443037974683544, 0.589041095890411, 0.5657894736842106, 0.5046147317167998, 0.7195552587509155)\n",
      "(0.5512820512820513, 0.589041095890411, 0.5695364238410597, 0.5091715920181218, 0.7261609435081482)\n",
      "(0.5584415584415584, 0.589041095890411, 0.5733333333333333, 0.5138062458110418, 0.7385987043380737)\n",
      "(0.5526315789473685, 0.5753424657534246, 0.563758389261745, 0.5032278298380919, 0.7396842241287231)\n",
      "(0.56, 0.5753424657534246, 0.5675675675675674, 0.5079385514567926, 0.7407654523849487)\n",
      "(0.5540540540540541, 0.5616438356164384, 0.5578231292517007, 0.4972633719731432, 0.743514895439148)\n",
      "(0.5616438356164384, 0.5616438356164384, 0.5616438356164384, 0.5020535190428815, 0.7512584924697876)\n",
      "(0.5555555555555556, 0.547945205479452, 0.5517241379310345, 0.491277332912799, 0.7515912055969238)\n",
      "(0.5633802816901409, 0.547945205479452, 0.5555555555555555, 0.4961505307570091, 0.7522017955780029)\n",
      "(0.5714285714285714, 0.547945205479452, 0.5594405594405595, 0.5011154061974199, 0.7522019147872925)\n",
      "(0.5588235294117647, 0.5205479452054794, 0.5390070921985816, 0.4792370046719041, 0.7522023320198059)\n",
      "(0.5522388059701493, 0.5068493150684932, 0.5285714285714286, 0.4681357486130678, 0.7537569999694824)\n",
      "(0.5606060606060606, 0.5068493150684932, 0.5323741007194245, 0.4731810010325658, 0.7537797689437866)\n",
      "(0.5692307692307692, 0.5068493150684932, 0.536231884057971, 0.47832825567542336, 0.753780722618103)\n",
      "(0.578125, 0.5068493150684932, 0.5401459854014599, 0.48358143814563354, 0.7537810802459717)\n",
      "(0.5873015873015873, 0.5068493150684932, 0.5441176470588236, 0.4889446893084347, 0.7537811398506165)\n",
      "(0.5806451612903226, 0.4931506849315068, 0.5333333333333333, 0.47770781348396407, 0.7537870407104492)\n",
      "(0.5737704918032787, 0.4794520547945205, 0.5223880597014925, 0.46634780979230545, 0.754169225692749)\n",
      "(0.5833333333333334, 0.4794520547945205, 0.5263157894736842, 0.471819949156107, 0.7552183866500854)\n",
      "(0.5932203389830508, 0.4794520547945205, 0.5303030303030303, 0.47741456662462617, 0.7633869647979736)\n",
      "(0.5862068965517241, 0.4657534246575342, 0.5190839694656488, 0.4659182608758292, 0.7639918923377991)\n",
      "(0.5789473684210527, 0.4520547945205479, 0.5076923076923077, 0.4542857922397978, 0.7660188674926758)\n",
      "(0.5892857142857143, 0.4520547945205479, 0.5116279069767442, 0.46000340164634734, 0.7682046890258789)\n",
      "(0.6, 0.4520547945205479, 0.515625, 0.46585836768084427, 0.7785959839820862)\n",
      "(0.6111111111111112, 0.4520547945205479, 0.5196850393700787, 0.47185694015035906, 0.7831449508666992)\n",
      "(0.6037735849056604, 0.4383561643835616, 0.507936507936508, 0.4600743128476988, 0.7891136407852173)\n",
      "(0.6153846153846154, 0.4383561643835616, 0.512, 0.46622514312940966, 0.7992188930511475)\n",
      "(0.6078431372549019, 0.4246575342465753, 0.4999999999999999, 0.45428921649007115, 0.7994481325149536)\n",
      "(0.6, 0.410958904109589, 0.4878048780487805, 0.4421911395490339, 0.7995601892471313)\n",
      "(0.5918367346938775, 0.3972602739726027, 0.47540983606557374, 0.4299236121530034, 0.7995797395706177)\n",
      "(0.6041666666666666, 0.3972602739726027, 0.4793388429752066, 0.43623710856590525, 0.7995799779891968)\n",
      "(0.5957446808510638, 0.3835616438356164, 0.4666666666666667, 0.4237884678175238, 0.7995803952217102)\n",
      "(0.5869565217391305, 0.3698630136986301, 0.453781512605042, 0.41115151968174446, 0.799659252166748)\n",
      "(0.6, 0.3698630136986301, 0.45762711864406774, 0.4176390319369136, 0.8004468083381653)\n",
      "(0.5909090909090909, 0.3561643835616438, 0.4444444444444445, 0.404798346803754, 0.8010056614875793)\n",
      "(0.5813953488372093, 0.3424657534246575, 0.43103448275862066, 0.39174699518237865, 0.8020696043968201)\n",
      "(0.5714285714285714, 0.3287671232876712, 0.41739130434782606, 0.3784740365125331, 0.8085611462593079)\n",
      "(0.5853658536585366, 0.3287671232876712, 0.42105263157894735, 0.3851389490657178, 0.8114547729492188)\n",
      "(0.575, 0.3150684931506849, 0.4070796460176991, 0.3716191302286109, 0.8154312372207642)\n",
      "(0.5897435897435898, 0.3150684931506849, 0.4107142857142857, 0.37849434427245765, 0.8156155347824097)\n",
      "(0.5789473684210527, 0.3013698630136986, 0.39639639639639634, 0.3647103399453675, 0.8161847591400146)\n",
      "(0.5675675675675675, 0.2876712328767123, 0.3818181818181817, 0.35065614226542297, 0.8162030577659607)\n",
      "(0.5833333333333334, 0.2876712328767123, 0.3853211009174312, 0.3577480895454716, 0.8162057399749756)\n",
      "(0.6, 0.2876712328767123, 0.3888888888888889, 0.3651061490130469, 0.8222827315330505)\n",
      "(0.5882352941176471, 0.273972602739726, 0.37383177570093457, 0.35073380080321387, 0.824730634689331)\n",
      "(0.5757575757575758, 0.2602739726027397, 0.3584905660377358, 0.33604430362909005, 0.8259192705154419)\n",
      "(0.59375, 0.2602739726027397, 0.3619047619047619, 0.34367030939302357, 0.8278613090515137)\n",
      "(0.6129032258064516, 0.2602739726027397, 0.36538461538461536, 0.3516197615442867, 0.8333424925804138)\n",
      "(0.6333333333333333, 0.2602739726027397, 0.36893203883495146, 0.35991890306534524, 0.8337457180023193)\n",
      "(0.6206896551724138, 0.2465753424657534, 0.35294117647058826, 0.3448618163481686, 0.8345327377319336)\n",
      "(0.6071428571428571, 0.2328767123287671, 0.33663366336633666, 0.32941799963625634, 0.8347922563552856)\n",
      "(0.5925925925925926, 0.2191780821917808, 0.32, 0.313557339948705, 0.8368188142776489)\n",
      "(0.6153846153846154, 0.2191780821917808, 0.3232323232323232, 0.3222488447007484, 0.8368812799453735)\n",
      "(0.6, 0.2054794520547945, 0.3061224489795918, 0.30592228802269406, 0.8392333984375)\n",
      "(0.625, 0.2054794520547945, 0.30927835051546393, 0.315073099429182, 0.8409807682037354)\n",
      "(0.6086956521739131, 0.1917808219178082, 0.29166666666666663, 0.2982331239975658, 0.846588134765625)\n",
      "(0.5909090909090909, 0.1780821917808219, 0.2736842105263158, 0.2808298609337591, 0.8532367944717407)\n",
      "(0.5714285714285714, 0.1643835616438356, 0.2553191489361702, 0.26280741511945294, 0.8537203073501587)\n",
      "(0.55, 0.1506849315068493, 0.23655913978494625, 0.24410109457453164, 0.8537205457687378)\n",
      "(0.5263157894736842, 0.136986301369863, 0.21739130434782608, 0.2246354665754645, 0.8537259101867676)\n",
      "(0.5, 0.1232876712328767, 0.19780219780219777, 0.20432184952152485, 0.8538134694099426)\n",
      "(0.5294117647058824, 0.1232876712328767, 0.2, 0.21374026859538542, 0.8568501472473145)\n",
      "(0.5, 0.1095890410958904, 0.1797752808988764, 0.19231190982771443, 0.8622720241546631)\n",
      "(0.5333333333333333, 0.1095890410958904, 0.18181818181818182, 0.2023546545849771, 0.8652635812759399)\n",
      "(0.5714285714285714, 0.1095890410958904, 0.1839080459770115, 0.21331750855352405, 0.8660643100738525)\n",
      "(0.5384615384615384, 0.0958904109589041, 0.1627906976744186, 0.19039759616679794, 0.8697233200073242)\n",
      "(0.5833333333333334, 0.0958904109589041, 0.16470588235294117, 0.2023587788935551, 0.872649073600769)\n",
      "(0.5454545454545454, 0.0821917808219178, 0.14285714285714285, 0.17776731533644813, 0.898523211479187)\n",
      "(0.5, 0.0684931506849315, 0.12048192771084336, 0.15127382447685542, 0.9055663347244263)\n",
      "(0.5555555555555556, 0.0684931506849315, 0.1219512195121951, 0.1643371061004847, 0.9058200120925903)\n",
      "(0.625, 0.0684931506849315, 0.12345679012345677, 0.17947388347530574, 0.905823826789856)\n",
      "(0.7142857142857143, 0.0684931506849315, 0.125, 0.19738150309845418, 0.9058247208595276)\n",
      "(0.6666666666666666, 0.0547945205479452, 0.10126582278481013, 0.16796600999030734, 0.9105516672134399)\n",
      "(0.6, 0.0410958904109589, 0.07692307692307691, 0.13453223656299196, 0.9157296419143677)\n",
      "(0.75, 0.0410958904109589, 0.07792207792207792, 0.15777618649678907, 0.9169957637786865)\n",
      "(1.0, 0.0410958904109589, 0.07894736842105263, 0.1906742270845093, 0.9172015190124512)\n",
      "(1.0, 0.0273972602739726, 0.05333333333333332, 0.15555677149019512, 0.9172077178955078)\n",
      "(1.0, 0.0136986301369863, 0.027027027027027025, 0.10990490279208061, 0.9172743558883667)\n",
      "(1.0, 0.0, 0.0, 0.0, 1.0)\n"
     ]
    }
   ],
   "source": [
    "for i in zip(precision,recall,f1,mccs,np.append(thresholds,1)):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-06T19:15:05.178430Z",
     "start_time": "2019-12-06T19:15:05.169778Z"
    }
   },
   "source": [
    "## Compare Predictions with Actually Toxic Compounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T09:20:03.395375Z",
     "start_time": "2019-12-09T09:20:03.380869Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max F1 Decision Threshold: 0.7035\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NR.AhR</th>\n",
       "      <th>DNN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>NCGC00261776-01</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>NCGC00261662-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>NCGC00261119-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>NCGC00260831-01</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>NCGC00261395-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>NCGC00357175-01</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>NCGC00356994-01</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>NCGC00357111-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>NCGC00357249-01</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>NCGC00357051-01</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>73 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 NR.AhR  DNN\n",
       "NCGC00261776-01       1    1\n",
       "NCGC00261662-01       1    0\n",
       "NCGC00261119-01       1    0\n",
       "NCGC00260831-01       1    1\n",
       "NCGC00261395-01       1    0\n",
       "...                 ...  ...\n",
       "NCGC00357175-01       1    1\n",
       "NCGC00356994-01       1    1\n",
       "NCGC00357111-01       1    0\n",
       "NCGC00357249-01       1    1\n",
       "NCGC00357051-01       1    1\n",
       "\n",
       "[73 rows x 2 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Max F1 Decision Threshold: %0.4f' % m_thresh)\n",
    "y_hat_testing_adj=adjusted_classes(p_te,m_thresh)\n",
    "y_hat=pd.DataFrame(y_hat_testing_adj,columns=['DNN'],index=y_te[rows_te].index)\n",
    "compare_TP = pd.concat([y_te[target][rows_te].astype('int'), y_hat],axis=1)\n",
    "compare_TP[compare_TP[target]==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
