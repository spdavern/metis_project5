{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T08:25:09.823164Z",
     "start_time": "2019-12-09T08:25:09.819032Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import io\n",
    "# from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, f1_score,\\\n",
    "                            precision_score, recall_score, accuracy_score,\\\n",
    "                            average_precision_score, precision_recall_curve,\\\n",
    "                            matthews_corrcoef, roc_curve\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from src.helper_functions import load_data, get_model_perfs, init_model_perfs,\\\n",
    "                                 save_model, save_model_perfs, check_is_best,\\\n",
    "                                 read_model, evaluate_model_predictions,\\\n",
    "                                 update_model_perfs, check_and_save,\\\n",
    "                                 adjusted_classes, mcc\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T08:25:15.804495Z",
     "start_time": "2019-12-09T08:25:15.796316Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.5'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "from keras.callbacks import Callback\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T08:25:29.310759Z",
     "start_time": "2019-12-09T08:25:17.938782Z"
    }
   },
   "outputs": [],
   "source": [
    "x_tr, y_tr, x_te, y_te = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Neural Network\n",
    "\n",
    "Following the desciption in section 2.2.4 of the [DeepTox article](https://www.frontiersin.org/articles/10.3389/fenvs.2015.00080/full), I tried to use intermediate values in [Table 2](https://www.frontiersin.org/articles/10.3389/fenvs.2015.00080/full#T2) to build the neural network:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following [this question/answer](https://datascience.stackexchange.com/questions/45165/how-to-get-accuracy-f1-precision-and-recall-for-a-keras-model) and [this question/answer](https://stackoverflow.com/questions/54065733/how-to-employ-the-scikit-learn-evaluation-metrics-functions-with-keras-in-python) to implement usage of recall in model training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T00:31:25.480771Z",
     "start_time": "2019-12-03T00:31:25.327888Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/sean/anaconda3/envs/metis/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Users/sean/anaconda3/envs/metis/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "h0_sigmoid_activation (Dense (None, 1024)              1684480   \n",
      "_________________________________________________________________\n",
      "Dropout0 (Dropout)           (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "h1_sigmoid_activation (Dense (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "Dropout1 (Dropout)           (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "h2_sigmoid_activation (Dense (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "Dropout2 (Dropout)           (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 1025      \n",
      "=================================================================\n",
      "Total params: 3,784,705\n",
      "Trainable params: 3,784,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "drop_out=0.5    # DeepTox range: 0.5, 0.2, 0\n",
    "L2_reg = 0.0001 # Default = 0.01\n",
    "layers = 3      # DeepTox range: 1, 2, 3, 4\n",
    "act = 'sigmoid' # Consider sigmoid and tanh\n",
    "neurons = 1024  # DeepTox range: 1024, 2048, 4096, 8192, 16384\n",
    "# Info on decay: https://datascience.stackexchange.com/questions/26112/decay-parameter-in-keras-optimizers\n",
    "decay = 0       # DeepTox range: 10^-4, 10^-5, 10^-6\n",
    "learn_rate = 0.1  #Research appropriate range\n",
    "DNN = keras.Sequential()\n",
    "DNN.add(Dense(neurons, activation=act,input_shape=x_tr.shape[1:],name='h0_'+act+'_activation'))\n",
    "DNN.add(Dropout(rate=drop_out,name='Dropout0'))\n",
    "for i in range(1,layers):\n",
    "    DNN.add(Dense(units=neurons, activation=act,\\\n",
    "                  name='h'+str(i)+'_'+act+'_activation',\\\n",
    "                  kernel_regularizer=keras.regularizers.l2(L2_reg)))\n",
    "    DNN.add(Dropout(rate=drop_out,name='Dropout'+str(i)))\n",
    "DNN.add(Dense(units=1, activation='sigmoid'))\n",
    "keras.optimizers.Adam(lr=learn_rate, beta_1=0.9,\\\n",
    "                      beta_2=0.999, decay=decay, amsgrad=False)\n",
    "DNN.compile(optimizer='adam', loss='binary_crossentropy',\\\n",
    "            metrics=['accuracy'])\n",
    "DNN.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Train DNN's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T00:45:37.293279Z",
     "start_time": "2019-12-03T00:31:53.696343Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/sean/anaconda3/envs/metis/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 11934 samples, validate on 1689 samples\n",
      "Epoch 1/100\n",
      "11934/11934 [==============================] - 3s 227us/step - loss: 1.1133 - acc: 0.5215 - val_loss: 0.7070 - val_acc: 0.8840\n",
      "Epoch 2/100\n",
      "11934/11934 [==============================] - 2s 175us/step - loss: 0.8746 - acc: 0.5747 - val_loss: 0.8319 - val_acc: 0.4938\n",
      "Epoch 3/100\n",
      "11934/11934 [==============================] - 2s 181us/step - loss: 0.7788 - acc: 0.6629 - val_loss: 0.7846 - val_acc: 0.5595\n",
      "Epoch 4/100\n",
      "11934/11934 [==============================] - 3s 210us/step - loss: 0.7400 - acc: 0.6865 - val_loss: 0.6602 - val_acc: 0.7105\n",
      "Epoch 5/100\n",
      "11934/11934 [==============================] - 3s 210us/step - loss: 0.7156 - acc: 0.6886 - val_loss: 0.5235 - val_acc: 0.8502\n",
      "Epoch 6/100\n",
      "11934/11934 [==============================] - 2s 200us/step - loss: 0.6984 - acc: 0.7056 - val_loss: 0.8266 - val_acc: 0.5317\n",
      "Epoch 7/100\n",
      "11934/11934 [==============================] - 2s 201us/step - loss: 0.6794 - acc: 0.7106 - val_loss: 0.5944 - val_acc: 0.7821\n",
      "Epoch 8/100\n",
      "11934/11934 [==============================] - 2s 201us/step - loss: 0.6682 - acc: 0.7123 - val_loss: 0.6837 - val_acc: 0.6092\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 9/100\n",
      "11934/11934 [==============================] - 2s 203us/step - loss: 0.6513 - acc: 0.7200 - val_loss: 0.7557 - val_acc: 0.5921\n",
      "Epoch 10/100\n",
      "11934/11934 [==============================] - 2s 205us/step - loss: 0.6357 - acc: 0.7319 - val_loss: 0.6479 - val_acc: 0.6750\n",
      "Epoch 11/100\n",
      "11934/11934 [==============================] - 2s 205us/step - loss: 0.6234 - acc: 0.7360 - val_loss: 0.5659 - val_acc: 0.8040\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 12/100\n",
      "11934/11934 [==============================] - 2s 205us/step - loss: 0.6146 - acc: 0.7387 - val_loss: 0.5735 - val_acc: 0.7916\n",
      "Epoch 13/100\n",
      "11934/11934 [==============================] - 2s 203us/step - loss: 0.6016 - acc: 0.7479 - val_loss: 0.6328 - val_acc: 0.6809\n",
      "Epoch 14/100\n",
      "11934/11934 [==============================] - 2s 206us/step - loss: 0.5965 - acc: 0.7479 - val_loss: 0.5107 - val_acc: 0.8378\n",
      "Epoch 15/100\n",
      "11934/11934 [==============================] - 3s 214us/step - loss: 0.5918 - acc: 0.7517 - val_loss: 0.5822 - val_acc: 0.7762\n",
      "Epoch 16/100\n",
      "11934/11934 [==============================] - 3s 210us/step - loss: 0.5871 - acc: 0.7556 - val_loss: 0.6022 - val_acc: 0.7010\n",
      "Epoch 17/100\n",
      "11934/11934 [==============================] - 2s 208us/step - loss: 0.5775 - acc: 0.7600 - val_loss: 0.5301 - val_acc: 0.8188\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 18/100\n",
      "11934/11934 [==============================] - 2s 207us/step - loss: 0.5706 - acc: 0.7588 - val_loss: 0.5502 - val_acc: 0.7833\n",
      "Epoch 19/100\n",
      "11934/11934 [==============================] - 3s 211us/step - loss: 0.5669 - acc: 0.7599 - val_loss: 0.5472 - val_acc: 0.7709\n",
      "Epoch 20/100\n",
      "11934/11934 [==============================] - 3s 212us/step - loss: 0.5605 - acc: 0.7637 - val_loss: 0.5162 - val_acc: 0.8265\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 21/100\n",
      "11934/11934 [==============================] - 2s 209us/step - loss: 0.5540 - acc: 0.7702 - val_loss: 0.5447 - val_acc: 0.7910\n",
      "Epoch 22/100\n",
      "11934/11934 [==============================] - 3s 215us/step - loss: 0.5547 - acc: 0.7665 - val_loss: 0.5377 - val_acc: 0.7845\n",
      "Epoch 23/100\n",
      "11934/11934 [==============================] - 3s 215us/step - loss: 0.5521 - acc: 0.7691 - val_loss: 0.5375 - val_acc: 0.7904\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 24/100\n",
      "11934/11934 [==============================] - 3s 210us/step - loss: 0.5515 - acc: 0.7733 - val_loss: 0.5496 - val_acc: 0.7815\n",
      "Epoch 25/100\n",
      "11934/11934 [==============================] - 3s 212us/step - loss: 0.5456 - acc: 0.7753 - val_loss: 0.5423 - val_acc: 0.7975\n",
      "Epoch 26/100\n",
      "11934/11934 [==============================] - 3s 210us/step - loss: 0.5473 - acc: 0.7756 - val_loss: 0.5340 - val_acc: 0.8011\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "Epoch 27/100\n",
      "11934/11934 [==============================] - 3s 214us/step - loss: 0.5476 - acc: 0.7745 - val_loss: 0.5473 - val_acc: 0.7869\n",
      "Epoch 28/100\n",
      "11934/11934 [==============================] - 3s 215us/step - loss: 0.5416 - acc: 0.7775 - val_loss: 0.5436 - val_acc: 0.7946\n",
      "Epoch 29/100\n",
      "11934/11934 [==============================] - 2s 208us/step - loss: 0.5424 - acc: 0.7764 - val_loss: 0.5403 - val_acc: 0.8005\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "Epoch 30/100\n",
      "11934/11934 [==============================] - 2s 205us/step - loss: 0.5402 - acc: 0.7745 - val_loss: 0.5377 - val_acc: 0.8034\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00030: early stopping\n",
      "         NR.AhR: 0.86472\n",
      "Model saved and metrics table updated.\n",
      "Model saved and metrics table updated.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>threshold</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>auc_roc</th>\n",
       "      <th>avg_precision</th>\n",
       "      <th>confusion_matrix</th>\n",
       "      <th>model_filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.911475</td>\n",
       "      <td>0.756757</td>\n",
       "      <td>0.383562</td>\n",
       "      <td>0.509091</td>\n",
       "      <td>0.905028</td>\n",
       "      <td>0.602102</td>\n",
       "      <td>[[528, 9], [45, 28]]</td>\n",
       "      <td>RF0.joblib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>RF_modT</td>\n",
       "      <td>0.235000</td>\n",
       "      <td>0.842623</td>\n",
       "      <td>0.424837</td>\n",
       "      <td>0.890411</td>\n",
       "      <td>0.575221</td>\n",
       "      <td>0.905028</td>\n",
       "      <td>0.602102</td>\n",
       "      <td>[[449, 88], [8, 65]]</td>\n",
       "      <td>RF_modT0.joblib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>DNN</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.814754</td>\n",
       "      <td>0.364865</td>\n",
       "      <td>0.739726</td>\n",
       "      <td>0.488688</td>\n",
       "      <td>0.864723</td>\n",
       "      <td>0.524576</td>\n",
       "      <td>[[443, 94], [19, 54]]</td>\n",
       "      <td>DNN0.h5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>DNN_modT</td>\n",
       "      <td>0.773516</td>\n",
       "      <td>0.901639</td>\n",
       "      <td>0.603175</td>\n",
       "      <td>0.520548</td>\n",
       "      <td>0.558824</td>\n",
       "      <td>0.864723</td>\n",
       "      <td>0.524576</td>\n",
       "      <td>[[512, 25], [35, 38]]</td>\n",
       "      <td>DNN_modT0.h5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      model  threshold  accuracy  precision    recall        f1   auc_roc  \\\n",
       "0        RF   0.500000  0.911475   0.756757  0.383562  0.509091  0.905028   \n",
       "1   RF_modT   0.235000  0.842623   0.424837  0.890411  0.575221  0.905028   \n",
       "2       DNN   0.500000  0.814754   0.364865  0.739726  0.488688  0.864723   \n",
       "3  DNN_modT   0.773516  0.901639   0.603175  0.520548  0.558824  0.864723   \n",
       "\n",
       "   avg_precision       confusion_matrix   model_filename  \n",
       "0       0.602102   [[528, 9], [45, 28]]       RF0.joblib  \n",
       "1       0.602102   [[449, 88], [8, 65]]  RF_modT0.joblib  \n",
       "2       0.524576  [[443, 94], [19, 54]]          DNN0.h5  \n",
       "3       0.524576  [[512, 25], [35, 38]]     DNN_modT0.h5  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14834 samples, validate on 1931 samples\n",
      "Epoch 1/100\n",
      "14834/14834 [==============================] - 3s 221us/step - loss: 1.0468 - acc: 0.4847 - val_loss: 0.9818 - val_acc: 0.4443\n",
      "Epoch 2/100\n",
      "14834/14834 [==============================] - 3s 221us/step - loss: 0.9139 - acc: 0.5450 - val_loss: 0.9631 - val_acc: 0.4459\n",
      "Epoch 3/100\n",
      "14834/14834 [==============================] - 3s 215us/step - loss: 0.8575 - acc: 0.5810 - val_loss: 0.8899 - val_acc: 0.4858\n",
      "Epoch 4/100\n",
      "14834/14834 [==============================] - 3s 217us/step - loss: 0.8338 - acc: 0.5866 - val_loss: 0.8453 - val_acc: 0.5049\n",
      "Epoch 5/100\n",
      "14834/14834 [==============================] - 3s 217us/step - loss: 0.8115 - acc: 0.6066 - val_loss: 0.8269 - val_acc: 0.5308\n",
      "Epoch 6/100\n",
      "14834/14834 [==============================] - 3s 209us/step - loss: 0.7909 - acc: 0.6201 - val_loss: 0.8087 - val_acc: 0.5526\n",
      "Epoch 7/100\n",
      "14834/14834 [==============================] - 3s 212us/step - loss: 0.7809 - acc: 0.6283 - val_loss: 0.7948 - val_acc: 0.5769\n",
      "Epoch 8/100\n",
      "14834/14834 [==============================] - 3s 207us/step - loss: 0.7699 - acc: 0.6393 - val_loss: 0.7865 - val_acc: 0.6100\n",
      "Epoch 9/100\n",
      "14834/14834 [==============================] - 3s 209us/step - loss: 0.7588 - acc: 0.6507 - val_loss: 0.7722 - val_acc: 0.6277\n",
      "Epoch 10/100\n",
      "14834/14834 [==============================] - 3s 212us/step - loss: 0.7521 - acc: 0.6536 - val_loss: 0.7701 - val_acc: 0.6282\n",
      "Epoch 11/100\n",
      "14834/14834 [==============================] - 3s 209us/step - loss: 0.7520 - acc: 0.6553 - val_loss: 0.7625 - val_acc: 0.6308\n",
      "Epoch 12/100\n",
      "14834/14834 [==============================] - 3s 207us/step - loss: 0.7426 - acc: 0.6642 - val_loss: 0.7514 - val_acc: 0.6349\n",
      "Epoch 13/100\n",
      "14834/14834 [==============================] - 3s 208us/step - loss: 0.7341 - acc: 0.6741 - val_loss: 0.7471 - val_acc: 0.6359\n",
      "Epoch 14/100\n",
      "14834/14834 [==============================] - 3s 211us/step - loss: 0.7322 - acc: 0.6757 - val_loss: 0.7426 - val_acc: 0.6401\n",
      "Epoch 15/100\n",
      "14834/14834 [==============================] - 3s 205us/step - loss: 0.7234 - acc: 0.6853 - val_loss: 0.7338 - val_acc: 0.6432\n",
      "Epoch 16/100\n",
      "14834/14834 [==============================] - 3s 203us/step - loss: 0.7211 - acc: 0.6891 - val_loss: 0.7311 - val_acc: 0.6432\n",
      "Epoch 17/100\n",
      "14834/14834 [==============================] - 3s 208us/step - loss: 0.7185 - acc: 0.6904 - val_loss: 0.7301 - val_acc: 0.6484\n",
      "Epoch 18/100\n",
      "14834/14834 [==============================] - 3s 204us/step - loss: 0.7134 - acc: 0.6927 - val_loss: 0.7207 - val_acc: 0.6530\n",
      "Epoch 19/100\n",
      "14834/14834 [==============================] - 3s 204us/step - loss: 0.7119 - acc: 0.6976 - val_loss: 0.7194 - val_acc: 0.6546\n",
      "Epoch 20/100\n",
      "14834/14834 [==============================] - 3s 205us/step - loss: 0.7103 - acc: 0.6972 - val_loss: 0.7152 - val_acc: 0.6587\n",
      "Epoch 21/100\n",
      "14834/14834 [==============================] - 3s 201us/step - loss: 0.7053 - acc: 0.7008 - val_loss: 0.7118 - val_acc: 0.6598\n",
      "Epoch 22/100\n",
      "14834/14834 [==============================] - 3s 219us/step - loss: 0.7015 - acc: 0.7063 - val_loss: 0.7116 - val_acc: 0.6608\n",
      "Epoch 23/100\n",
      "14834/14834 [==============================] - 3s 205us/step - loss: 0.7019 - acc: 0.7053 - val_loss: 0.7125 - val_acc: 0.6613\n",
      "Epoch 24/100\n",
      "14834/14834 [==============================] - 3s 212us/step - loss: 0.6991 - acc: 0.7087 - val_loss: 0.7069 - val_acc: 0.6639\n",
      "Epoch 25/100\n",
      "14834/14834 [==============================] - 3s 202us/step - loss: 0.6962 - acc: 0.7134 - val_loss: 0.7000 - val_acc: 0.6665\n",
      "Epoch 26/100\n",
      "14834/14834 [==============================] - 3s 204us/step - loss: 0.6957 - acc: 0.7093 - val_loss: 0.7000 - val_acc: 0.6655\n",
      "Epoch 27/100\n",
      "14834/14834 [==============================] - 3s 200us/step - loss: 0.6924 - acc: 0.7108 - val_loss: 0.6999 - val_acc: 0.6649\n",
      "Epoch 28/100\n",
      "14834/14834 [==============================] - 3s 207us/step - loss: 0.6893 - acc: 0.7152 - val_loss: 0.6927 - val_acc: 0.6696\n",
      "Epoch 29/100\n",
      "14834/14834 [==============================] - 3s 212us/step - loss: 0.6886 - acc: 0.7161 - val_loss: 0.6924 - val_acc: 0.6706\n",
      "Epoch 30/100\n",
      "14834/14834 [==============================] - 3s 203us/step - loss: 0.6873 - acc: 0.7136 - val_loss: 0.6918 - val_acc: 0.6712\n",
      "Epoch 31/100\n",
      "14834/14834 [==============================] - 3s 202us/step - loss: 0.6852 - acc: 0.7179 - val_loss: 0.6865 - val_acc: 0.6743\n",
      "Epoch 32/100\n",
      "14834/14834 [==============================] - 3s 201us/step - loss: 0.6816 - acc: 0.7201 - val_loss: 0.6897 - val_acc: 0.6727\n",
      "Epoch 33/100\n",
      "14834/14834 [==============================] - 3s 211us/step - loss: 0.6827 - acc: 0.7188 - val_loss: 0.6831 - val_acc: 0.6774\n",
      "Epoch 34/100\n",
      "14834/14834 [==============================] - 3s 218us/step - loss: 0.6815 - acc: 0.7245 - val_loss: 0.6882 - val_acc: 0.6753\n",
      "Epoch 35/100\n",
      "14834/14834 [==============================] - 3s 200us/step - loss: 0.6784 - acc: 0.7219 - val_loss: 0.6830 - val_acc: 0.6789\n",
      "Epoch 36/100\n",
      "14834/14834 [==============================] - 3s 205us/step - loss: 0.6777 - acc: 0.7226 - val_loss: 0.6808 - val_acc: 0.6800\n",
      "Epoch 37/100\n",
      "14834/14834 [==============================] - 3s 201us/step - loss: 0.6747 - acc: 0.7241 - val_loss: 0.6827 - val_acc: 0.6794\n",
      "Epoch 38/100\n",
      "14834/14834 [==============================] - 3s 202us/step - loss: 0.6711 - acc: 0.7267 - val_loss: 0.6767 - val_acc: 0.6815\n",
      "Epoch 39/100\n",
      "14834/14834 [==============================] - 3s 211us/step - loss: 0.6739 - acc: 0.7264 - val_loss: 0.6728 - val_acc: 0.6836\n",
      "Epoch 40/100\n",
      "14834/14834 [==============================] - 3s 218us/step - loss: 0.6725 - acc: 0.7226 - val_loss: 0.6704 - val_acc: 0.6846\n",
      "Epoch 41/100\n",
      "14834/14834 [==============================] - 3s 221us/step - loss: 0.6710 - acc: 0.7265 - val_loss: 0.6765 - val_acc: 0.6831\n",
      "Epoch 42/100\n",
      "14834/14834 [==============================] - 3s 209us/step - loss: 0.6683 - acc: 0.7299 - val_loss: 0.6699 - val_acc: 0.6882\n",
      "Epoch 43/100\n",
      "14834/14834 [==============================] - 3s 214us/step - loss: 0.6661 - acc: 0.7270 - val_loss: 0.6664 - val_acc: 0.6898\n",
      "Epoch 44/100\n",
      "14834/14834 [==============================] - 3s 200us/step - loss: 0.6609 - acc: 0.7363 - val_loss: 0.6657 - val_acc: 0.6919\n",
      "Epoch 45/100\n",
      "14834/14834 [==============================] - 3s 206us/step - loss: 0.6661 - acc: 0.7308 - val_loss: 0.6673 - val_acc: 0.6914\n",
      "Epoch 46/100\n",
      "14834/14834 [==============================] - 3s 205us/step - loss: 0.6634 - acc: 0.7304 - val_loss: 0.6627 - val_acc: 0.6929\n",
      "Epoch 47/100\n",
      "14834/14834 [==============================] - 3s 202us/step - loss: 0.6588 - acc: 0.7340 - val_loss: 0.6579 - val_acc: 0.6950\n",
      "Epoch 48/100\n",
      "14834/14834 [==============================] - 3s 207us/step - loss: 0.6595 - acc: 0.7340 - val_loss: 0.6645 - val_acc: 0.6929\n",
      "Epoch 49/100\n",
      "14834/14834 [==============================] - 3s 211us/step - loss: 0.6568 - acc: 0.7337 - val_loss: 0.6578 - val_acc: 0.6965\n",
      "Epoch 50/100\n",
      "14834/14834 [==============================] - 3s 207us/step - loss: 0.6545 - acc: 0.7350 - val_loss: 0.6500 - val_acc: 0.6996\n",
      "Epoch 51/100\n",
      "14834/14834 [==============================] - 3s 206us/step - loss: 0.6552 - acc: 0.7355 - val_loss: 0.6539 - val_acc: 0.6981\n",
      "Epoch 52/100\n",
      "14834/14834 [==============================] - 3s 206us/step - loss: 0.6546 - acc: 0.7355 - val_loss: 0.6441 - val_acc: 0.7033\n",
      "Epoch 53/100\n",
      "14834/14834 [==============================] - 3s 204us/step - loss: 0.6525 - acc: 0.7329 - val_loss: 0.6566 - val_acc: 0.6991\n",
      "Epoch 54/100\n",
      "14834/14834 [==============================] - 3s 201us/step - loss: 0.6486 - acc: 0.7361 - val_loss: 0.6474 - val_acc: 0.7027\n",
      "Epoch 55/100\n",
      "14834/14834 [==============================] - 3s 213us/step - loss: 0.6481 - acc: 0.7407 - val_loss: 0.6474 - val_acc: 0.7043\n",
      "\n",
      "Epoch 00055: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "Epoch 56/100\n",
      "14834/14834 [==============================] - 3s 202us/step - loss: 0.6482 - acc: 0.7383 - val_loss: 0.6472 - val_acc: 0.7038\n",
      "Epoch 57/100\n",
      "14834/14834 [==============================] - 3s 204us/step - loss: 0.6509 - acc: 0.7384 - val_loss: 0.6416 - val_acc: 0.7074\n",
      "Epoch 58/100\n",
      "14834/14834 [==============================] - 3s 202us/step - loss: 0.6473 - acc: 0.7371 - val_loss: 0.6424 - val_acc: 0.7090\n",
      "Epoch 59/100\n",
      "14834/14834 [==============================] - 3s 220us/step - loss: 0.6479 - acc: 0.7403 - val_loss: 0.6392 - val_acc: 0.7095\n",
      "Epoch 60/100\n",
      "14834/14834 [==============================] - 3s 203us/step - loss: 0.6470 - acc: 0.7397 - val_loss: 0.6413 - val_acc: 0.7084\n",
      "Epoch 61/100\n",
      "14834/14834 [==============================] - 3s 203us/step - loss: 0.6461 - acc: 0.7375 - val_loss: 0.6384 - val_acc: 0.7100\n",
      "Epoch 62/100\n",
      "14834/14834 [==============================] - 3s 205us/step - loss: 0.6441 - acc: 0.7381 - val_loss: 0.6373 - val_acc: 0.7100\n",
      "Epoch 63/100\n",
      "14834/14834 [==============================] - 3s 214us/step - loss: 0.6458 - acc: 0.7405 - val_loss: 0.6407 - val_acc: 0.7090\n",
      "Epoch 64/100\n",
      "14834/14834 [==============================] - 3s 209us/step - loss: 0.6437 - acc: 0.7392 - val_loss: 0.6404 - val_acc: 0.7084\n",
      "Epoch 65/100\n",
      "14834/14834 [==============================] - 3s 207us/step - loss: 0.6416 - acc: 0.7410 - val_loss: 0.6358 - val_acc: 0.7141\n",
      "Epoch 66/100\n",
      "14834/14834 [==============================] - 3s 208us/step - loss: 0.6403 - acc: 0.7414 - val_loss: 0.6347 - val_acc: 0.7147\n",
      "Epoch 67/100\n",
      "14834/14834 [==============================] - 3s 203us/step - loss: 0.6412 - acc: 0.7417 - val_loss: 0.6335 - val_acc: 0.7147\n",
      "Epoch 68/100\n",
      "14834/14834 [==============================] - 3s 203us/step - loss: 0.6412 - acc: 0.7392 - val_loss: 0.6373 - val_acc: 0.7136\n",
      "Epoch 69/100\n",
      "14834/14834 [==============================] - 3s 201us/step - loss: 0.6404 - acc: 0.7396 - val_loss: 0.6307 - val_acc: 0.7157\n",
      "Epoch 70/100\n",
      "14834/14834 [==============================] - 3s 206us/step - loss: 0.6370 - acc: 0.7425 - val_loss: 0.6339 - val_acc: 0.7141\n",
      "Epoch 71/100\n",
      "14834/14834 [==============================] - 3s 204us/step - loss: 0.6425 - acc: 0.7370 - val_loss: 0.6329 - val_acc: 0.7141\n",
      "Epoch 72/100\n",
      "14834/14834 [==============================] - 3s 207us/step - loss: 0.6358 - acc: 0.7437 - val_loss: 0.6319 - val_acc: 0.7162\n",
      "\n",
      "Epoch 00072: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "Epoch 73/100\n",
      "14834/14834 [==============================] - 3s 200us/step - loss: 0.6385 - acc: 0.7396 - val_loss: 0.6294 - val_acc: 0.7172\n",
      "Epoch 74/100\n",
      "14834/14834 [==============================] - 3s 204us/step - loss: 0.6360 - acc: 0.7432 - val_loss: 0.6295 - val_acc: 0.7172\n",
      "Epoch 75/100\n",
      "14834/14834 [==============================] - 3s 202us/step - loss: 0.6371 - acc: 0.7459 - val_loss: 0.6288 - val_acc: 0.7178\n",
      "Epoch 76/100\n",
      "14834/14834 [==============================] - 3s 210us/step - loss: 0.6363 - acc: 0.7440 - val_loss: 0.6299 - val_acc: 0.7178\n",
      "Epoch 77/100\n",
      "14834/14834 [==============================] - 3s 204us/step - loss: 0.6368 - acc: 0.7387 - val_loss: 0.6263 - val_acc: 0.7188\n",
      "Epoch 78/100\n",
      "14834/14834 [==============================] - 3s 205us/step - loss: 0.6379 - acc: 0.7449 - val_loss: 0.6283 - val_acc: 0.7183\n",
      "Epoch 79/100\n",
      "14834/14834 [==============================] - 3s 207us/step - loss: 0.6361 - acc: 0.7454 - val_loss: 0.6271 - val_acc: 0.7193\n",
      "Epoch 80/100\n",
      "14834/14834 [==============================] - 3s 207us/step - loss: 0.6364 - acc: 0.7444 - val_loss: 0.6230 - val_acc: 0.7214\n",
      "Epoch 81/100\n",
      "14834/14834 [==============================] - 3s 201us/step - loss: 0.6374 - acc: 0.7439 - val_loss: 0.6251 - val_acc: 0.7204\n",
      "Epoch 82/100\n",
      "14834/14834 [==============================] - 3s 205us/step - loss: 0.6359 - acc: 0.7435 - val_loss: 0.6254 - val_acc: 0.7198\n",
      "Epoch 83/100\n",
      "14834/14834 [==============================] - 3s 203us/step - loss: 0.6355 - acc: 0.7449 - val_loss: 0.6255 - val_acc: 0.7204\n",
      "\n",
      "Epoch 00083: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
      "Epoch 84/100\n",
      "14834/14834 [==============================] - 3s 204us/step - loss: 0.6337 - acc: 0.7447 - val_loss: 0.6267 - val_acc: 0.7193\n",
      "Epoch 85/100\n",
      "14834/14834 [==============================] - 3s 205us/step - loss: 0.6345 - acc: 0.7437 - val_loss: 0.6248 - val_acc: 0.7209\n",
      "Epoch 86/100\n",
      "14834/14834 [==============================] - 3s 206us/step - loss: 0.6341 - acc: 0.7436 - val_loss: 0.6255 - val_acc: 0.7204\n",
      "\n",
      "Epoch 00086: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
      "Epoch 87/100\n",
      "14834/14834 [==============================] - 3s 202us/step - loss: 0.6353 - acc: 0.7441 - val_loss: 0.6252 - val_acc: 0.7209\n",
      "Epoch 88/100\n",
      "14834/14834 [==============================] - 3s 208us/step - loss: 0.6361 - acc: 0.7421 - val_loss: 0.6250 - val_acc: 0.7214\n",
      "Epoch 89/100\n",
      "14834/14834 [==============================] - 3s 208us/step - loss: 0.6325 - acc: 0.7479 - val_loss: 0.6248 - val_acc: 0.7214\n",
      "\n",
      "Epoch 00089: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
      "Epoch 90/100\n",
      "14834/14834 [==============================] - 3s 204us/step - loss: 0.6316 - acc: 0.7439 - val_loss: 0.6247 - val_acc: 0.7214\n",
      "Epoch 91/100\n",
      "14834/14834 [==============================] - 3s 203us/step - loss: 0.6362 - acc: 0.7445 - val_loss: 0.6245 - val_acc: 0.7219\n",
      "Epoch 92/100\n",
      "14834/14834 [==============================] - 3s 210us/step - loss: 0.6372 - acc: 0.7421 - val_loss: 0.6245 - val_acc: 0.7219\n",
      "\n",
      "Epoch 00092: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.\n",
      "Epoch 93/100\n",
      "14834/14834 [==============================] - 3s 205us/step - loss: 0.6326 - acc: 0.7457 - val_loss: 0.6245 - val_acc: 0.7214\n",
      "Epoch 94/100\n",
      "14834/14834 [==============================] - 3s 207us/step - loss: 0.6328 - acc: 0.7454 - val_loss: 0.6247 - val_acc: 0.7214\n",
      "Epoch 95/100\n",
      "14834/14834 [==============================] - 3s 208us/step - loss: 0.6340 - acc: 0.7419 - val_loss: 0.6246 - val_acc: 0.7214\n",
      "\n",
      "Epoch 00095: ReduceLROnPlateau reducing learning rate to 6.103515914901436e-08.\n",
      "Epoch 96/100\n",
      "14834/14834 [==============================] - 3s 207us/step - loss: 0.6356 - acc: 0.7445 - val_loss: 0.6246 - val_acc: 0.7219\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00096: early stopping\n",
      "          NR.AR: 0.74151\n",
      "Model saved and metrics table updated.\n",
      "Model saved and metrics table updated.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>threshold</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>auc_roc</th>\n",
       "      <th>avg_precision</th>\n",
       "      <th>confusion_matrix</th>\n",
       "      <th>model_filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.981229</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.678934</td>\n",
       "      <td>0.232723</td>\n",
       "      <td>[[573, 1], [10, 2]]</td>\n",
       "      <td>RF0.joblib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>RF_modT</td>\n",
       "      <td>0.730000</td>\n",
       "      <td>0.982935</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.678934</td>\n",
       "      <td>0.232723</td>\n",
       "      <td>[[574, 0], [10, 2]]</td>\n",
       "      <td>RF_modT0.joblib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>DNN</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.721843</td>\n",
       "      <td>0.042424</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.079096</td>\n",
       "      <td>0.741507</td>\n",
       "      <td>0.168663</td>\n",
       "      <td>[[416, 158], [5, 7]]</td>\n",
       "      <td>DNN0.h5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>DNN_modT</td>\n",
       "      <td>0.792447</td>\n",
       "      <td>0.965870</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.741507</td>\n",
       "      <td>0.168663</td>\n",
       "      <td>[[562, 12], [8, 4]]</td>\n",
       "      <td>DNN_modT0.h5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      model  threshold  accuracy  precision    recall        f1   auc_roc  \\\n",
       "0        RF   0.500000  0.981229   0.666667  0.166667  0.266667  0.678934   \n",
       "1   RF_modT   0.730000  0.982935   1.000000  0.166667  0.285714  0.678934   \n",
       "2       DNN   0.500000  0.721843   0.042424  0.583333  0.079096  0.741507   \n",
       "3  DNN_modT   0.792447  0.965870   0.250000  0.333333  0.285714  0.741507   \n",
       "\n",
       "   avg_precision      confusion_matrix   model_filename  \n",
       "0       0.232723   [[573, 1], [10, 2]]       RF0.joblib  \n",
       "1       0.232723   [[574, 0], [10, 2]]  RF_modT0.joblib  \n",
       "2       0.168663  [[416, 158], [5, 7]]          DNN0.h5  \n",
       "3       0.168663   [[562, 12], [8, 4]]     DNN_modT0.h5  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13670 samples, validate on 1771 samples\n",
      "Epoch 1/100\n",
      "13670/13670 [==============================] - 3s 207us/step - loss: 0.6926 - acc: 0.7134 - val_loss: 0.5940 - val_acc: 0.7493\n",
      "Epoch 2/100\n",
      "13670/13670 [==============================] - 3s 214us/step - loss: 0.6916 - acc: 0.7143 - val_loss: 0.5947 - val_acc: 0.7493\n",
      "Epoch 3/100\n",
      "13670/13670 [==============================] - 3s 212us/step - loss: 0.6977 - acc: 0.7115 - val_loss: 0.5955 - val_acc: 0.7487\n",
      "Epoch 4/100\n",
      "13670/13670 [==============================] - 3s 211us/step - loss: 0.6946 - acc: 0.7108 - val_loss: 0.5961 - val_acc: 0.7487\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 3.051757957450718e-08.\n",
      "Epoch 5/100\n",
      "13670/13670 [==============================] - 3s 210us/step - loss: 0.6950 - acc: 0.7102 - val_loss: 0.5964 - val_acc: 0.7487\n",
      "Epoch 6/100\n",
      "13670/13670 [==============================] - 3s 212us/step - loss: 0.6970 - acc: 0.7123 - val_loss: 0.5967 - val_acc: 0.7487\n",
      "Epoch 7/100\n",
      "13670/13670 [==============================] - 3s 208us/step - loss: 0.6924 - acc: 0.7127 - val_loss: 0.5971 - val_acc: 0.7487\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 1.525878978725359e-08.\n",
      "Epoch 8/100\n",
      "13670/13670 [==============================] - 3s 214us/step - loss: 0.6927 - acc: 0.7119 - val_loss: 0.5972 - val_acc: 0.7487\n",
      "Epoch 9/100\n",
      "13670/13670 [==============================] - 3s 212us/step - loss: 0.6910 - acc: 0.7113 - val_loss: 0.5974 - val_acc: 0.7487\n",
      "Epoch 10/100\n",
      "13670/13670 [==============================] - 3s 212us/step - loss: 0.6926 - acc: 0.7102 - val_loss: 0.5976 - val_acc: 0.7487\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 7.629394893626795e-09.\n",
      "Epoch 11/100\n",
      "13670/13670 [==============================] - 3s 209us/step - loss: 0.6916 - acc: 0.7091 - val_loss: 0.5976 - val_acc: 0.7487\n",
      "Epoch 12/100\n",
      "13670/13670 [==============================] - 3s 208us/step - loss: 0.6926 - acc: 0.7125 - val_loss: 0.5977 - val_acc: 0.7487\n",
      "Epoch 13/100\n",
      "13670/13670 [==============================] - 3s 204us/step - loss: 0.6938 - acc: 0.7119 - val_loss: 0.5978 - val_acc: 0.7487\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 3.814697446813398e-09.\n",
      "Epoch 14/100\n",
      "13670/13670 [==============================] - 3s 206us/step - loss: 0.6929 - acc: 0.7125 - val_loss: 0.5978 - val_acc: 0.7487\n",
      "Epoch 15/100\n",
      "13670/13670 [==============================] - 3s 212us/step - loss: 0.6896 - acc: 0.7121 - val_loss: 0.5979 - val_acc: 0.7487\n",
      "Epoch 16/100\n",
      "13670/13670 [==============================] - 3s 209us/step - loss: 0.6905 - acc: 0.7138 - val_loss: 0.5979 - val_acc: 0.7487\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.907348723406699e-09.\n",
      "Epoch 17/100\n",
      "13670/13670 [==============================] - 3s 207us/step - loss: 0.6933 - acc: 0.7110 - val_loss: 0.5979 - val_acc: 0.7487\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00017: early stopping\n",
      "      NR.AR.LBD: 0.63513\n",
      "Model saved and metrics table updated.\n",
      "Model saved and metrics table updated.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>threshold</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>auc_roc</th>\n",
       "      <th>avg_precision</th>\n",
       "      <th>confusion_matrix</th>\n",
       "      <th>model_filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.982818</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.763284</td>\n",
       "      <td>0.060344</td>\n",
       "      <td>[[572, 2], [8, 0]]</td>\n",
       "      <td>RF0.joblib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>RF_modT</td>\n",
       "      <td>0.270000</td>\n",
       "      <td>0.979381</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.763284</td>\n",
       "      <td>0.060344</td>\n",
       "      <td>[[569, 5], [7, 1]]</td>\n",
       "      <td>RF_modT0.joblib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>DNN</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.738832</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.037975</td>\n",
       "      <td>0.635126</td>\n",
       "      <td>0.030153</td>\n",
       "      <td>[[427, 147], [5, 3]]</td>\n",
       "      <td>DNN0.h5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>DNN_modT</td>\n",
       "      <td>0.762549</td>\n",
       "      <td>0.929553</td>\n",
       "      <td>0.054054</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.088889</td>\n",
       "      <td>0.635126</td>\n",
       "      <td>0.030153</td>\n",
       "      <td>[[539, 35], [6, 2]]</td>\n",
       "      <td>DNN_modT0.h5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      model  threshold  accuracy  precision  recall        f1   auc_roc  \\\n",
       "0        RF   0.500000  0.982818   0.000000   0.000  0.000000  0.763284   \n",
       "1   RF_modT   0.270000  0.979381   0.166667   0.125  0.142857  0.763284   \n",
       "2       DNN   0.500000  0.738832   0.020000   0.375  0.037975  0.635126   \n",
       "3  DNN_modT   0.762549  0.929553   0.054054   0.250  0.088889  0.635126   \n",
       "\n",
       "   avg_precision      confusion_matrix   model_filename  \n",
       "0       0.060344    [[572, 2], [8, 0]]       RF0.joblib  \n",
       "1       0.060344    [[569, 5], [7, 1]]  RF_modT0.joblib  \n",
       "2       0.030153  [[427, 147], [5, 3]]          DNN0.h5  \n",
       "3       0.030153   [[539, 35], [6, 2]]     DNN_modT0.h5  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11300 samples, validate on 1488 samples\n",
      "Epoch 1/100\n",
      "11300/11300 [==============================] - 2s 201us/step - loss: 0.7350 - acc: 0.6379 - val_loss: 0.5618 - val_acc: 0.7708\n",
      "Epoch 2/100\n",
      "11300/11300 [==============================] - 2s 209us/step - loss: 0.7377 - acc: 0.6377 - val_loss: 0.5619 - val_acc: 0.7708\n",
      "Epoch 3/100\n",
      "11300/11300 [==============================] - 2s 214us/step - loss: 0.7319 - acc: 0.6388 - val_loss: 0.5619 - val_acc: 0.7708\n",
      "Epoch 4/100\n",
      "11300/11300 [==============================] - 2s 210us/step - loss: 0.7355 - acc: 0.6370 - val_loss: 0.5620 - val_acc: 0.7708\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 9.536743617033494e-10.\n",
      "Epoch 5/100\n",
      "11300/11300 [==============================] - 2s 213us/step - loss: 0.7381 - acc: 0.6356 - val_loss: 0.5620 - val_acc: 0.7708\n",
      "Epoch 6/100\n",
      "11300/11300 [==============================] - 2s 214us/step - loss: 0.7360 - acc: 0.6382 - val_loss: 0.5620 - val_acc: 0.7708\n",
      "Epoch 7/100\n",
      "11300/11300 [==============================] - 2s 215us/step - loss: 0.7324 - acc: 0.6381 - val_loss: 0.5621 - val_acc: 0.7702\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 4.768371808516747e-10.\n",
      "Epoch 8/100\n",
      "11300/11300 [==============================] - 2s 215us/step - loss: 0.7318 - acc: 0.6371 - val_loss: 0.5621 - val_acc: 0.7702\n",
      "Epoch 9/100\n",
      "11300/11300 [==============================] - 2s 215us/step - loss: 0.7365 - acc: 0.6379 - val_loss: 0.5621 - val_acc: 0.7702\n",
      "Epoch 10/100\n",
      "11300/11300 [==============================] - 2s 212us/step - loss: 0.7357 - acc: 0.6411 - val_loss: 0.5621 - val_acc: 0.7702\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 2.3841859042583735e-10.\n",
      "Epoch 11/100\n",
      "11300/11300 [==============================] - 3s 224us/step - loss: 0.7339 - acc: 0.6389 - val_loss: 0.5621 - val_acc: 0.7702\n",
      "Epoch 12/100\n",
      "11300/11300 [==============================] - 2s 219us/step - loss: 0.7361 - acc: 0.6339 - val_loss: 0.5621 - val_acc: 0.7702\n",
      "Epoch 13/100\n",
      "11300/11300 [==============================] - 2s 215us/step - loss: 0.7371 - acc: 0.6361 - val_loss: 0.5621 - val_acc: 0.7702\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 1.1920929521291868e-10.\n",
      "Epoch 14/100\n",
      "11300/11300 [==============================] - 2s 214us/step - loss: 0.7354 - acc: 0.6356 - val_loss: 0.5621 - val_acc: 0.7702\n",
      "Epoch 15/100\n",
      "11300/11300 [==============================] - 2s 214us/step - loss: 0.7362 - acc: 0.6336 - val_loss: 0.5621 - val_acc: 0.7702\n",
      "Epoch 16/100\n",
      "11300/11300 [==============================] - 2s 204us/step - loss: 0.7342 - acc: 0.6396 - val_loss: 0.5621 - val_acc: 0.7702\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 5.960464760645934e-11.\n",
      "Epoch 17/100\n",
      "11300/11300 [==============================] - 2s 210us/step - loss: 0.7347 - acc: 0.6350 - val_loss: 0.5621 - val_acc: 0.7702\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00017: early stopping\n",
      "   NR.Aromatase: 0.71816\n",
      "Model saved and metrics table updated.\n",
      "Model saved and metrics table updated.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>threshold</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>auc_roc</th>\n",
       "      <th>avg_precision</th>\n",
       "      <th>confusion_matrix</th>\n",
       "      <th>model_filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.928030</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.771931</td>\n",
       "      <td>0.311884</td>\n",
       "      <td>[[489, 0], [38, 1]]</td>\n",
       "      <td>RF0.joblib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>RF_modT</td>\n",
       "      <td>0.130000</td>\n",
       "      <td>0.840909</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.512821</td>\n",
       "      <td>0.322581</td>\n",
       "      <td>0.771931</td>\n",
       "      <td>0.311884</td>\n",
       "      <td>[[424, 65], [19, 20]]</td>\n",
       "      <td>RF_modT0.joblib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>DNN</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.772727</td>\n",
       "      <td>0.159664</td>\n",
       "      <td>0.487179</td>\n",
       "      <td>0.240506</td>\n",
       "      <td>0.718158</td>\n",
       "      <td>0.143086</td>\n",
       "      <td>[[389, 100], [20, 19]]</td>\n",
       "      <td>DNN0.h5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>DNN_modT</td>\n",
       "      <td>0.475931</td>\n",
       "      <td>0.767045</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.254545</td>\n",
       "      <td>0.718158</td>\n",
       "      <td>0.143086</td>\n",
       "      <td>[[384, 105], [18, 21]]</td>\n",
       "      <td>DNN_modT0.h5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      model  threshold  accuracy  precision    recall        f1   auc_roc  \\\n",
       "0        RF   0.500000  0.928030   1.000000  0.025641  0.050000  0.771931   \n",
       "1   RF_modT   0.130000  0.840909   0.235294  0.512821  0.322581  0.771931   \n",
       "2       DNN   0.500000  0.772727   0.159664  0.487179  0.240506  0.718158   \n",
       "3  DNN_modT   0.475931  0.767045   0.166667  0.538462  0.254545  0.718158   \n",
       "\n",
       "   avg_precision        confusion_matrix   model_filename  \n",
       "0       0.311884     [[489, 0], [38, 1]]       RF0.joblib  \n",
       "1       0.311884   [[424, 65], [19, 20]]  RF_modT0.joblib  \n",
       "2       0.143086  [[389, 100], [20, 19]]          DNN0.h5  \n",
       "3       0.143086  [[384, 105], [18, 21]]     DNN_modT0.h5  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11196 samples, validate on 1593 samples\n",
      "Epoch 1/100\n",
      "11196/11196 [==============================] - 2s 200us/step - loss: 0.7824 - acc: 0.6451 - val_loss: 0.5880 - val_acc: 0.7589\n",
      "Epoch 2/100\n",
      "11196/11196 [==============================] - 2s 217us/step - loss: 0.7853 - acc: 0.6436 - val_loss: 0.5880 - val_acc: 0.7589\n",
      "Epoch 3/100\n",
      "11196/11196 [==============================] - 2s 214us/step - loss: 0.7875 - acc: 0.6437 - val_loss: 0.5880 - val_acc: 0.7589\n",
      "Epoch 4/100\n",
      "11196/11196 [==============================] - 2s 209us/step - loss: 0.7822 - acc: 0.6476 - val_loss: 0.5880 - val_acc: 0.7589\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 2.980232380322967e-11.\n",
      "Epoch 5/100\n",
      "11196/11196 [==============================] - 2s 208us/step - loss: 0.7862 - acc: 0.6417 - val_loss: 0.5880 - val_acc: 0.7589\n",
      "Epoch 6/100\n",
      "11196/11196 [==============================] - 2s 212us/step - loss: 0.7820 - acc: 0.6442 - val_loss: 0.5880 - val_acc: 0.7589\n",
      "Epoch 7/100\n",
      "11196/11196 [==============================] - 2s 214us/step - loss: 0.7882 - acc: 0.6391 - val_loss: 0.5880 - val_acc: 0.7589\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 1.4901161901614834e-11.\n",
      "Epoch 8/100\n",
      "11196/11196 [==============================] - 2s 211us/step - loss: 0.7848 - acc: 0.6426 - val_loss: 0.5880 - val_acc: 0.7589\n",
      "Epoch 9/100\n",
      "11196/11196 [==============================] - 2s 208us/step - loss: 0.7884 - acc: 0.6458 - val_loss: 0.5880 - val_acc: 0.7589\n",
      "Epoch 10/100\n",
      "11196/11196 [==============================] - 2s 209us/step - loss: 0.7862 - acc: 0.6430 - val_loss: 0.5880 - val_acc: 0.7589\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 7.450580950807417e-12.\n",
      "Epoch 11/100\n",
      "11196/11196 [==============================] - 2s 215us/step - loss: 0.7821 - acc: 0.6445 - val_loss: 0.5880 - val_acc: 0.7589\n",
      "Epoch 12/100\n",
      "11196/11196 [==============================] - 2s 209us/step - loss: 0.7886 - acc: 0.6399 - val_loss: 0.5880 - val_acc: 0.7589\n",
      "Epoch 13/100\n",
      "11196/11196 [==============================] - 2s 209us/step - loss: 0.7858 - acc: 0.6439 - val_loss: 0.5880 - val_acc: 0.7589\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 3.725290475403709e-12.\n",
      "Epoch 14/100\n",
      "11196/11196 [==============================] - 2s 209us/step - loss: 0.7855 - acc: 0.6454 - val_loss: 0.5880 - val_acc: 0.7589\n",
      "Epoch 15/100\n",
      "11196/11196 [==============================] - 2s 203us/step - loss: 0.7872 - acc: 0.6462 - val_loss: 0.5880 - val_acc: 0.7589\n",
      "Epoch 16/100\n",
      "11196/11196 [==============================] - 2s 208us/step - loss: 0.7828 - acc: 0.6470 - val_loss: 0.5880 - val_acc: 0.7589\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.8626452377018543e-12.\n",
      "Epoch 17/100\n",
      "11196/11196 [==============================] - 2s 199us/step - loss: 0.7822 - acc: 0.6457 - val_loss: 0.5880 - val_acc: 0.7589\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00017: early stopping\n",
      "          NR.ER: 0.73637\n",
      "Model saved and metrics table updated.\n",
      "Model saved and metrics table updated.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>threshold</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>auc_roc</th>\n",
       "      <th>avg_precision</th>\n",
       "      <th>confusion_matrix</th>\n",
       "      <th>model_filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.215686</td>\n",
       "      <td>0.338462</td>\n",
       "      <td>0.769007</td>\n",
       "      <td>0.445565</td>\n",
       "      <td>[[462, 3], [40, 11]]</td>\n",
       "      <td>RF0.joblib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>RF_modT</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.912791</td>\n",
       "      <td>0.607143</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.430380</td>\n",
       "      <td>0.769007</td>\n",
       "      <td>0.445565</td>\n",
       "      <td>[[454, 11], [34, 17]]</td>\n",
       "      <td>RF_modT0.joblib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>DNN</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.748062</td>\n",
       "      <td>0.203008</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.293478</td>\n",
       "      <td>0.736369</td>\n",
       "      <td>0.280064</td>\n",
       "      <td>[[359, 106], [24, 27]]</td>\n",
       "      <td>DNN0.h5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>DNN_modT</td>\n",
       "      <td>0.625295</td>\n",
       "      <td>0.815891</td>\n",
       "      <td>0.270833</td>\n",
       "      <td>0.509804</td>\n",
       "      <td>0.353741</td>\n",
       "      <td>0.736369</td>\n",
       "      <td>0.280064</td>\n",
       "      <td>[[395, 70], [25, 26]]</td>\n",
       "      <td>DNN_modT0.h5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      model  threshold  accuracy  precision    recall        f1   auc_roc  \\\n",
       "0        RF   0.500000  0.916667   0.785714  0.215686  0.338462  0.769007   \n",
       "1   RF_modT   0.400000  0.912791   0.607143  0.333333  0.430380  0.769007   \n",
       "2       DNN   0.500000  0.748062   0.203008  0.529412  0.293478  0.736369   \n",
       "3  DNN_modT   0.625295  0.815891   0.270833  0.509804  0.353741  0.736369   \n",
       "\n",
       "   avg_precision        confusion_matrix   model_filename  \n",
       "0       0.445565    [[462, 3], [40, 11]]       RF0.joblib  \n",
       "1       0.445565   [[454, 11], [34, 17]]  RF_modT0.joblib  \n",
       "2       0.280064  [[359, 106], [24, 27]]          DNN0.h5  \n",
       "3       0.280064   [[395, 70], [25, 26]]     DNN_modT0.h5  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13734 samples, validate on 1808 samples\n",
      "Epoch 1/100\n",
      "13734/13734 [==============================] - 3s 200us/step - loss: 0.7634 - acc: 0.6589 - val_loss: 0.6040 - val_acc: 0.7406\n",
      "Epoch 2/100\n",
      "13734/13734 [==============================] - 3s 211us/step - loss: 0.7638 - acc: 0.6589 - val_loss: 0.6040 - val_acc: 0.7406\n",
      "Epoch 3/100\n",
      "13734/13734 [==============================] - 3s 211us/step - loss: 0.7642 - acc: 0.6591 - val_loss: 0.6040 - val_acc: 0.7406\n",
      "Epoch 4/100\n",
      "13734/13734 [==============================] - 3s 208us/step - loss: 0.7649 - acc: 0.6557 - val_loss: 0.6040 - val_acc: 0.7406\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 9.313226188509272e-13.\n",
      "Epoch 5/100\n",
      "13734/13734 [==============================] - 3s 212us/step - loss: 0.7620 - acc: 0.6594 - val_loss: 0.6040 - val_acc: 0.7406\n",
      "Epoch 6/100\n",
      "13734/13734 [==============================] - 3s 210us/step - loss: 0.7610 - acc: 0.6589 - val_loss: 0.6040 - val_acc: 0.7406\n",
      "Epoch 7/100\n",
      "13734/13734 [==============================] - 3s 209us/step - loss: 0.7661 - acc: 0.6593 - val_loss: 0.6040 - val_acc: 0.7406\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 4.656613094254636e-13.\n",
      "Epoch 8/100\n",
      "13734/13734 [==============================] - 3s 213us/step - loss: 0.7624 - acc: 0.6576 - val_loss: 0.6040 - val_acc: 0.7406\n",
      "Epoch 9/100\n",
      "13734/13734 [==============================] - 3s 211us/step - loss: 0.7625 - acc: 0.6600 - val_loss: 0.6040 - val_acc: 0.7406\n",
      "Epoch 10/100\n",
      "13734/13734 [==============================] - 3s 209us/step - loss: 0.7645 - acc: 0.6574 - val_loss: 0.6040 - val_acc: 0.7406\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 2.328306547127318e-13.\n",
      "Epoch 11/100\n",
      "13734/13734 [==============================] - 3s 212us/step - loss: 0.7647 - acc: 0.6549 - val_loss: 0.6040 - val_acc: 0.7406\n",
      "Epoch 12/100\n",
      "13734/13734 [==============================] - 3s 205us/step - loss: 0.7610 - acc: 0.6553 - val_loss: 0.6040 - val_acc: 0.7406\n",
      "Epoch 13/100\n",
      "13734/13734 [==============================] - 3s 206us/step - loss: 0.7646 - acc: 0.6568 - val_loss: 0.6040 - val_acc: 0.7406\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 1.164153273563659e-13.\n",
      "Epoch 14/100\n",
      "13734/13734 [==============================] - 3s 201us/step - loss: 0.7617 - acc: 0.6571 - val_loss: 0.6040 - val_acc: 0.7406\n",
      "Epoch 15/100\n",
      "13734/13734 [==============================] - 3s 203us/step - loss: 0.7621 - acc: 0.6591 - val_loss: 0.6040 - val_acc: 0.7406\n",
      "Epoch 16/100\n",
      "13734/13734 [==============================] - 3s 207us/step - loss: 0.7641 - acc: 0.6596 - val_loss: 0.6040 - val_acc: 0.7406\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 5.820766367818295e-14.\n",
      "Epoch 17/100\n",
      "13734/13734 [==============================] - 3s 200us/step - loss: 0.7643 - acc: 0.6552 - val_loss: 0.6040 - val_acc: 0.7406\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00017: early stopping\n",
      "      NR.ER.LBD: 0.67832\n",
      "Model saved and metrics table updated.\n",
      "Model saved and metrics table updated.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>threshold</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>auc_roc</th>\n",
       "      <th>avg_precision</th>\n",
       "      <th>confusion_matrix</th>\n",
       "      <th>model_filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.50000</td>\n",
       "      <td>0.971667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.260870</td>\n",
       "      <td>0.750345</td>\n",
       "      <td>0.268778</td>\n",
       "      <td>[[580, 0], [17, 3]]</td>\n",
       "      <td>RF0.joblib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>RF_modT</td>\n",
       "      <td>0.30500</td>\n",
       "      <td>0.965000</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.750345</td>\n",
       "      <td>0.268778</td>\n",
       "      <td>[[573, 7], [14, 6]]</td>\n",
       "      <td>RF_modT0.joblib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>DNN</td>\n",
       "      <td>0.50000</td>\n",
       "      <td>0.725000</td>\n",
       "      <td>0.049689</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.088398</td>\n",
       "      <td>0.678319</td>\n",
       "      <td>0.069591</td>\n",
       "      <td>[[427, 153], [12, 8]]</td>\n",
       "      <td>DNN0.h5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>DNN_modT</td>\n",
       "      <td>0.74828</td>\n",
       "      <td>0.915000</td>\n",
       "      <td>0.102564</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.135593</td>\n",
       "      <td>0.678319</td>\n",
       "      <td>0.069591</td>\n",
       "      <td>[[545, 35], [16, 4]]</td>\n",
       "      <td>DNN_modT0.h5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      model  threshold  accuracy  precision  recall        f1   auc_roc  \\\n",
       "0        RF    0.50000  0.971667   1.000000    0.15  0.260870  0.750345   \n",
       "1   RF_modT    0.30500  0.965000   0.461538    0.30  0.363636  0.750345   \n",
       "2       DNN    0.50000  0.725000   0.049689    0.40  0.088398  0.678319   \n",
       "3  DNN_modT    0.74828  0.915000   0.102564    0.20  0.135593  0.678319   \n",
       "\n",
       "   avg_precision       confusion_matrix   model_filename  \n",
       "0       0.268778    [[580, 0], [17, 3]]       RF0.joblib  \n",
       "1       0.268778    [[573, 7], [14, 6]]  RF_modT0.joblib  \n",
       "2       0.069591  [[427, 153], [12, 8]]          DNN0.h5  \n",
       "3       0.069591   [[545, 35], [16, 4]]     DNN_modT0.h5  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13140 samples, validate on 1691 samples\n",
      "Epoch 1/100\n",
      "13140/13140 [==============================] - 3s 201us/step - loss: 0.8473 - acc: 0.6414 - val_loss: 0.5922 - val_acc: 0.7463\n",
      "Epoch 2/100\n",
      "13140/13140 [==============================] - 3s 213us/step - loss: 0.8494 - acc: 0.6419 - val_loss: 0.5922 - val_acc: 0.7463\n",
      "Epoch 3/100\n",
      "13140/13140 [==============================] - 3s 213us/step - loss: 0.8480 - acc: 0.6416 - val_loss: 0.5922 - val_acc: 0.7463\n",
      "Epoch 4/100\n",
      "13140/13140 [==============================] - 3s 211us/step - loss: 0.8501 - acc: 0.6409 - val_loss: 0.5922 - val_acc: 0.7463\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 2.9103831839091474e-14.\n",
      "Epoch 5/100\n",
      "13140/13140 [==============================] - 3s 210us/step - loss: 0.8500 - acc: 0.6400 - val_loss: 0.5922 - val_acc: 0.7463\n",
      "Epoch 6/100\n",
      "13140/13140 [==============================] - 3s 211us/step - loss: 0.8496 - acc: 0.6407 - val_loss: 0.5922 - val_acc: 0.7463\n",
      "Epoch 7/100\n",
      "13140/13140 [==============================] - 3s 207us/step - loss: 0.8504 - acc: 0.6414 - val_loss: 0.5922 - val_acc: 0.7463\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 1.4551915919545737e-14.\n",
      "Epoch 8/100\n",
      "13140/13140 [==============================] - 3s 211us/step - loss: 0.8466 - acc: 0.6377 - val_loss: 0.5922 - val_acc: 0.7463\n",
      "Epoch 9/100\n",
      "13140/13140 [==============================] - 3s 212us/step - loss: 0.8507 - acc: 0.6404 - val_loss: 0.5922 - val_acc: 0.7463\n",
      "Epoch 10/100\n",
      "13140/13140 [==============================] - 3s 210us/step - loss: 0.8489 - acc: 0.6435 - val_loss: 0.5922 - val_acc: 0.7463\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 7.275957959772868e-15.\n",
      "Epoch 11/100\n",
      "13140/13140 [==============================] - 3s 208us/step - loss: 0.8507 - acc: 0.6406 - val_loss: 0.5922 - val_acc: 0.7463\n",
      "Epoch 12/100\n",
      "13140/13140 [==============================] - 3s 209us/step - loss: 0.8512 - acc: 0.6407 - val_loss: 0.5922 - val_acc: 0.7463\n",
      "Epoch 13/100\n",
      "13140/13140 [==============================] - 3s 214us/step - loss: 0.8524 - acc: 0.6384 - val_loss: 0.5922 - val_acc: 0.7463\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 3.637978979886434e-15.\n",
      "Epoch 14/100\n",
      "13140/13140 [==============================] - 3s 200us/step - loss: 0.8479 - acc: 0.6425 - val_loss: 0.5922 - val_acc: 0.7463\n",
      "Epoch 15/100\n",
      "13140/13140 [==============================] - 3s 202us/step - loss: 0.8506 - acc: 0.6368 - val_loss: 0.5922 - val_acc: 0.7463\n",
      "Epoch 16/100\n",
      "13140/13140 [==============================] - 3s 203us/step - loss: 0.8475 - acc: 0.6420 - val_loss: 0.5922 - val_acc: 0.7463\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.818989489943217e-15.\n",
      "Epoch 17/100\n",
      "13140/13140 [==============================] - 3s 203us/step - loss: 0.8496 - acc: 0.6376 - val_loss: 0.5922 - val_acc: 0.7463\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00017: early stopping\n",
      "  NR.PPAR.gamma: 0.74039\n",
      "Model saved and metrics table updated.\n",
      "Model saved and metrics table updated.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>threshold</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>auc_roc</th>\n",
       "      <th>avg_precision</th>\n",
       "      <th>confusion_matrix</th>\n",
       "      <th>model_filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.947107</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.709846</td>\n",
       "      <td>0.129989</td>\n",
       "      <td>[[573, 1], [31, 0]]</td>\n",
       "      <td>RF0.joblib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>RF_modT</td>\n",
       "      <td>0.130000</td>\n",
       "      <td>0.890909</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.322581</td>\n",
       "      <td>0.232558</td>\n",
       "      <td>0.709846</td>\n",
       "      <td>0.129989</td>\n",
       "      <td>[[529, 45], [21, 10]]</td>\n",
       "      <td>RF_modT0.joblib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>DNN</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.750413</td>\n",
       "      <td>0.115385</td>\n",
       "      <td>0.580645</td>\n",
       "      <td>0.192513</td>\n",
       "      <td>0.740390</td>\n",
       "      <td>0.109550</td>\n",
       "      <td>[[436, 138], [13, 18]]</td>\n",
       "      <td>DNN0.h5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>DNN_modT</td>\n",
       "      <td>0.577705</td>\n",
       "      <td>0.798347</td>\n",
       "      <td>0.130081</td>\n",
       "      <td>0.516129</td>\n",
       "      <td>0.207792</td>\n",
       "      <td>0.740390</td>\n",
       "      <td>0.109550</td>\n",
       "      <td>[[467, 107], [15, 16]]</td>\n",
       "      <td>DNN_modT0.h5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      model  threshold  accuracy  precision    recall        f1   auc_roc  \\\n",
       "0        RF   0.500000  0.947107   0.000000  0.000000  0.000000  0.709846   \n",
       "1   RF_modT   0.130000  0.890909   0.181818  0.322581  0.232558  0.709846   \n",
       "2       DNN   0.500000  0.750413   0.115385  0.580645  0.192513  0.740390   \n",
       "3  DNN_modT   0.577705  0.798347   0.130081  0.516129  0.207792  0.740390   \n",
       "\n",
       "   avg_precision        confusion_matrix   model_filename  \n",
       "0       0.129989     [[573, 1], [31, 0]]       RF0.joblib  \n",
       "1       0.129989   [[529, 45], [21, 10]]  RF_modT0.joblib  \n",
       "2       0.109550  [[436, 138], [13, 18]]          DNN0.h5  \n",
       "3       0.109550  [[467, 107], [15, 16]]     DNN_modT0.h5  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10006 samples, validate on 1481 samples\n",
      "Epoch 1/100\n",
      "10006/10006 [==============================] - 2s 198us/step - loss: 0.8277 - acc: 0.6072 - val_loss: 0.6143 - val_acc: 0.7387\n",
      "Epoch 2/100\n",
      "10006/10006 [==============================] - 2s 209us/step - loss: 0.8290 - acc: 0.6072 - val_loss: 0.6143 - val_acc: 0.7387\n",
      "Epoch 3/100\n",
      "10006/10006 [==============================] - 2s 214us/step - loss: 0.8288 - acc: 0.6097 - val_loss: 0.6143 - val_acc: 0.7387\n",
      "Epoch 4/100\n",
      "10006/10006 [==============================] - 2s 217us/step - loss: 0.8261 - acc: 0.6060 - val_loss: 0.6143 - val_acc: 0.7387\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 9.094947449716085e-16.\n",
      "Epoch 5/100\n",
      "10006/10006 [==============================] - 2s 211us/step - loss: 0.8253 - acc: 0.6076 - val_loss: 0.6143 - val_acc: 0.7387\n",
      "Epoch 6/100\n",
      "10006/10006 [==============================] - 2s 213us/step - loss: 0.8243 - acc: 0.6088 - val_loss: 0.6143 - val_acc: 0.7387\n",
      "Epoch 7/100\n",
      "10006/10006 [==============================] - 2s 213us/step - loss: 0.8269 - acc: 0.6092 - val_loss: 0.6143 - val_acc: 0.7387\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 4.547473724858043e-16.\n",
      "Epoch 8/100\n",
      "10006/10006 [==============================] - 2s 211us/step - loss: 0.8283 - acc: 0.6095 - val_loss: 0.6143 - val_acc: 0.7387\n",
      "Epoch 9/100\n",
      "10006/10006 [==============================] - 2s 218us/step - loss: 0.8291 - acc: 0.6070 - val_loss: 0.6143 - val_acc: 0.7387\n",
      "Epoch 10/100\n",
      "10006/10006 [==============================] - 2s 212us/step - loss: 0.8243 - acc: 0.6079 - val_loss: 0.6143 - val_acc: 0.7387\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 2.2737368624290214e-16.\n",
      "Epoch 11/100\n",
      "10006/10006 [==============================] - 2s 208us/step - loss: 0.8267 - acc: 0.6096 - val_loss: 0.6143 - val_acc: 0.7387\n",
      "Epoch 12/100\n",
      "10006/10006 [==============================] - 2s 218us/step - loss: 0.8263 - acc: 0.6059 - val_loss: 0.6143 - val_acc: 0.7387\n",
      "Epoch 13/100\n",
      "10006/10006 [==============================] - 2s 211us/step - loss: 0.8271 - acc: 0.6047 - val_loss: 0.6143 - val_acc: 0.7387\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 1.1368684312145107e-16.\n",
      "Epoch 14/100\n",
      "10006/10006 [==============================] - 2s 214us/step - loss: 0.8230 - acc: 0.6084 - val_loss: 0.6143 - val_acc: 0.7387\n",
      "Epoch 15/100\n",
      "10006/10006 [==============================] - 2s 208us/step - loss: 0.8242 - acc: 0.6107 - val_loss: 0.6143 - val_acc: 0.7387\n",
      "Epoch 16/100\n",
      "10006/10006 [==============================] - 2s 212us/step - loss: 0.8264 - acc: 0.6108 - val_loss: 0.6143 - val_acc: 0.7387\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 5.684342156072553e-17.\n",
      "Epoch 17/100\n",
      "10006/10006 [==============================] - 2s 214us/step - loss: 0.8261 - acc: 0.6097 - val_loss: 0.6143 - val_acc: 0.7387\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00017: early stopping\n",
      "         SR.ARE: 0.70550\n",
      "Model saved and metrics table updated.\n",
      "Model saved and metrics table updated.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>threshold</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>auc_roc</th>\n",
       "      <th>avg_precision</th>\n",
       "      <th>confusion_matrix</th>\n",
       "      <th>model_filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.841441</td>\n",
       "      <td>0.592593</td>\n",
       "      <td>0.172043</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.780838</td>\n",
       "      <td>0.430711</td>\n",
       "      <td>[[451, 11], [77, 16]]</td>\n",
       "      <td>RF0.joblib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>RF_modT</td>\n",
       "      <td>0.340000</td>\n",
       "      <td>0.823423</td>\n",
       "      <td>0.476190</td>\n",
       "      <td>0.537634</td>\n",
       "      <td>0.505051</td>\n",
       "      <td>0.780838</td>\n",
       "      <td>0.430711</td>\n",
       "      <td>[[407, 55], [43, 50]]</td>\n",
       "      <td>RF_modT0.joblib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>DNN</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.762162</td>\n",
       "      <td>0.355556</td>\n",
       "      <td>0.516129</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>0.705500</td>\n",
       "      <td>0.347718</td>\n",
       "      <td>[[375, 87], [45, 48]]</td>\n",
       "      <td>DNN0.h5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>DNN_modT</td>\n",
       "      <td>0.457539</td>\n",
       "      <td>0.753153</td>\n",
       "      <td>0.353333</td>\n",
       "      <td>0.569892</td>\n",
       "      <td>0.436214</td>\n",
       "      <td>0.705500</td>\n",
       "      <td>0.347718</td>\n",
       "      <td>[[365, 97], [40, 53]]</td>\n",
       "      <td>DNN_modT0.h5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      model  threshold  accuracy  precision    recall        f1   auc_roc  \\\n",
       "0        RF   0.500000  0.841441   0.592593  0.172043  0.266667  0.780838   \n",
       "1   RF_modT   0.340000  0.823423   0.476190  0.537634  0.505051  0.780838   \n",
       "2       DNN   0.500000  0.762162   0.355556  0.516129  0.421053  0.705500   \n",
       "3  DNN_modT   0.457539  0.753153   0.353333  0.569892  0.436214  0.705500   \n",
       "\n",
       "   avg_precision       confusion_matrix   model_filename  \n",
       "0       0.430711  [[451, 11], [77, 16]]       RF0.joblib  \n",
       "1       0.430711  [[407, 55], [43, 50]]  RF_modT0.joblib  \n",
       "2       0.347718  [[375, 87], [45, 48]]          DNN0.h5  \n",
       "3       0.347718  [[365, 97], [40, 53]]     DNN_modT0.h5  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14400 samples, validate on 1873 samples\n",
      "Epoch 1/100\n",
      "14400/14400 [==============================] - 3s 207us/step - loss: 0.7856 - acc: 0.6460 - val_loss: 0.6163 - val_acc: 0.7277\n",
      "Epoch 2/100\n",
      "14400/14400 [==============================] - 3s 216us/step - loss: 0.7849 - acc: 0.6498 - val_loss: 0.6163 - val_acc: 0.7277\n",
      "Epoch 3/100\n",
      "14400/14400 [==============================] - 3s 213us/step - loss: 0.7839 - acc: 0.6488 - val_loss: 0.6163 - val_acc: 0.7277\n",
      "Epoch 4/100\n",
      "14400/14400 [==============================] - 3s 215us/step - loss: 0.7871 - acc: 0.6468 - val_loss: 0.6163 - val_acc: 0.7277\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 2.842171078036277e-17.\n",
      "Epoch 5/100\n",
      "14400/14400 [==============================] - 3s 210us/step - loss: 0.7841 - acc: 0.6508 - val_loss: 0.6163 - val_acc: 0.7277\n",
      "Epoch 6/100\n",
      "14400/14400 [==============================] - 3s 213us/step - loss: 0.7849 - acc: 0.6462 - val_loss: 0.6163 - val_acc: 0.7277\n",
      "Epoch 7/100\n",
      "14400/14400 [==============================] - 3s 212us/step - loss: 0.7881 - acc: 0.6475 - val_loss: 0.6163 - val_acc: 0.7277\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 1.4210855390181384e-17.\n",
      "Epoch 8/100\n",
      "14400/14400 [==============================] - 3s 212us/step - loss: 0.7859 - acc: 0.6503 - val_loss: 0.6163 - val_acc: 0.7277\n",
      "Epoch 9/100\n",
      "14400/14400 [==============================] - 3s 213us/step - loss: 0.7861 - acc: 0.6492 - val_loss: 0.6163 - val_acc: 0.7277\n",
      "Epoch 10/100\n",
      "14400/14400 [==============================] - 3s 212us/step - loss: 0.7865 - acc: 0.6467 - val_loss: 0.6163 - val_acc: 0.7277\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 7.105427695090692e-18.\n",
      "Epoch 11/100\n",
      "14400/14400 [==============================] - 3s 210us/step - loss: 0.7842 - acc: 0.6473 - val_loss: 0.6163 - val_acc: 0.7277\n",
      "Epoch 12/100\n",
      "14400/14400 [==============================] - 3s 214us/step - loss: 0.7872 - acc: 0.6451 - val_loss: 0.6163 - val_acc: 0.7277\n",
      "Epoch 13/100\n",
      "14400/14400 [==============================] - 3s 210us/step - loss: 0.7902 - acc: 0.6468 - val_loss: 0.6163 - val_acc: 0.7277\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 3.552713847545346e-18.\n",
      "Epoch 14/100\n",
      "14400/14400 [==============================] - 3s 214us/step - loss: 0.7862 - acc: 0.6485 - val_loss: 0.6163 - val_acc: 0.7277\n",
      "Epoch 15/100\n",
      "14400/14400 [==============================] - 3s 206us/step - loss: 0.7876 - acc: 0.6485 - val_loss: 0.6163 - val_acc: 0.7277\n",
      "Epoch 16/100\n",
      "14400/14400 [==============================] - 3s 201us/step - loss: 0.7894 - acc: 0.6469 - val_loss: 0.6163 - val_acc: 0.7277\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.776356923772673e-18.\n",
      "Epoch 17/100\n",
      "14400/14400 [==============================] - 3s 214us/step - loss: 0.7857 - acc: 0.6475 - val_loss: 0.6163 - val_acc: 0.7277\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00017: early stopping\n",
      "       SR.ATAD5: 0.71161\n",
      "Model saved and metrics table updated.\n",
      "Model saved and metrics table updated.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>threshold</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>auc_roc</th>\n",
       "      <th>avg_precision</th>\n",
       "      <th>confusion_matrix</th>\n",
       "      <th>model_filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.938907</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.795625</td>\n",
       "      <td>0.427828</td>\n",
       "      <td>[[584, 0], [38, 0]]</td>\n",
       "      <td>RF0.joblib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>RF_modT</td>\n",
       "      <td>0.255000</td>\n",
       "      <td>0.948553</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.795625</td>\n",
       "      <td>0.427828</td>\n",
       "      <td>[[576, 8], [24, 14]]</td>\n",
       "      <td>RF_modT0.joblib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>DNN</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.736334</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.552632</td>\n",
       "      <td>0.203883</td>\n",
       "      <td>0.711608</td>\n",
       "      <td>0.153415</td>\n",
       "      <td>[[437, 147], [17, 21]]</td>\n",
       "      <td>DNN0.h5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>DNN_modT</td>\n",
       "      <td>0.577705</td>\n",
       "      <td>0.789389</td>\n",
       "      <td>0.155556</td>\n",
       "      <td>0.552632</td>\n",
       "      <td>0.242775</td>\n",
       "      <td>0.711608</td>\n",
       "      <td>0.153415</td>\n",
       "      <td>[[470, 114], [17, 21]]</td>\n",
       "      <td>DNN_modT0.h5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      model  threshold  accuracy  precision    recall        f1   auc_roc  \\\n",
       "0        RF   0.500000  0.938907   0.000000  0.000000  0.000000  0.795625   \n",
       "1   RF_modT   0.255000  0.948553   0.636364  0.368421  0.466667  0.795625   \n",
       "2       DNN   0.500000  0.736334   0.125000  0.552632  0.203883  0.711608   \n",
       "3  DNN_modT   0.577705  0.789389   0.155556  0.552632  0.242775  0.711608   \n",
       "\n",
       "   avg_precision        confusion_matrix   model_filename  \n",
       "0       0.427828     [[584, 0], [38, 0]]       RF0.joblib  \n",
       "1       0.427828    [[576, 8], [24, 14]]  RF_modT0.joblib  \n",
       "2       0.153415  [[437, 147], [17, 21]]          DNN0.h5  \n",
       "3       0.153415  [[470, 114], [17, 21]]     DNN_modT0.h5  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12766 samples, validate on 1684 samples\n",
      "Epoch 1/100\n",
      "12766/12766 [==============================] - 3s 201us/step - loss: 0.8939 - acc: 0.5726 - val_loss: 0.6040 - val_acc: 0.7387\n",
      "Epoch 2/100\n",
      "12766/12766 [==============================] - 3s 216us/step - loss: 0.8961 - acc: 0.5725 - val_loss: 0.6040 - val_acc: 0.7387\n",
      "Epoch 3/100\n",
      "12766/12766 [==============================] - 3s 213us/step - loss: 0.8974 - acc: 0.5715 - val_loss: 0.6040 - val_acc: 0.7387\n",
      "Epoch 4/100\n",
      "12766/12766 [==============================] - 3s 213us/step - loss: 0.8990 - acc: 0.5734 - val_loss: 0.6040 - val_acc: 0.7387\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 8.881784618863365e-19.\n",
      "Epoch 5/100\n",
      "12766/12766 [==============================] - 3s 210us/step - loss: 0.8931 - acc: 0.5728 - val_loss: 0.6040 - val_acc: 0.7387\n",
      "Epoch 6/100\n",
      "12766/12766 [==============================] - 3s 209us/step - loss: 0.8964 - acc: 0.5714 - val_loss: 0.6040 - val_acc: 0.7387\n",
      "Epoch 7/100\n",
      "12766/12766 [==============================] - 3s 225us/step - loss: 0.8986 - acc: 0.5725 - val_loss: 0.6040 - val_acc: 0.7387\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 4.440892309431682e-19.\n",
      "Epoch 8/100\n",
      "12766/12766 [==============================] - 3s 214us/step - loss: 0.8973 - acc: 0.5717 - val_loss: 0.6040 - val_acc: 0.7387\n",
      "Epoch 9/100\n",
      "12766/12766 [==============================] - 3s 213us/step - loss: 0.8969 - acc: 0.5734 - val_loss: 0.6040 - val_acc: 0.7387\n",
      "Epoch 10/100\n",
      "12766/12766 [==============================] - 3s 207us/step - loss: 0.8970 - acc: 0.5706 - val_loss: 0.6040 - val_acc: 0.7387\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 2.220446154715841e-19.\n",
      "Epoch 11/100\n",
      "12766/12766 [==============================] - 3s 210us/step - loss: 0.8962 - acc: 0.5743 - val_loss: 0.6040 - val_acc: 0.7387\n",
      "Epoch 12/100\n",
      "12766/12766 [==============================] - 3s 204us/step - loss: 0.8947 - acc: 0.5739 - val_loss: 0.6040 - val_acc: 0.7387\n",
      "Epoch 13/100\n",
      "12766/12766 [==============================] - 3s 214us/step - loss: 0.8954 - acc: 0.5743 - val_loss: 0.6040 - val_acc: 0.7387\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 1.1102230773579206e-19.\n",
      "Epoch 14/100\n",
      "12766/12766 [==============================] - 3s 211us/step - loss: 0.8960 - acc: 0.5734 - val_loss: 0.6040 - val_acc: 0.7387\n",
      "Epoch 15/100\n",
      "12766/12766 [==============================] - 3s 202us/step - loss: 0.9010 - acc: 0.5701 - val_loss: 0.6040 - val_acc: 0.7387\n",
      "Epoch 16/100\n",
      "12766/12766 [==============================] - 3s 202us/step - loss: 0.8929 - acc: 0.5774 - val_loss: 0.6040 - val_acc: 0.7387\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 5.551115386789603e-20.\n",
      "Epoch 17/100\n",
      "12766/12766 [==============================] - 3s 204us/step - loss: 0.8962 - acc: 0.5713 - val_loss: 0.6040 - val_acc: 0.7387\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00017: early stopping\n",
      "         SR.HSE: 0.62674\n",
      "Model saved and metrics table updated.\n",
      "Model saved and metrics table updated.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>threshold</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>auc_roc</th>\n",
       "      <th>avg_precision</th>\n",
       "      <th>confusion_matrix</th>\n",
       "      <th>model_filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.965574</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.759586</td>\n",
       "      <td>0.239563</td>\n",
       "      <td>[[586, 2], [19, 3]]</td>\n",
       "      <td>RF0.joblib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>RF_modT</td>\n",
       "      <td>0.340000</td>\n",
       "      <td>0.965574</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.759586</td>\n",
       "      <td>0.239563</td>\n",
       "      <td>[[583, 5], [16, 6]]</td>\n",
       "      <td>RF_modT0.joblib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>DNN</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.726230</td>\n",
       "      <td>0.055215</td>\n",
       "      <td>0.409091</td>\n",
       "      <td>0.097297</td>\n",
       "      <td>0.626739</td>\n",
       "      <td>0.054621</td>\n",
       "      <td>[[434, 154], [13, 9]]</td>\n",
       "      <td>DNN0.h5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>DNN_modT</td>\n",
       "      <td>0.450345</td>\n",
       "      <td>0.708197</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.118812</td>\n",
       "      <td>0.626739</td>\n",
       "      <td>0.054621</td>\n",
       "      <td>[[420, 168], [10, 12]]</td>\n",
       "      <td>DNN_modT0.h5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      model  threshold  accuracy  precision    recall        f1   auc_roc  \\\n",
       "0        RF   0.500000  0.965574   0.600000  0.136364  0.222222  0.759586   \n",
       "1   RF_modT   0.340000  0.965574   0.545455  0.272727  0.363636  0.759586   \n",
       "2       DNN   0.500000  0.726230   0.055215  0.409091  0.097297  0.626739   \n",
       "3  DNN_modT   0.450345  0.708197   0.066667  0.545455  0.118812  0.626739   \n",
       "\n",
       "   avg_precision        confusion_matrix   model_filename  \n",
       "0       0.239563     [[586, 2], [19, 3]]       RF0.joblib  \n",
       "1       0.239563     [[583, 5], [16, 6]]  RF_modT0.joblib  \n",
       "2       0.054621   [[434, 154], [13, 9]]          DNN0.h5  \n",
       "3       0.054621  [[420, 168], [10, 12]]     DNN_modT0.h5  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10204 samples, validate on 1512 samples\n",
      "Epoch 1/100\n",
      "10204/10204 [==============================] - 2s 201us/step - loss: 0.7619 - acc: 0.6424 - val_loss: 0.6140 - val_acc: 0.7421\n",
      "Epoch 2/100\n",
      "10204/10204 [==============================] - 2s 218us/step - loss: 0.7654 - acc: 0.6400 - val_loss: 0.6140 - val_acc: 0.7421\n",
      "Epoch 3/100\n",
      "10204/10204 [==============================] - 2s 215us/step - loss: 0.7604 - acc: 0.6392 - val_loss: 0.6140 - val_acc: 0.7421\n",
      "Epoch 4/100\n",
      "10204/10204 [==============================] - 2s 219us/step - loss: 0.7587 - acc: 0.6449 - val_loss: 0.6140 - val_acc: 0.7421\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 2.7755576933948015e-20.\n",
      "Epoch 5/100\n",
      "10204/10204 [==============================] - 2s 214us/step - loss: 0.7617 - acc: 0.6446 - val_loss: 0.6140 - val_acc: 0.7421\n",
      "Epoch 6/100\n",
      "10204/10204 [==============================] - 2s 213us/step - loss: 0.7611 - acc: 0.6400 - val_loss: 0.6140 - val_acc: 0.7421\n",
      "Epoch 7/100\n",
      "10204/10204 [==============================] - 2s 212us/step - loss: 0.7633 - acc: 0.6404 - val_loss: 0.6140 - val_acc: 0.7421\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 1.3877788466974007e-20.\n",
      "Epoch 8/100\n",
      "10204/10204 [==============================] - 2s 216us/step - loss: 0.7594 - acc: 0.6465 - val_loss: 0.6140 - val_acc: 0.7421\n",
      "Epoch 9/100\n",
      "10204/10204 [==============================] - 2s 216us/step - loss: 0.7610 - acc: 0.6450 - val_loss: 0.6140 - val_acc: 0.7421\n",
      "Epoch 10/100\n",
      "10204/10204 [==============================] - 2s 215us/step - loss: 0.7598 - acc: 0.6421 - val_loss: 0.6140 - val_acc: 0.7421\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 6.938894233487004e-21.\n",
      "Epoch 11/100\n",
      "10204/10204 [==============================] - 2s 221us/step - loss: 0.7635 - acc: 0.6415 - val_loss: 0.6140 - val_acc: 0.7421\n",
      "Epoch 12/100\n",
      "10204/10204 [==============================] - 2s 213us/step - loss: 0.7603 - acc: 0.6439 - val_loss: 0.6140 - val_acc: 0.7421\n",
      "Epoch 13/100\n",
      "10204/10204 [==============================] - 2s 216us/step - loss: 0.7582 - acc: 0.6419 - val_loss: 0.6140 - val_acc: 0.7421\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 3.469447116743502e-21.\n",
      "Epoch 14/100\n",
      "10204/10204 [==============================] - 2s 216us/step - loss: 0.7601 - acc: 0.6458 - val_loss: 0.6140 - val_acc: 0.7421\n",
      "Epoch 15/100\n",
      "10204/10204 [==============================] - 2s 215us/step - loss: 0.7570 - acc: 0.6437 - val_loss: 0.6140 - val_acc: 0.7421\n",
      "Epoch 16/100\n",
      "10204/10204 [==============================] - 2s 213us/step - loss: 0.7629 - acc: 0.6403 - val_loss: 0.6140 - val_acc: 0.7421\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.734723558371751e-21.\n",
      "Epoch 17/100\n",
      "10204/10204 [==============================] - 2s 209us/step - loss: 0.7598 - acc: 0.6424 - val_loss: 0.6140 - val_acc: 0.7421\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00017: early stopping\n",
      "         SR.MMP: 0.80388\n",
      "Model saved and metrics table updated.\n",
      "Model saved and metrics table updated.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>threshold</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>auc_roc</th>\n",
       "      <th>avg_precision</th>\n",
       "      <th>confusion_matrix</th>\n",
       "      <th>model_filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.906077</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.484848</td>\n",
       "      <td>0.930331</td>\n",
       "      <td>0.560184</td>\n",
       "      <td>[[468, 15], [36, 24]]</td>\n",
       "      <td>RF0.joblib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>RF_modT</td>\n",
       "      <td>0.382500</td>\n",
       "      <td>0.913444</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.624000</td>\n",
       "      <td>0.930331</td>\n",
       "      <td>0.560184</td>\n",
       "      <td>[[457, 26], [21, 39]]</td>\n",
       "      <td>RF_modT0.joblib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>DNN</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.815838</td>\n",
       "      <td>0.330508</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.438202</td>\n",
       "      <td>0.803882</td>\n",
       "      <td>0.296565</td>\n",
       "      <td>[[404, 79], [21, 39]]</td>\n",
       "      <td>DNN0.h5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>DNN_modT</td>\n",
       "      <td>0.505391</td>\n",
       "      <td>0.817680</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.440678</td>\n",
       "      <td>0.803882</td>\n",
       "      <td>0.296565</td>\n",
       "      <td>[[405, 78], [21, 39]]</td>\n",
       "      <td>DNN_modT0.h5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      model  threshold  accuracy  precision  recall        f1   auc_roc  \\\n",
       "0        RF   0.500000  0.906077   0.615385    0.40  0.484848  0.930331   \n",
       "1   RF_modT   0.382500  0.913444   0.600000    0.65  0.624000  0.930331   \n",
       "2       DNN   0.500000  0.815838   0.330508    0.65  0.438202  0.803882   \n",
       "3  DNN_modT   0.505391  0.817680   0.333333    0.65  0.440678  0.803882   \n",
       "\n",
       "   avg_precision       confusion_matrix   model_filename  \n",
       "0       0.560184  [[468, 15], [36, 24]]       RF0.joblib  \n",
       "1       0.560184  [[457, 26], [21, 39]]  RF_modT0.joblib  \n",
       "2       0.296565  [[404, 79], [21, 39]]          DNN0.h5  \n",
       "3       0.296565  [[405, 78], [21, 39]]     DNN_modT0.h5  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13340 samples, validate on 1781 samples\n",
      "Epoch 1/100\n",
      "13340/13340 [==============================] - 3s 206us/step - loss: 0.7926 - acc: 0.6395 - val_loss: 0.6058 - val_acc: 0.7344\n",
      "Epoch 2/100\n",
      "13340/13340 [==============================] - 3s 218us/step - loss: 0.7896 - acc: 0.6397 - val_loss: 0.6058 - val_acc: 0.7344\n",
      "Epoch 3/100\n",
      "13340/13340 [==============================] - 3s 215us/step - loss: 0.7886 - acc: 0.6389 - val_loss: 0.6058 - val_acc: 0.7344\n",
      "Epoch 4/100\n",
      "13340/13340 [==============================] - 3s 214us/step - loss: 0.7935 - acc: 0.6389 - val_loss: 0.6058 - val_acc: 0.7344\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 8.673617791858755e-22.\n",
      "Epoch 5/100\n",
      "13340/13340 [==============================] - 3s 213us/step - loss: 0.7917 - acc: 0.6403 - val_loss: 0.6058 - val_acc: 0.7344\n",
      "Epoch 6/100\n",
      "13340/13340 [==============================] - 3s 212us/step - loss: 0.7906 - acc: 0.6421 - val_loss: 0.6058 - val_acc: 0.7344\n",
      "Epoch 7/100\n",
      "13340/13340 [==============================] - 3s 215us/step - loss: 0.7909 - acc: 0.6407 - val_loss: 0.6058 - val_acc: 0.7344\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 4.336808895929377e-22.\n",
      "Epoch 8/100\n",
      "13340/13340 [==============================] - 3s 219us/step - loss: 0.7873 - acc: 0.6408 - val_loss: 0.6058 - val_acc: 0.7344\n",
      "Epoch 9/100\n",
      "13340/13340 [==============================] - 3s 224us/step - loss: 0.7938 - acc: 0.6364 - val_loss: 0.6058 - val_acc: 0.7344\n",
      "Epoch 10/100\n",
      "13340/13340 [==============================] - 3s 225us/step - loss: 0.7931 - acc: 0.6383 - val_loss: 0.6058 - val_acc: 0.7344\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 2.1684044479646887e-22.\n",
      "Epoch 11/100\n",
      "13340/13340 [==============================] - 3s 221us/step - loss: 0.7897 - acc: 0.6435 - val_loss: 0.6058 - val_acc: 0.7344\n",
      "Epoch 12/100\n",
      "13340/13340 [==============================] - 3s 211us/step - loss: 0.7910 - acc: 0.6357 - val_loss: 0.6058 - val_acc: 0.7344\n",
      "Epoch 13/100\n",
      "13340/13340 [==============================] - 3s 217us/step - loss: 0.7885 - acc: 0.6378 - val_loss: 0.6058 - val_acc: 0.7344\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 1.0842022239823443e-22.\n",
      "Epoch 14/100\n",
      "13340/13340 [==============================] - 3s 209us/step - loss: 0.7954 - acc: 0.6359 - val_loss: 0.6058 - val_acc: 0.7344\n",
      "Epoch 15/100\n",
      "13340/13340 [==============================] - 3s 208us/step - loss: 0.7901 - acc: 0.6382 - val_loss: 0.6058 - val_acc: 0.7344\n",
      "Epoch 16/100\n",
      "13340/13340 [==============================] - 3s 213us/step - loss: 0.7933 - acc: 0.6373 - val_loss: 0.6058 - val_acc: 0.7344\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 5.421011119911722e-23.\n",
      "Epoch 17/100\n",
      "13340/13340 [==============================] - 3s 207us/step - loss: 0.7926 - acc: 0.6397 - val_loss: 0.6058 - val_acc: 0.7344\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00017: early stopping\n",
      "         SR.p53: 0.76870\n",
      "Model saved and metrics table updated.\n",
      "Model saved and metrics table updated.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>threshold</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>auc_roc</th>\n",
       "      <th>avg_precision</th>\n",
       "      <th>confusion_matrix</th>\n",
       "      <th>model_filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.933442</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.024390</td>\n",
       "      <td>0.046512</td>\n",
       "      <td>0.806702</td>\n",
       "      <td>0.221990</td>\n",
       "      <td>[[574, 1], [40, 1]]</td>\n",
       "      <td>RF0.joblib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>RF_modT</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>0.886364</td>\n",
       "      <td>0.254237</td>\n",
       "      <td>0.365854</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.806702</td>\n",
       "      <td>0.221990</td>\n",
       "      <td>[[531, 44], [26, 15]]</td>\n",
       "      <td>RF_modT0.joblib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>DNN</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.753247</td>\n",
       "      <td>0.150943</td>\n",
       "      <td>0.585366</td>\n",
       "      <td>0.240000</td>\n",
       "      <td>0.768696</td>\n",
       "      <td>0.168234</td>\n",
       "      <td>[[440, 135], [17, 24]]</td>\n",
       "      <td>DNN0.h5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>DNN_modT</td>\n",
       "      <td>0.540362</td>\n",
       "      <td>0.780844</td>\n",
       "      <td>0.169014</td>\n",
       "      <td>0.585366</td>\n",
       "      <td>0.262295</td>\n",
       "      <td>0.768696</td>\n",
       "      <td>0.168234</td>\n",
       "      <td>[[457, 118], [17, 24]]</td>\n",
       "      <td>DNN_modT0.h5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      model  threshold  accuracy  precision    recall        f1   auc_roc  \\\n",
       "0        RF   0.500000  0.933442   0.500000  0.024390  0.046512  0.806702   \n",
       "1   RF_modT   0.220000  0.886364   0.254237  0.365854  0.300000  0.806702   \n",
       "2       DNN   0.500000  0.753247   0.150943  0.585366  0.240000  0.768696   \n",
       "3  DNN_modT   0.540362  0.780844   0.169014  0.585366  0.262295  0.768696   \n",
       "\n",
       "   avg_precision        confusion_matrix   model_filename  \n",
       "0       0.221990     [[574, 1], [40, 1]]       RF0.joblib  \n",
       "1       0.221990   [[531, 44], [26, 15]]  RF_modT0.joblib  \n",
       "2       0.168234  [[440, 135], [17, 24]]          DNN0.h5  \n",
       "3       0.168234  [[457, 118], [17, 24]]     DNN_modT0.h5  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for target in y_tr.columns:\n",
    "    # Determine rows with available data\n",
    "    rows_tr = np.isfinite(y_tr[target]).values\n",
    "    rows_te = np.isfinite(y_te[target]).values\n",
    "    x,y = x_tr[rows_tr], y_tr[target][rows_tr]\n",
    "    \n",
    "    # Address Class Imbalance\n",
    "    x_train, x_val, y_train, y_val = train_test_split(x, y, stratify=y, \\\n",
    "                                                      test_size=0.2, random_state=42)\n",
    "    ros = RandomOverSampler(random_state=0)\n",
    "    x_resampled, y_resampled = ros.fit_sample(x_train,y_train)\n",
    "    \n",
    "    # Train the DNN\n",
    "    DNN.fit(\n",
    "        x_resampled, y_resampled, batch_size=512, epochs=100,\\\n",
    "        validation_data=(x_val,y_val), verbose=1,\n",
    "        callbacks=[\n",
    "            keras.callbacks.EarlyStopping(patience=16,verbose=1,\\\n",
    "                                          restore_best_weights=True),\n",
    "            keras.callbacks.ReduceLROnPlateau(factor=0.5,patience=3,verbose=1)\n",
    "        ])\n",
    "    \n",
    "    # Get predictions, calculate model performance and save info\n",
    "    p_te = DNN.predict(x_te[rows_te])[:,0]\n",
    "    y_testing=y_te[target][rows_te]\n",
    "    auc_te = roc_auc_score(y_testing, p_te)\n",
    "    print(\"%15s: %3.5f\" % (target, auc_te))\n",
    "\n",
    "    y_hat_testing=DNN.predict_classes(x_te[rows_te])\n",
    "    average_precision=average_precision_score(y_testing,p_te)\n",
    "    mv=evaluate_model_predictions(target,'DNN',0.5,y_testing,y_hat_testing,\\\n",
    "                                  auc_te,average_precision)\n",
    "    filename = check_and_save(target,mv,DNN,True)\n",
    "    \n",
    "    # Find max F1 varying probability threshold, calculate modified performance, save\n",
    "    precision, recall, thresholds = precision_recall_curve(y_testing,p_te)\n",
    "    # f1 = 2*precision*recall/(precision+recall)  # Sometimes precision=recall=0!\n",
    "    p,r,t = zip(*[i for i in zip(precision,recall,np.append(thresholds,1)) if i[0:2]!=(0,0)])\n",
    "    p,r,t = np.array(p),np.array(r),np.array(t)\n",
    "    f1 = 2*p*r/(p+r)\n",
    "    m_idx = np.argmax(f1)\n",
    "    m_thresh = thresholds[m_idx]\n",
    "    y_hat_testing_adj=adjusted_classes(p_te,m_thresh)\n",
    "    mv=evaluate_model_predictions(target,'DNN_modT',m_thresh,y_testing,y_hat_testing_adj,\\\n",
    "                                  auc_te,average_precision)\n",
    "    if filename is None:\n",
    "        check_and_save(target,mv,DNN,True)\n",
    "    else:\n",
    "        check_and_save(target,mv,filename,True)\n",
    "    display(get_model_perfs(target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Performance Metrics Saving Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T05:43:49.580236Z",
     "start_time": "2019-12-02T05:43:49.383833Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "p_te = DNN.predict(x_te[rows_te])[:,0]\n",
    "y_testing=y_te[target][rows_te]\n",
    "auc_te = roc_auc_score(y_testing, p_te)\n",
    "print(\"%15s: %3.5f\" % (target, auc_te))\n",
    "\n",
    "y_hat_testing=DNN.predict_classes(x_te[rows_te])\n",
    "average_precision=average_precision_score(y_testing,p_te)\n",
    "mv=evaluate_model_predictions(target,'DNN',0.5,y_testing,y_hat_testing,\\\n",
    "                              auc_te,average_precision)\n",
    "#     check_and_save(target,mv,DNN,True)\n",
    "display(mv)\n",
    "precision, recall, thresholds = precision_recall_curve(y_testing,p_te)\n",
    "# f1 = 2*precision*recall/(precision+recall)\n",
    "p,r,t = zip(*[i for i in zip(precision,recall,np.append(thresholds,1)) if i[0:2]!=(0,0)])\n",
    "p,r,t = np.array(p),np.array(r),np.array(t)\n",
    "f1 = 2*p*r/(p+r)\n",
    "m_idx = np.argmax(f1)\n",
    "m_thresh = thresholds[m_idx]\n",
    "y_hat_testing_adj=adjusted_classes(p_te,m_thresh)\n",
    "mv=evaluate_model_predictions(target,'DNN_modT',m_thresh,y_testing,y_hat_testing_adj,\\\n",
    "                              auc_te,average_precision)\n",
    "#     check_and_save(target,mv,DNN,True)\n",
    "# display(get_model_perfs(target))\n",
    "display(mv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Troubleshooting Precision-Recall Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T05:08:08.069861Z",
     "start_time": "2019-12-02T05:08:08.008719Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "f1 = 2*precision*recall/(precision+recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T05:52:54.150467Z",
     "start_time": "2019-12-02T05:52:54.089918Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "a, b = zip(*zip(precision,recall))\n",
    "type(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T05:41:49.450684Z",
     "start_time": "2019-12-02T05:41:49.389635Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "p,r,t = zip(*[i for i in zip(precision,recall,np.append(thresholds,1)) if i[0:2]!=(0,0)])\n",
    "p,r,t = np.array(p),np.array(r),np.array(t)\n",
    "f1 = 2*p*r/(p+r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Testing DNN Model Saving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T00:48:15.368581Z",
     "start_time": "2019-12-03T00:48:15.274930Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DNN_test0.h5'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = save_model(target,'DNN_test',DNN,True)\n",
    "filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T00:48:29.774885Z",
     "start_time": "2019-12-03T00:48:28.661557Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.engine.sequential.Sequential at 0x1276ace10>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_model(target,filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T00:48:35.160766Z",
     "start_time": "2019-12-03T00:48:35.156689Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.5'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Saved DNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T08:55:11.028102Z",
     "start_time": "2019-12-09T08:55:10.413604Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "target = 'NR.PPAR.gamma'\n",
    "target = 'NR.AhR'\n",
    "DNN = read_model(target,'DNN_modT0.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T08:55:12.233662Z",
     "start_time": "2019-12-09T08:55:12.230236Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_144\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "h0_sigmoid_activation (Dense (None, 64)                105280    \n",
      "_________________________________________________________________\n",
      "Dropout0 (Dropout)           (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_144 (Dense)            (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 105,345\n",
      "Trainable params: 105,345\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "DNN.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance of Read DNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T08:55:23.458973Z",
     "start_time": "2019-12-09T08:55:23.373904Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         NR.AhR: 0.85306\n"
     ]
    }
   ],
   "source": [
    "rows_tr = np.isfinite(y_tr[target]).values\n",
    "rows_te = np.isfinite(y_te[target]).values\n",
    "y_testing=y_te[target][rows_te]\n",
    "p_te = DNN.predict(x_te[rows_te])[:,0]\n",
    "auc_te = roc_auc_score(y_te[target][rows_te], p_te)\n",
    "average_precision=average_precision_score(y_testing, p_te)\n",
    "print(\"%15s: %3.5f\" % (target, auc_te))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T08:55:25.275732Z",
     "start_time": "2019-12-09T08:55:25.246157Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model                                  DNN\n",
       "threshold                              0.5\n",
       "accuracy                               0.8\n",
       "precision                         0.347826\n",
       "recall                            0.767123\n",
       "f1                                0.478632\n",
       "auc_roc                           0.853065\n",
       "mcc                               0.420919\n",
       "avg_precision                      0.49692\n",
       "confusion_matrix    [[432, 105], [17, 56]]\n",
       "dtype: object"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat_testing=DNN.predict_classes(x_te[rows_te])\n",
    "evaluate_model_predictions(target, 'DNN', 0.5, y_testing, y_hat_testing, auc_te, average_precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T08:55:35.925743Z",
     "start_time": "2019-12-09T08:55:35.923364Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T08:55:37.016341Z",
     "start_time": "2019-12-09T08:55:36.870419Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_te[target][rows_te], p_te)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "         lw=2, label='ROC curve (area = %0.3f)' % auc_te)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title(target+' Receiver operating characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix at Alternate Decision Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T09:27:12.382586Z",
     "start_time": "2019-12-09T09:27:12.365384Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision-Recall Curve \"Average Precision\": 0.4969\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "model                                 DNN\n",
       "threshold                          0.7035\n",
       "accuracy                         0.893443\n",
       "precision                        0.547619\n",
       "recall                           0.630137\n",
       "f1                               0.585987\n",
       "auc_roc                          0.853065\n",
       "mcc                              0.526888\n",
       "avg_precision                     0.49692\n",
       "confusion_matrix    [[499, 38], [27, 46]]\n",
       "dtype: object"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decision_threshold = 0.7035\n",
    "y_testing=y_te[target][rows_te]\n",
    "y_hat_testing_adj=adjusted_classes(p_te,decision_threshold)\n",
    "print('Precision-Recall Curve \"Average Precision\": %0.4f' %average_precision)\n",
    "mv=evaluate_model_predictions(target,'DNN',decision_threshold,y_testing,y_hat_testing_adj,\\\n",
    "                              auc_te,average_precision)\n",
    "mv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision-Recall Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T08:55:52.975333Z",
     "start_time": "2019-12-09T08:55:52.967256Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max F1: 0.59494, threshold prob: 0.70347\n",
      "Max F1 Precision: 0.55294, Recall: 0.64384\n"
     ]
    }
   ],
   "source": [
    "precision, recall, thresholds = precision_recall_curve(y_testing,p_te)\n",
    "precision, recall, thresholds = zip(*[i for i in zip(precision,recall,np.append(thresholds,1)) if i[0:2]!=(0,0)])\n",
    "precision, recall, thresholds = np.array(precision),np.array(recall),np.array(thresholds)\n",
    "f1 = 2*precision*recall/(precision+recall)\n",
    "m_idx = np.argmax(f1)\n",
    "m_thresh = thresholds[m_idx]\n",
    "print('Max F1: %0.5f, threshold prob: %0.5f' % (f1[m_idx], m_thresh))\n",
    "print('Max F1 Precision: %0.5f, Recall: %0.5f' % (precision[m_idx],recall[m_idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T08:55:53.712552Z",
     "start_time": "2019-12-09T08:55:53.708882Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17328140139579773"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thresholds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T08:55:56.820798Z",
     "start_time": "2019-12-09T08:55:56.085080Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sean/anaconda3/envs/metis/lib/python3.7/site-packages/sklearn/metrics/classification.py:872: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
     ]
    }
   ],
   "source": [
    "mccs = []\n",
    "for th in thresholds:\n",
    "    y_hat_testing_adj=adjusted_classes(p_te,th)\n",
    "    mccs.append(matthews_corrcoef(y_testing,y_hat_testing_adj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T12:18:13.003466Z",
     "start_time": "2019-12-09T12:18:12.616351Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "no_skill = len(y_testing[y_testing==1]) / len(y_testing)\n",
    "fig = plt.figure(figsize=[8,8*4.8/6.4])\n",
    "plt.step(recall, precision, color='b', alpha=0.2, where='post')\n",
    "plt.fill_between(recall, precision, step='post', alpha=0.2, color='b',\\\n",
    "                 label='DNN')\n",
    "plt.plot([0, 1], [no_skill, no_skill], color='r', linestyle='--', label='No Skill:'+' %0.3f' % no_skill)\n",
    "plt.plot([0, 1], [precision[m_idx],precision[m_idx]], color='k', alpha=0.4, linestyle=':')\n",
    "plt.plot([recall[m_idx],recall[m_idx]],[0, 1], color='k', alpha=0.4,linestyle=':',\\\n",
    "         label='Prec/Rec @ Max F1')\n",
    "plt.text(recall[m_idx], f1[m_idx]+0.01, 'Max F1={0:0.3f}'.format(f1[m_idx]))\n",
    "plt.plot(recall,f1,color='k',label='F1')\n",
    "# plt.plot(recall,mccs,label=\"Matthew's Corr Coef\")\n",
    "plt.xlabel('Recall', fontsize=14)\n",
    "plt.ylabel('Precision or F1 Score', fontsize=14)\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.legend()\n",
    "title=target+' DNN\\n Precision-Recall curve: AP={0:0.2f}'.format(\n",
    "          average_precision)\n",
    "plt.title(title, fontsize=16);\n",
    "fig.savefig('./reports/figures/'+target+'_DNN_PrecisionRecallCurve2.svg',\\\n",
    "            format='svg', dpi=1200, transparent=True, bbox_inches = \"tight\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T09:09:41.402783Z",
     "start_time": "2019-12-09T09:09:41.392980Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.16704805491990846, 1.0, 0.28627450980392155, 0.23198324564311515, 0.17328140139579773)\n",
      "(0.1651376146788991, 0.9863013698630136, 0.2829076620825148, 0.22173369633779677, 0.17465224862098694)\n",
      "(0.16551724137931034, 0.9863013698630136, 0.28346456692913385, 0.2226895755682889, 0.1813332438468933)\n",
      "(0.16589861751152074, 0.9863013698630136, 0.28402366863905326, 0.22364576427189672, 0.1842179298400879)\n",
      "(0.16628175519630484, 0.9863013698630136, 0.2845849802371542, 0.2246022885079155, 0.18537762761116028)\n",
      "(0.16511627906976745, 0.9726027397260274, 0.2823061630218688, 0.216399972952515, 0.18541648983955383)\n",
      "(0.1655011655011655, 0.9726027397260274, 0.28286852589641437, 0.2173758670266673, 0.18947604298591614)\n",
      "(0.1658878504672897, 0.9726027397260274, 0.2834331337325349, 0.2183519976700012, 0.1915532648563385)\n",
      "(0.16627634660421545, 0.9726027397260274, 0.284, 0.21932839276507035, 0.19245806336402893)\n",
      "(0.16666666666666666, 0.9726027397260274, 0.2845691382765531, 0.22030508001372517, 0.19489645957946777)\n",
      "(0.16705882352941176, 0.9726027397260274, 0.285140562248996, 0.22128208694377893, 0.19898191094398499)\n",
      "(0.16745283018867924, 0.9726027397260274, 0.2857142857142857, 0.2222594409155313, 0.20502963662147522)\n",
      "(0.16784869976359337, 0.9726027397260274, 0.28629032258064513, 0.22323716912815372, 0.21005719900131226)\n",
      "(0.16587677725118483, 0.958904109589041, 0.2828282828282828, 0.21327709227902755, 0.21884813904762268)\n",
      "(0.166270783847981, 0.958904109589041, 0.2834008097165992, 0.21427167674288639, 0.21893590688705444)\n",
      "(0.16666666666666666, 0.958904109589041, 0.2839756592292089, 0.21526650922957605, 0.22046548128128052)\n",
      "(0.16467780429594273, 0.9452054794520548, 0.2804878048780488, 0.20537087420303693, 0.22505328059196472)\n",
      "(0.16507177033492823, 0.9452054794520548, 0.28105906313645623, 0.2063817021262738, 0.22538992762565613)\n",
      "(0.16546762589928057, 0.9452054794520548, 0.2816326530612245, 0.20739266406115034, 0.22658228874206543)\n",
      "(0.1658653846153846, 0.9452054794520548, 0.2822085889570552, 0.20840379054879396, 0.22743678092956543)\n",
      "(0.16626506024096385, 0.9452054794520548, 0.2827868852459016, 0.20941511193011525, 0.2280375361442566)\n",
      "(0.16666666666666666, 0.9452054794520548, 0.28336755646817247, 0.21042665835305346, 0.2308761477470398)\n",
      "(0.16707021791767554, 0.9452054794520548, 0.2839506172839506, 0.21143845977967277, 0.230894535779953)\n",
      "(0.16747572815533981, 0.9452054794520548, 0.2845360824742268, 0.21245054599311503, 0.23091742396354675)\n",
      "(0.1678832116788321, 0.9452054794520548, 0.28512396694214875, 0.21346294660441392, 0.23093682527542114)\n",
      "(0.16829268292682928, 0.9452054794520548, 0.28571428571428575, 0.2144756910591756, 0.23449993133544922)\n",
      "(0.1687041564792176, 0.9452054794520548, 0.28630705394190875, 0.21548880864413, 0.2361835241317749)\n",
      "(0.16911764705882354, 0.9452054794520548, 0.28690228690228686, 0.2165023284935588, 0.24313059449195862)\n",
      "(0.16953316953316952, 0.9452054794520548, 0.2875, 0.2175162795956027, 0.24752092361450195)\n",
      "(0.16995073891625614, 0.9452054794520548, 0.2881002087682672, 0.21853069079845414, 0.2511894702911377)\n",
      "(0.17037037037037037, 0.9452054794520548, 0.2887029288702929, 0.21954559081643807, 0.2520563304424286)\n",
      "(0.1707920792079208, 0.9452054794520548, 0.289308176100629, 0.2205610082359862, 0.25265181064605713)\n",
      "(0.17121588089330025, 0.9452054794520548, 0.28991596638655465, 0.2215769715215076, 0.25850510597229004)\n",
      "(0.17164179104477612, 0.9452054794520548, 0.2905263157894737, 0.2225935090211599, 0.26115885376930237)\n",
      "(0.17336683417085427, 0.9452054794520548, 0.2929936305732484, 0.22666596437071188, 0.2611609101295471)\n",
      "(0.17380352644836272, 0.9452054794520548, 0.29361702127659567, 0.2276857944868091, 0.26182693243026733)\n",
      "(0.17424242424242425, 0.9452054794520548, 0.2942430703624734, 0.22870636677627437, 0.2626449763774872)\n",
      "(0.17468354430379746, 0.9452054794520548, 0.2948717948717948, 0.22972770892749622, 0.26353734731674194)\n",
      "(0.1751269035532995, 0.9452054794520548, 0.29550321199143476, 0.23074984855562236, 0.2664661407470703)\n",
      "(0.17557251908396945, 0.9452054794520548, 0.296137339055794, 0.23177281320758947, 0.2691808342933655)\n",
      "(0.1760204081632653, 0.9452054794520548, 0.29677419354838713, 0.23279663036708545, 0.2701396942138672)\n",
      "(0.17647058823529413, 0.9452054794520548, 0.2974137931034483, 0.23382132745944795, 0.27947187423706055)\n",
      "(0.17692307692307693, 0.9452054794520548, 0.2980561555075594, 0.2348469318565005, 0.28066617250442505)\n",
      "(0.17737789203084833, 0.9452054794520548, 0.2987012987012987, 0.2358734708813301, 0.2826305031776428)\n",
      "(0.17783505154639176, 0.9452054794520548, 0.299349240780911, 0.23690097181300804, 0.2844478189945221)\n",
      "(0.17829457364341086, 0.9452054794520548, 0.3, 0.2379294618912565, 0.2878362834453583)\n",
      "(0.17922077922077922, 0.9452054794520548, 0.3013100436681223, 0.23998951827724943, 0.28785502910614014)\n",
      "(0.1796875, 0.9452054794520548, 0.30196936542669583, 0.24102113890898402, 0.28961360454559326)\n",
      "(0.18110236220472442, 0.9452054794520548, 0.3039647577092512, 0.24412269605809403, 0.29591673612594604)\n",
      "(0.18157894736842106, 0.9452054794520548, 0.30463576158940403, 0.24515887052643942, 0.29658931493759155)\n",
      "(0.1820580474934037, 0.9452054794520548, 0.30530973451327437, 0.2461962511865791, 0.3028804063796997)\n",
      "(0.18253968253968253, 0.9452054794520548, 0.30598669623059865, 0.24723486512631915, 0.30307096242904663)\n",
      "(0.1830238726790451, 0.9452054794520548, 0.30666666666666664, 0.24827473943831163, 0.30376410484313965)\n",
      "(0.18351063829787234, 0.9452054794520548, 0.30734966592427615, 0.24931590122427322, 0.30378958582878113)\n",
      "(0.184, 0.9452054794520548, 0.3080357142857143, 0.2503583775991756, 0.31390249729156494)\n",
      "(0.18449197860962566, 0.9452054794520548, 0.30872483221476504, 0.25140219569540917, 0.3148218095302582)\n",
      "(0.18498659517426275, 0.9452054794520548, 0.3094170403587444, 0.2524473826669214, 0.3149172365665436)\n",
      "(0.18548387096774194, 0.9452054794520548, 0.3101123595505618, 0.25349396569333293, 0.32115602493286133)\n",
      "(0.18598382749326145, 0.9452054794520548, 0.3108108108108108, 0.25454197198403106, 0.3213396668434143)\n",
      "(0.1837837837837838, 0.9315068493150684, 0.30699774266365687, 0.2452525182015308, 0.3221105933189392)\n",
      "(0.1842818428184282, 0.9315068493150684, 0.30769230769230776, 0.24631095429795052, 0.32274746894836426)\n",
      "(0.18478260869565216, 0.9315068493150684, 0.30839002267573695, 0.24737076327577912, 0.3255764842033386)\n",
      "(0.19101123595505617, 0.9315068493150684, 0.317016317016317, 0.2602058511245016, 0.327288955450058)\n",
      "(0.192090395480226, 0.9315068493150684, 0.3185011709601874, 0.2623680951392541, 0.3273174464702606)\n",
      "(0.19263456090651557, 0.9315068493150684, 0.31924882629107976, 0.2634519038366168, 0.3282669484615326)\n",
      "(0.19318181818181818, 0.9315068493150684, 0.32, 0.26453754243433975, 0.335477352142334)\n",
      "(0.19428571428571428, 0.9315068493150684, 0.3215130023640662, 0.26671442622519625, 0.3356223702430725)\n",
      "(0.19484240687679083, 0.9315068493150684, 0.3222748815165877, 0.2678057301189512, 0.33631378412246704)\n",
      "(0.19653179190751446, 0.9315068493150684, 0.324582338902148, 0.27109144426703724, 0.3374090790748596)\n",
      "(0.19710144927536233, 0.9315068493150684, 0.3253588516746411, 0.2721907156066147, 0.3415788412094116)\n",
      "(0.19767441860465115, 0.9315068493150684, 0.32613908872901676, 0.2732920534625334, 0.34907305240631104)\n",
      "(0.19825072886297376, 0.9315068493150684, 0.3269230769230769, 0.2743954879488086, 0.35145437717437744)\n",
      "(0.19883040935672514, 0.9315068493150684, 0.32771084337349393, 0.2755010493129492, 0.3523406684398651)\n",
      "(0.19941348973607037, 0.9315068493150684, 0.32850241545893716, 0.27660876794025185, 0.3578851521015167)\n",
      "(0.2, 0.9315068493150684, 0.3292978208232446, 0.2777186743581163, 0.35950174927711487)\n",
      "(0.20058997050147492, 0.9315068493150684, 0.3300970873786408, 0.27883079924038284, 0.3601006865501404)\n",
      "(0.20118343195266272, 0.9315068493150684, 0.3309002433090025, 0.2799451734116934, 0.3634076714515686)\n",
      "(0.20178041543026706, 0.9315068493150684, 0.3317073170731707, 0.2810618278518785, 0.36704957485198975)\n",
      "(0.22388059701492538, 0.821917808219178, 0.3519061583577713, 0.2842095116928305, 0.3719310760498047)\n",
      "(0.2247191011235955, 0.821917808219178, 0.35294117647058826, 0.28554421284083487, 0.3727127015590668)\n",
      "(0.22556390977443608, 0.821917808219178, 0.3539823008849557, 0.28688319052989053, 0.3776320517063141)\n",
      "(0.22641509433962265, 0.821917808219178, 0.35502958579881655, 0.2882264987905576, 0.37996989488601685)\n",
      "(0.22727272727272727, 0.821917808219178, 0.3560830860534125, 0.2895741922240921, 0.381315678358078)\n",
      "(0.22813688212927757, 0.821917808219178, 0.35714285714285715, 0.2909263260137553, 0.3820660412311554)\n",
      "(0.22900763358778625, 0.821917808219178, 0.3582089552238806, 0.2922829559363349, 0.38274821639060974)\n",
      "(0.22988505747126436, 0.821917808219178, 0.35928143712574856, 0.2936441383738822, 0.38496947288513184)\n",
      "(0.23076923076923078, 0.821917808219178, 0.36036036036036034, 0.2950099303256722, 0.3851257860660553)\n",
      "(0.23166023166023167, 0.821917808219178, 0.3614457831325301, 0.29638038942039235, 0.385318398475647)\n",
      "(0.23255813953488372, 0.821917808219178, 0.36253776435045315, 0.29775557392856566, 0.3880103528499603)\n",
      "(0.22957198443579765, 0.8082191780821918, 0.35757575757575755, 0.2889066801095456, 0.3955206274986267)\n",
      "(0.23046875, 0.8082191780821918, 0.3586626139817629, 0.2902860200856487, 0.39750370383262634)\n",
      "(0.23137254901960785, 0.8082191780821918, 0.35975609756097565, 0.29167014235656474, 0.40052223205566406)\n",
      "(0.23228346456692914, 0.8082191780821918, 0.3608562691131499, 0.2930591073083078, 0.40716466307640076)\n",
      "(0.233201581027668, 0.8082191780821918, 0.36196319018404904, 0.29445297602014026, 0.40972521901130676)\n",
      "(0.23412698412698413, 0.8082191780821918, 0.3630769230769231, 0.2958518102783651, 0.4136120080947876)\n",
      "(0.2350597609561753, 0.8082191780821918, 0.3641975308641976, 0.29725567259039654, 0.41694873571395874)\n",
      "(0.236, 0.8082191780821918, 0.3653250773993808, 0.29866462619911543, 0.4180801510810852)\n",
      "(0.23694779116465864, 0.8082191780821918, 0.36645962732919257, 0.3000787350975182, 0.41919639706611633)\n",
      "(0.23577235772357724, 0.7945205479452054, 0.3636363636363637, 0.2940567758257069, 0.42404794692993164)\n",
      "(0.23265306122448978, 0.7808219178082192, 0.3584905660377359, 0.2851825957360278, 0.43023058772087097)\n",
      "(0.296875, 0.7808219178082192, 0.43018867924528303, 0.37001087127925764, 0.4336148798465729)\n",
      "(0.29842931937172773, 0.7808219178082192, 0.43181818181818177, 0.3718385809811919, 0.43379145860671997)\n",
      "(0.3, 0.7808219178082192, 0.4334600760456273, 0.37367691386197166, 0.43380308151245117)\n",
      "(0.30158730158730157, 0.7808219178082192, 0.43511450381679384, 0.37552601958122805, 0.4338268041610718)\n",
      "(0.30319148936170215, 0.7808219178082192, 0.43678160919540227, 0.37738605045438156, 0.44090986251831055)\n",
      "(0.3048128342245989, 0.7808219178082192, 0.4384615384615385, 0.379257161516972, 0.44210466742515564)\n",
      "(0.3064516129032258, 0.7808219178082192, 0.44015444015444016, 0.3811395105908674, 0.44248050451278687)\n",
      "(0.3081081081081081, 0.7808219178082192, 0.44186046511627913, 0.38303325835241836, 0.4425780475139618)\n",
      "(0.30978260869565216, 0.7808219178082192, 0.443579766536965, 0.3849385684026259, 0.44821488857269287)\n",
      "(0.3131868131868132, 0.7808219178082192, 0.4470588235294118, 0.3887845448319498, 0.4482244551181793)\n",
      "(0.3149171270718232, 0.7808219178082192, 0.44881889763779526, 0.39072555369747913, 0.44839003682136536)\n",
      "(0.31666666666666665, 0.7808219178082192, 0.450592885375494, 0.392678809980109, 0.44890159368515015)\n",
      "(0.31843575418994413, 0.7808219178082192, 0.4523809523809524, 0.3946444930322696, 0.44973453879356384)\n",
      "(0.3202247191011236, 0.7808219178082192, 0.4541832669322709, 0.3966227855985539, 0.450889527797699)\n",
      "(0.3220338983050847, 0.7808219178082192, 0.45599999999999996, 0.39861387390215625, 0.45164141058921814)\n",
      "(0.32386363636363635, 0.7808219178082192, 0.45783132530120485, 0.4006179477339859, 0.45217472314834595)\n",
      "(0.32571428571428573, 0.7808219178082192, 0.45967741935483875, 0.4026352005445552, 0.45397713780403137)\n",
      "(0.3275862068965517, 0.7808219178082192, 0.46153846153846156, 0.4046658295387445, 0.45853912830352783)\n",
      "(0.32947976878612717, 0.7808219178082192, 0.46341463414634143, 0.4067100357735539, 0.4633287787437439)\n",
      "(0.3313953488372093, 0.7808219178082192, 0.46530612244897956, 0.4087680242589512, 0.4638950824737549)\n",
      "(0.3333333333333333, 0.7808219178082192, 0.4672131147540984, 0.41084000406193644, 0.46691304445266724)\n",
      "(0.3352941176470588, 0.7808219178082192, 0.4691358024691358, 0.41292618841394346, 0.47025978565216064)\n",
      "(0.33727810650887574, 0.7808219178082192, 0.47107438016528924, 0.4150267948217078, 0.47523051500320435)\n",
      "(0.3413173652694611, 0.7808219178082192, 0.4750000000000001, 0.41927216589849775, 0.4752364754676819)\n",
      "(0.3433734939759036, 0.7808219178082192, 0.4769874476987447, 0.42141738800654555, 0.4776079058647156)\n",
      "(0.34545454545454546, 0.7808219178082192, 0.4789915966386555, 0.4235779472966141, 0.47870761156082153)\n",
      "(0.3475609756097561, 0.7808219178082192, 0.48101265822784806, 0.42575408444596025, 0.4796389937400818)\n",
      "(0.34355828220858897, 0.7671232876712328, 0.47457627118644063, 0.4165321547440876, 0.4827778935432434)\n",
      "(0.345679012345679, 0.7671232876712328, 0.4765957446808511, 0.4187178011201899, 0.4845709800720215)\n",
      "(0.34782608695652173, 0.7671232876712328, 0.4786324786324786, 0.42091948729564005, 0.5028659105300903)\n",
      "(0.35, 0.7671232876712328, 0.48068669527896984, 0.4231374707334297, 0.5120404958724976)\n",
      "(0.3522012578616352, 0.7671232876712328, 0.48275862068965514, 0.42537201440845973, 0.5121070742607117)\n",
      "(0.35443037974683544, 0.7671232876712328, 0.48484848484848486, 0.4276233869641479, 0.5136734843254089)\n",
      "(0.35668789808917195, 0.7671232876712328, 0.48695652173913045, 0.4298918628744842, 0.5142155885696411)\n",
      "(0.358974358974359, 0.7671232876712328, 0.48908296943231433, 0.4321777226117572, 0.5142219662666321)\n",
      "(0.36129032258064514, 0.7671232876712328, 0.4912280701754385, 0.4344812528201906, 0.5179610252380371)\n",
      "(0.36363636363636365, 0.7671232876712328, 0.49339207048458145, 0.43680274649573786, 0.5200366973876953)\n",
      "(0.3660130718954248, 0.7671232876712328, 0.49557522123893805, 0.439142503172294, 0.5200368165969849)\n",
      "(0.3618421052631579, 0.7534246575342466, 0.48888888888888893, 0.42982395148019603, 0.5264448523521423)\n",
      "(0.36423841059602646, 0.7534246575342466, 0.49107142857142855, 0.4321753274482815, 0.5319575071334839)\n",
      "(0.36666666666666664, 0.7534246575342466, 0.4932735426008969, 0.4345455640235713, 0.5340980291366577)\n",
      "(0.3691275167785235, 0.7534246575342466, 0.4954954954954955, 0.4369349835255295, 0.5355691313743591)\n",
      "(0.3716216216216216, 0.7534246575342466, 0.4977375565610859, 0.43934391571820525, 0.5400340557098389)\n",
      "(0.3741496598639456, 0.7534246575342466, 0.5, 0.4417726980372222, 0.5410366654396057)\n",
      "(0.3698630136986301, 0.7397260273972602, 0.4931506849315068, 0.43238456145109644, 0.5411461591720581)\n",
      "(0.3724137931034483, 0.7397260273972602, 0.4954128440366973, 0.43482611946858185, 0.5431700944900513)\n",
      "(0.375, 0.7397260273972602, 0.4976958525345623, 0.43728821191848627, 0.5432511568069458)\n",
      "(0.3776223776223776, 0.7397260273972602, 0.5, 0.4397712031544869, 0.5433512330055237)\n",
      "(0.38028169014084506, 0.7397260273972602, 0.5023255813953488, 0.44227546631508113, 0.5433924794197083)\n",
      "(0.3829787234042553, 0.7397260273972602, 0.5046728971962616, 0.4448013836026574, 0.5435643196105957)\n",
      "(0.38571428571428573, 0.7397260273972602, 0.5070422535211268, 0.4473493465734601, 0.5491951704025269)\n",
      "(0.38848920863309355, 0.7397260273972602, 0.5094339622641509, 0.4499197564389544, 0.5498624444007874)\n",
      "(0.391304347826087, 0.7397260273972602, 0.5118483412322274, 0.4525130243791265, 0.5549741387367249)\n",
      "(0.39416058394160586, 0.7397260273972602, 0.5142857142857143, 0.4551295718682841, 0.5551242828369141)\n",
      "(0.39705882352941174, 0.7397260273972602, 0.5167464114832535, 0.45776983101394964, 0.5551245808601379)\n",
      "(0.4, 0.7397260273972602, 0.5192307692307693, 0.46043424490947676, 0.5551498532295227)\n",
      "(0.40298507462686567, 0.7397260273972602, 0.5217391304347826, 0.46312326800105114, 0.558928906917572)\n",
      "(0.40601503759398494, 0.7397260273972602, 0.5242718446601942, 0.46583736646977547, 0.5596955418586731)\n",
      "(0.4015151515151515, 0.726027397260274, 0.5170731707317072, 0.4563116572596843, 0.5599369406700134)\n",
      "(0.40458015267175573, 0.726027397260274, 0.5196078431372549, 0.45904348719619065, 0.5644659399986267)\n",
      "(0.4, 0.7123287671232876, 0.5123152709359605, 0.4494678111186665, 0.5647518038749695)\n",
      "(0.40310077519379844, 0.7123287671232876, 0.5148514851485148, 0.452217451469057, 0.5698819756507874)\n",
      "(0.40625, 0.7123287671232876, 0.5174129353233831, 0.4549936928788712, 0.5821796655654907)\n",
      "(0.4094488188976378, 0.7123287671232876, 0.52, 0.4577970619991535, 0.5833263397216797)\n",
      "(0.4126984126984127, 0.7123287671232876, 0.5226130653266331, 0.46062809984755, 0.5848428606987)\n",
      "(0.416, 0.7123287671232876, 0.5252525252525253, 0.46348736232246696, 0.5879695415496826)\n",
      "(0.41935483870967744, 0.7123287671232876, 0.5279187817258884, 0.46637542073989785, 0.5905325412750244)\n",
      "(0.42276422764227645, 0.7123287671232876, 0.5306122448979592, 0.4692928623941122, 0.5988749265670776)\n",
      "(0.4180327868852459, 0.6986301369863014, 0.5230769230769231, 0.45961354539097243, 0.5992032885551453)\n",
      "(0.4214876033057851, 0.6986301369863014, 0.5257731958762886, 0.4625524836528188, 0.6000864505767822)\n",
      "(0.425, 0.6986301369863014, 0.5284974093264249, 0.4655220873514139, 0.6016438603401184)\n",
      "(0.42857142857142855, 0.6986301369863014, 0.53125, 0.4685230019448543, 0.605911910533905)\n",
      "(0.4322033898305085, 0.6986301369863014, 0.5340314136125655, 0.47155589172162327, 0.607475757598877)\n",
      "(0.4358974358974359, 0.6986301369863014, 0.5368421052631579, 0.4746214405202779, 0.6074758172035217)\n",
      "(0.4396551724137931, 0.6986301369863014, 0.5396825396825397, 0.4777203524830585, 0.607756495475769)\n",
      "(0.4434782608695652, 0.6986301369863014, 0.5425531914893617, 0.48085335284532826, 0.6141749620437622)\n",
      "(0.4473684210526316, 0.6986301369863014, 0.5454545454545455, 0.48402118876287875, 0.6158145666122437)\n",
      "(0.45132743362831856, 0.6986301369863014, 0.5483870967741936, 0.4872246301792675, 0.615862250328064)\n",
      "(0.44642857142857145, 0.684931506849315, 0.5405405405405406, 0.4774190653483623, 0.6209880709648132)\n",
      "(0.44144144144144143, 0.6712328767123288, 0.532608695652174, 0.4675597290529613, 0.6261598467826843)\n",
      "(0.44545454545454544, 0.6712328767123288, 0.5355191256830601, 0.47078242319401425, 0.6312482357025146)\n",
      "(0.44954128440366975, 0.6712328767123288, 0.5384615384615385, 0.47404258074605204, 0.6399688720703125)\n",
      "(0.4537037037037037, 0.6712328767123288, 0.5414364640883979, 0.4773410668274493, 0.6504337191581726)\n",
      "(0.45794392523364486, 0.6712328767123288, 0.5444444444444444, 0.48067877443095997, 0.6526235342025757)\n",
      "(0.46226415094339623, 0.6712328767123288, 0.5474860335195532, 0.48405662559904206, 0.6549820899963379)\n",
      "(0.4666666666666667, 0.6712328767123288, 0.550561797752809, 0.4874755726603849, 0.6602199077606201)\n",
      "(0.47115384615384615, 0.6712328767123288, 0.5536723163841807, 0.49093659953144553, 0.663005530834198)\n",
      "(0.47572815533980584, 0.6712328767123288, 0.5568181818181819, 0.49444072308707887, 0.6661838293075562)\n",
      "(0.4803921568627451, 0.6712328767123288, 0.5599999999999999, 0.4979889946046414, 0.6677386164665222)\n",
      "(0.48514851485148514, 0.6712328767123288, 0.5632183908045977, 0.5015825012862766, 0.6680070161819458)\n",
      "(0.49, 0.6712328767123288, 0.5664739884393063, 0.5052223678644394, 0.6685200333595276)\n",
      "(0.494949494949495, 0.6712328767123288, 0.569767441860465, 0.5089097582960982, 0.6685491800308228)\n",
      "(0.5, 0.6712328767123288, 0.5730994152046783, 0.5126458775514693, 0.6696735620498657)\n",
      "(0.5051546391752577, 0.6712328767123288, 0.5764705882352941, 0.5164319735035874, 0.67081618309021)\n",
      "(0.5104166666666666, 0.6712328767123288, 0.5798816568047337, 0.5202693389255078, 0.6714624166488647)\n",
      "(0.5157894736842106, 0.6712328767123288, 0.5833333333333334, 0.5241593136024689, 0.6749551296234131)\n",
      "(0.5212765957446809, 0.6712328767123288, 0.5868263473053892, 0.5281032865669272, 0.6763989925384521)\n",
      "(0.5268817204301075, 0.6712328767123288, 0.5903614457831325, 0.5321026984650106, 0.6812368631362915)\n",
      "(0.5217391304347826, 0.6575342465753424, 0.5818181818181818, 0.5220459424463529, 0.6812376976013184)\n",
      "(0.5274725274725275, 0.6575342465753424, 0.5853658536585367, 0.5260971181786139, 0.6918179988861084)\n",
      "(0.5333333333333333, 0.6575342465753424, 0.588957055214724, 0.5302072168374239, 0.6931205987930298)\n",
      "(0.5393258426966292, 0.6575342465753424, 0.5925925925925926, 0.5343778794974818, 0.6961268782615662)\n",
      "(0.5340909090909091, 0.6438356164383562, 0.5838509316770186, 0.5242359169169968, 0.699643611907959)\n",
      "(0.5402298850574713, 0.6438356164383562, 0.5875, 0.5284643417793875, 0.7009394764900208)\n",
      "(0.5465116279069767, 0.6438356164383562, 0.5911949685534591, 0.5327573419224498, 0.7015763521194458)\n",
      "(0.5529411764705883, 0.6438356164383562, 0.5949367088607594, 0.5371167987126113, 0.703474760055542)\n",
      "(0.5476190476190477, 0.6301369863013698, 0.5859872611464968, 0.5268875474612481, 0.7047172784805298)\n",
      "(0.5421686746987951, 0.6164383561643836, 0.576923076923077, 0.5165806816519808, 0.7116811871528625)\n",
      "(0.5365853658536586, 0.6027397260273972, 0.567741935483871, 0.5061939017981382, 0.7117826342582703)\n",
      "(0.5308641975308642, 0.589041095890411, 0.5584415584415585, 0.4957248182676853, 0.7145785093307495)\n",
      "(0.5375, 0.589041095890411, 0.5620915032679739, 0.5001332455961321, 0.7175037860870361)\n",
      "(0.5443037974683544, 0.589041095890411, 0.5657894736842106, 0.5046147317167998, 0.7195552587509155)\n",
      "(0.5512820512820513, 0.589041095890411, 0.5695364238410597, 0.5091715920181218, 0.7261609435081482)\n",
      "(0.5584415584415584, 0.589041095890411, 0.5733333333333333, 0.5138062458110418, 0.7385987043380737)\n",
      "(0.5526315789473685, 0.5753424657534246, 0.563758389261745, 0.5032278298380919, 0.7396842241287231)\n",
      "(0.56, 0.5753424657534246, 0.5675675675675674, 0.5079385514567926, 0.7407654523849487)\n",
      "(0.5540540540540541, 0.5616438356164384, 0.5578231292517007, 0.4972633719731432, 0.743514895439148)\n",
      "(0.5616438356164384, 0.5616438356164384, 0.5616438356164384, 0.5020535190428815, 0.7512584924697876)\n",
      "(0.5555555555555556, 0.547945205479452, 0.5517241379310345, 0.491277332912799, 0.7515912055969238)\n",
      "(0.5633802816901409, 0.547945205479452, 0.5555555555555555, 0.4961505307570091, 0.7522017955780029)\n",
      "(0.5714285714285714, 0.547945205479452, 0.5594405594405595, 0.5011154061974199, 0.7522019147872925)\n",
      "(0.5588235294117647, 0.5205479452054794, 0.5390070921985816, 0.4792370046719041, 0.7522023320198059)\n",
      "(0.5522388059701493, 0.5068493150684932, 0.5285714285714286, 0.4681357486130678, 0.7537569999694824)\n",
      "(0.5606060606060606, 0.5068493150684932, 0.5323741007194245, 0.4731810010325658, 0.7537797689437866)\n",
      "(0.5692307692307692, 0.5068493150684932, 0.536231884057971, 0.47832825567542336, 0.753780722618103)\n",
      "(0.578125, 0.5068493150684932, 0.5401459854014599, 0.48358143814563354, 0.7537810802459717)\n",
      "(0.5873015873015873, 0.5068493150684932, 0.5441176470588236, 0.4889446893084347, 0.7537811398506165)\n",
      "(0.5806451612903226, 0.4931506849315068, 0.5333333333333333, 0.47770781348396407, 0.7537870407104492)\n",
      "(0.5737704918032787, 0.4794520547945205, 0.5223880597014925, 0.46634780979230545, 0.754169225692749)\n",
      "(0.5833333333333334, 0.4794520547945205, 0.5263157894736842, 0.471819949156107, 0.7552183866500854)\n",
      "(0.5932203389830508, 0.4794520547945205, 0.5303030303030303, 0.47741456662462617, 0.7633869647979736)\n",
      "(0.5862068965517241, 0.4657534246575342, 0.5190839694656488, 0.4659182608758292, 0.7639918923377991)\n",
      "(0.5789473684210527, 0.4520547945205479, 0.5076923076923077, 0.4542857922397978, 0.7660188674926758)\n",
      "(0.5892857142857143, 0.4520547945205479, 0.5116279069767442, 0.46000340164634734, 0.7682046890258789)\n",
      "(0.6, 0.4520547945205479, 0.515625, 0.46585836768084427, 0.7785959839820862)\n",
      "(0.6111111111111112, 0.4520547945205479, 0.5196850393700787, 0.47185694015035906, 0.7831449508666992)\n",
      "(0.6037735849056604, 0.4383561643835616, 0.507936507936508, 0.4600743128476988, 0.7891136407852173)\n",
      "(0.6153846153846154, 0.4383561643835616, 0.512, 0.46622514312940966, 0.7992188930511475)\n",
      "(0.6078431372549019, 0.4246575342465753, 0.4999999999999999, 0.45428921649007115, 0.7994481325149536)\n",
      "(0.6, 0.410958904109589, 0.4878048780487805, 0.4421911395490339, 0.7995601892471313)\n",
      "(0.5918367346938775, 0.3972602739726027, 0.47540983606557374, 0.4299236121530034, 0.7995797395706177)\n",
      "(0.6041666666666666, 0.3972602739726027, 0.4793388429752066, 0.43623710856590525, 0.7995799779891968)\n",
      "(0.5957446808510638, 0.3835616438356164, 0.4666666666666667, 0.4237884678175238, 0.7995803952217102)\n",
      "(0.5869565217391305, 0.3698630136986301, 0.453781512605042, 0.41115151968174446, 0.799659252166748)\n",
      "(0.6, 0.3698630136986301, 0.45762711864406774, 0.4176390319369136, 0.8004468083381653)\n",
      "(0.5909090909090909, 0.3561643835616438, 0.4444444444444445, 0.404798346803754, 0.8010056614875793)\n",
      "(0.5813953488372093, 0.3424657534246575, 0.43103448275862066, 0.39174699518237865, 0.8020696043968201)\n",
      "(0.5714285714285714, 0.3287671232876712, 0.41739130434782606, 0.3784740365125331, 0.8085611462593079)\n",
      "(0.5853658536585366, 0.3287671232876712, 0.42105263157894735, 0.3851389490657178, 0.8114547729492188)\n",
      "(0.575, 0.3150684931506849, 0.4070796460176991, 0.3716191302286109, 0.8154312372207642)\n",
      "(0.5897435897435898, 0.3150684931506849, 0.4107142857142857, 0.37849434427245765, 0.8156155347824097)\n",
      "(0.5789473684210527, 0.3013698630136986, 0.39639639639639634, 0.3647103399453675, 0.8161847591400146)\n",
      "(0.5675675675675675, 0.2876712328767123, 0.3818181818181817, 0.35065614226542297, 0.8162030577659607)\n",
      "(0.5833333333333334, 0.2876712328767123, 0.3853211009174312, 0.3577480895454716, 0.8162057399749756)\n",
      "(0.6, 0.2876712328767123, 0.3888888888888889, 0.3651061490130469, 0.8222827315330505)\n",
      "(0.5882352941176471, 0.273972602739726, 0.37383177570093457, 0.35073380080321387, 0.824730634689331)\n",
      "(0.5757575757575758, 0.2602739726027397, 0.3584905660377358, 0.33604430362909005, 0.8259192705154419)\n",
      "(0.59375, 0.2602739726027397, 0.3619047619047619, 0.34367030939302357, 0.8278613090515137)\n",
      "(0.6129032258064516, 0.2602739726027397, 0.36538461538461536, 0.3516197615442867, 0.8333424925804138)\n",
      "(0.6333333333333333, 0.2602739726027397, 0.36893203883495146, 0.35991890306534524, 0.8337457180023193)\n",
      "(0.6206896551724138, 0.2465753424657534, 0.35294117647058826, 0.3448618163481686, 0.8345327377319336)\n",
      "(0.6071428571428571, 0.2328767123287671, 0.33663366336633666, 0.32941799963625634, 0.8347922563552856)\n",
      "(0.5925925925925926, 0.2191780821917808, 0.32, 0.313557339948705, 0.8368188142776489)\n",
      "(0.6153846153846154, 0.2191780821917808, 0.3232323232323232, 0.3222488447007484, 0.8368812799453735)\n",
      "(0.6, 0.2054794520547945, 0.3061224489795918, 0.30592228802269406, 0.8392333984375)\n",
      "(0.625, 0.2054794520547945, 0.30927835051546393, 0.315073099429182, 0.8409807682037354)\n",
      "(0.6086956521739131, 0.1917808219178082, 0.29166666666666663, 0.2982331239975658, 0.846588134765625)\n",
      "(0.5909090909090909, 0.1780821917808219, 0.2736842105263158, 0.2808298609337591, 0.8532367944717407)\n",
      "(0.5714285714285714, 0.1643835616438356, 0.2553191489361702, 0.26280741511945294, 0.8537203073501587)\n",
      "(0.55, 0.1506849315068493, 0.23655913978494625, 0.24410109457453164, 0.8537205457687378)\n",
      "(0.5263157894736842, 0.136986301369863, 0.21739130434782608, 0.2246354665754645, 0.8537259101867676)\n",
      "(0.5, 0.1232876712328767, 0.19780219780219777, 0.20432184952152485, 0.8538134694099426)\n",
      "(0.5294117647058824, 0.1232876712328767, 0.2, 0.21374026859538542, 0.8568501472473145)\n",
      "(0.5, 0.1095890410958904, 0.1797752808988764, 0.19231190982771443, 0.8622720241546631)\n",
      "(0.5333333333333333, 0.1095890410958904, 0.18181818181818182, 0.2023546545849771, 0.8652635812759399)\n",
      "(0.5714285714285714, 0.1095890410958904, 0.1839080459770115, 0.21331750855352405, 0.8660643100738525)\n",
      "(0.5384615384615384, 0.0958904109589041, 0.1627906976744186, 0.19039759616679794, 0.8697233200073242)\n",
      "(0.5833333333333334, 0.0958904109589041, 0.16470588235294117, 0.2023587788935551, 0.872649073600769)\n",
      "(0.5454545454545454, 0.0821917808219178, 0.14285714285714285, 0.17776731533644813, 0.898523211479187)\n",
      "(0.5, 0.0684931506849315, 0.12048192771084336, 0.15127382447685542, 0.9055663347244263)\n",
      "(0.5555555555555556, 0.0684931506849315, 0.1219512195121951, 0.1643371061004847, 0.9058200120925903)\n",
      "(0.625, 0.0684931506849315, 0.12345679012345677, 0.17947388347530574, 0.905823826789856)\n",
      "(0.7142857142857143, 0.0684931506849315, 0.125, 0.19738150309845418, 0.9058247208595276)\n",
      "(0.6666666666666666, 0.0547945205479452, 0.10126582278481013, 0.16796600999030734, 0.9105516672134399)\n",
      "(0.6, 0.0410958904109589, 0.07692307692307691, 0.13453223656299196, 0.9157296419143677)\n",
      "(0.75, 0.0410958904109589, 0.07792207792207792, 0.15777618649678907, 0.9169957637786865)\n",
      "(1.0, 0.0410958904109589, 0.07894736842105263, 0.1906742270845093, 0.9172015190124512)\n",
      "(1.0, 0.0273972602739726, 0.05333333333333332, 0.15555677149019512, 0.9172077178955078)\n",
      "(1.0, 0.0136986301369863, 0.027027027027027025, 0.10990490279208061, 0.9172743558883667)\n",
      "(1.0, 0.0, 0.0, 0.0, 1.0)\n"
     ]
    }
   ],
   "source": [
    "for i in zip(precision,recall,f1,mccs,np.append(thresholds,1)):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-06T19:15:05.178430Z",
     "start_time": "2019-12-06T19:15:05.169778Z"
    }
   },
   "source": [
    "## Compare Predictions with Actually Toxic Compounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T09:20:03.395375Z",
     "start_time": "2019-12-09T09:20:03.380869Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max F1 Decision Threshold: 0.7035\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NR.AhR</th>\n",
       "      <th>DNN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>NCGC00261776-01</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>NCGC00261662-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>NCGC00261119-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>NCGC00260831-01</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>NCGC00261395-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>NCGC00357175-01</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>NCGC00356994-01</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>NCGC00357111-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>NCGC00357249-01</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>NCGC00357051-01</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>73 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 NR.AhR  DNN\n",
       "NCGC00261776-01       1    1\n",
       "NCGC00261662-01       1    0\n",
       "NCGC00261119-01       1    0\n",
       "NCGC00260831-01       1    1\n",
       "NCGC00261395-01       1    0\n",
       "...                 ...  ...\n",
       "NCGC00357175-01       1    1\n",
       "NCGC00356994-01       1    1\n",
       "NCGC00357111-01       1    0\n",
       "NCGC00357249-01       1    1\n",
       "NCGC00357051-01       1    1\n",
       "\n",
       "[73 rows x 2 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Max F1 Decision Threshold: %0.4f' % m_thresh)\n",
    "y_hat_testing_adj=adjusted_classes(p_te,m_thresh)\n",
    "y_hat=pd.DataFrame(y_hat_testing_adj,columns=['DNN'],index=y_te[rows_te].index)\n",
    "compare_TP = pd.concat([y_te[target][rows_te].astype('int'), y_hat],axis=1)\n",
    "compare_TP[compare_TP[target]==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:metis] *",
   "language": "python",
   "name": "conda-env-metis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
