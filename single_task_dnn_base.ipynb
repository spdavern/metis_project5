{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-12T03:55:13.459982Z",
     "start_time": "2019-12-12T03:55:13.455298Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import io\n",
    "# from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, f1_score,\\\n",
    "                            precision_score, recall_score, accuracy_score,\\\n",
    "                            average_precision_score, precision_recall_curve,\\\n",
    "                            matthews_corrcoef, roc_curve\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from src.helper_functions import load_data, get_model_perfs, init_model_perfs,\\\n",
    "                                 save_model, save_model_perfs, check_is_best,\\\n",
    "                                 read_model, evaluate_model_predictions,\\\n",
    "                                 update_model_perfs, check_and_save,\\\n",
    "                                 adjusted_classes, mcc\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-12T03:55:14.364441Z",
     "start_time": "2019-12-12T03:55:14.358637Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.5'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "from keras.callbacks import Callback\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-12T03:55:29.822979Z",
     "start_time": "2019-12-12T03:55:18.409278Z"
    }
   },
   "outputs": [],
   "source": [
    "x_tr, y_tr, x_te, y_te = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Build Neural Network\n",
    "\n",
    "Following the desciption in section 2.2.4 of the [DeepTox article](https://www.frontiersin.org/articles/10.3389/fenvs.2015.00080/full), I tried to use intermediate values in [Table 2](https://www.frontiersin.org/articles/10.3389/fenvs.2015.00080/full#T2) to build the neural network:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Following [this question/answer](https://datascience.stackexchange.com/questions/45165/how-to-get-accuracy-f1-precision-and-recall-for-a-keras-model) and [this question/answer](https://stackoverflow.com/questions/54065733/how-to-employ-the-scikit-learn-evaluation-metrics-functions-with-keras-in-python) to implement usage of recall in model training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T00:31:25.480771Z",
     "start_time": "2019-12-03T00:31:25.327888Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/sean/anaconda3/envs/metis/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Users/sean/anaconda3/envs/metis/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "h0_sigmoid_activation (Dense (None, 1024)              1684480   \n",
      "_________________________________________________________________\n",
      "Dropout0 (Dropout)           (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "h1_sigmoid_activation (Dense (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "Dropout1 (Dropout)           (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "h2_sigmoid_activation (Dense (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "Dropout2 (Dropout)           (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 1025      \n",
      "=================================================================\n",
      "Total params: 3,784,705\n",
      "Trainable params: 3,784,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "drop_out=0.5    # DeepTox range: 0.5, 0.2, 0\n",
    "L2_reg = 0.0001 # Default = 0.01\n",
    "layers = 3      # DeepTox range: 1, 2, 3, 4\n",
    "act = 'sigmoid' # Consider sigmoid and tanh\n",
    "neurons = 1024  # DeepTox range: 1024, 2048, 4096, 8192, 16384\n",
    "# Info on decay: https://datascience.stackexchange.com/questions/26112/decay-parameter-in-keras-optimizers\n",
    "decay = 0       # DeepTox range: 10^-4, 10^-5, 10^-6\n",
    "learn_rate = 0.1  #Research appropriate range\n",
    "DNN = keras.Sequential()\n",
    "DNN.add(Dense(neurons, activation=act,input_shape=x_tr.shape[1:],name='h0_'+act+'_activation'))\n",
    "DNN.add(Dropout(rate=drop_out,name='Dropout0'))\n",
    "for i in range(1,layers):\n",
    "    DNN.add(Dense(units=neurons, activation=act,\\\n",
    "                  name='h'+str(i)+'_'+act+'_activation',\\\n",
    "                  kernel_regularizer=keras.regularizers.l2(L2_reg)))\n",
    "    DNN.add(Dropout(rate=drop_out,name='Dropout'+str(i)))\n",
    "DNN.add(Dense(units=1, activation='sigmoid'))\n",
    "keras.optimizers.Adam(lr=learn_rate, beta_1=0.9,\\\n",
    "                      beta_2=0.999, decay=decay, amsgrad=False)\n",
    "DNN.compile(optimizer='adam', loss='binary_crossentropy',\\\n",
    "            metrics=['accuracy'])\n",
    "DNN.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Train DNN's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T00:45:37.293279Z",
     "start_time": "2019-12-03T00:31:53.696343Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/sean/anaconda3/envs/metis/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 11934 samples, validate on 1689 samples\n",
      "Epoch 1/100\n",
      "11934/11934 [==============================] - 3s 227us/step - loss: 1.1133 - acc: 0.5215 - val_loss: 0.7070 - val_acc: 0.8840\n",
      "Epoch 2/100\n",
      "11934/11934 [==============================] - 2s 175us/step - loss: 0.8746 - acc: 0.5747 - val_loss: 0.8319 - val_acc: 0.4938\n",
      "Epoch 3/100\n",
      "11934/11934 [==============================] - 2s 181us/step - loss: 0.7788 - acc: 0.6629 - val_loss: 0.7846 - val_acc: 0.5595\n",
      "Epoch 4/100\n",
      "11934/11934 [==============================] - 3s 210us/step - loss: 0.7400 - acc: 0.6865 - val_loss: 0.6602 - val_acc: 0.7105\n",
      "Epoch 5/100\n",
      "11934/11934 [==============================] - 3s 210us/step - loss: 0.7156 - acc: 0.6886 - val_loss: 0.5235 - val_acc: 0.8502\n",
      "Epoch 6/100\n",
      "11934/11934 [==============================] - 2s 200us/step - loss: 0.6984 - acc: 0.7056 - val_loss: 0.8266 - val_acc: 0.5317\n",
      "Epoch 7/100\n",
      "11934/11934 [==============================] - 2s 201us/step - loss: 0.6794 - acc: 0.7106 - val_loss: 0.5944 - val_acc: 0.7821\n",
      "Epoch 8/100\n",
      "11934/11934 [==============================] - 2s 201us/step - loss: 0.6682 - acc: 0.7123 - val_loss: 0.6837 - val_acc: 0.6092\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 9/100\n",
      "11934/11934 [==============================] - 2s 203us/step - loss: 0.6513 - acc: 0.7200 - val_loss: 0.7557 - val_acc: 0.5921\n",
      "Epoch 10/100\n",
      "11934/11934 [==============================] - 2s 205us/step - loss: 0.6357 - acc: 0.7319 - val_loss: 0.6479 - val_acc: 0.6750\n",
      "Epoch 11/100\n",
      "11934/11934 [==============================] - 2s 205us/step - loss: 0.6234 - acc: 0.7360 - val_loss: 0.5659 - val_acc: 0.8040\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 12/100\n",
      "11934/11934 [==============================] - 2s 205us/step - loss: 0.6146 - acc: 0.7387 - val_loss: 0.5735 - val_acc: 0.7916\n",
      "Epoch 13/100\n",
      "11934/11934 [==============================] - 2s 203us/step - loss: 0.6016 - acc: 0.7479 - val_loss: 0.6328 - val_acc: 0.6809\n",
      "Epoch 14/100\n",
      "11934/11934 [==============================] - 2s 206us/step - loss: 0.5965 - acc: 0.7479 - val_loss: 0.5107 - val_acc: 0.8378\n",
      "Epoch 15/100\n",
      "11934/11934 [==============================] - 3s 214us/step - loss: 0.5918 - acc: 0.7517 - val_loss: 0.5822 - val_acc: 0.7762\n",
      "Epoch 16/100\n",
      "11934/11934 [==============================] - 3s 210us/step - loss: 0.5871 - acc: 0.7556 - val_loss: 0.6022 - val_acc: 0.7010\n",
      "Epoch 17/100\n",
      "11934/11934 [==============================] - 2s 208us/step - loss: 0.5775 - acc: 0.7600 - val_loss: 0.5301 - val_acc: 0.8188\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 18/100\n",
      "11934/11934 [==============================] - 2s 207us/step - loss: 0.5706 - acc: 0.7588 - val_loss: 0.5502 - val_acc: 0.7833\n",
      "Epoch 19/100\n",
      "11934/11934 [==============================] - 3s 211us/step - loss: 0.5669 - acc: 0.7599 - val_loss: 0.5472 - val_acc: 0.7709\n",
      "Epoch 20/100\n",
      "11934/11934 [==============================] - 3s 212us/step - loss: 0.5605 - acc: 0.7637 - val_loss: 0.5162 - val_acc: 0.8265\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 21/100\n",
      "11934/11934 [==============================] - 2s 209us/step - loss: 0.5540 - acc: 0.7702 - val_loss: 0.5447 - val_acc: 0.7910\n",
      "Epoch 22/100\n",
      "11934/11934 [==============================] - 3s 215us/step - loss: 0.5547 - acc: 0.7665 - val_loss: 0.5377 - val_acc: 0.7845\n",
      "Epoch 23/100\n",
      "11934/11934 [==============================] - 3s 215us/step - loss: 0.5521 - acc: 0.7691 - val_loss: 0.5375 - val_acc: 0.7904\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 24/100\n",
      "11934/11934 [==============================] - 3s 210us/step - loss: 0.5515 - acc: 0.7733 - val_loss: 0.5496 - val_acc: 0.7815\n",
      "Epoch 25/100\n",
      "11934/11934 [==============================] - 3s 212us/step - loss: 0.5456 - acc: 0.7753 - val_loss: 0.5423 - val_acc: 0.7975\n",
      "Epoch 26/100\n",
      "11934/11934 [==============================] - 3s 210us/step - loss: 0.5473 - acc: 0.7756 - val_loss: 0.5340 - val_acc: 0.8011\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "Epoch 27/100\n",
      "11934/11934 [==============================] - 3s 214us/step - loss: 0.5476 - acc: 0.7745 - val_loss: 0.5473 - val_acc: 0.7869\n",
      "Epoch 28/100\n",
      "11934/11934 [==============================] - 3s 215us/step - loss: 0.5416 - acc: 0.7775 - val_loss: 0.5436 - val_acc: 0.7946\n",
      "Epoch 29/100\n",
      "11934/11934 [==============================] - 2s 208us/step - loss: 0.5424 - acc: 0.7764 - val_loss: 0.5403 - val_acc: 0.8005\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "Epoch 30/100\n",
      "11934/11934 [==============================] - 2s 205us/step - loss: 0.5402 - acc: 0.7745 - val_loss: 0.5377 - val_acc: 0.8034\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00030: early stopping\n",
      "         NR.AhR: 0.86472\n",
      "Model saved and metrics table updated.\n",
      "Model saved and metrics table updated.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>threshold</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>auc_roc</th>\n",
       "      <th>avg_precision</th>\n",
       "      <th>confusion_matrix</th>\n",
       "      <th>model_filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.911475</td>\n",
       "      <td>0.756757</td>\n",
       "      <td>0.383562</td>\n",
       "      <td>0.509091</td>\n",
       "      <td>0.905028</td>\n",
       "      <td>0.602102</td>\n",
       "      <td>[[528, 9], [45, 28]]</td>\n",
       "      <td>RF0.joblib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>RF_modT</td>\n",
       "      <td>0.235000</td>\n",
       "      <td>0.842623</td>\n",
       "      <td>0.424837</td>\n",
       "      <td>0.890411</td>\n",
       "      <td>0.575221</td>\n",
       "      <td>0.905028</td>\n",
       "      <td>0.602102</td>\n",
       "      <td>[[449, 88], [8, 65]]</td>\n",
       "      <td>RF_modT0.joblib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>DNN</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.814754</td>\n",
       "      <td>0.364865</td>\n",
       "      <td>0.739726</td>\n",
       "      <td>0.488688</td>\n",
       "      <td>0.864723</td>\n",
       "      <td>0.524576</td>\n",
       "      <td>[[443, 94], [19, 54]]</td>\n",
       "      <td>DNN0.h5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>DNN_modT</td>\n",
       "      <td>0.773516</td>\n",
       "      <td>0.901639</td>\n",
       "      <td>0.603175</td>\n",
       "      <td>0.520548</td>\n",
       "      <td>0.558824</td>\n",
       "      <td>0.864723</td>\n",
       "      <td>0.524576</td>\n",
       "      <td>[[512, 25], [35, 38]]</td>\n",
       "      <td>DNN_modT0.h5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      model  threshold  accuracy  precision    recall        f1   auc_roc  \\\n",
       "0        RF   0.500000  0.911475   0.756757  0.383562  0.509091  0.905028   \n",
       "1   RF_modT   0.235000  0.842623   0.424837  0.890411  0.575221  0.905028   \n",
       "2       DNN   0.500000  0.814754   0.364865  0.739726  0.488688  0.864723   \n",
       "3  DNN_modT   0.773516  0.901639   0.603175  0.520548  0.558824  0.864723   \n",
       "\n",
       "   avg_precision       confusion_matrix   model_filename  \n",
       "0       0.602102   [[528, 9], [45, 28]]       RF0.joblib  \n",
       "1       0.602102   [[449, 88], [8, 65]]  RF_modT0.joblib  \n",
       "2       0.524576  [[443, 94], [19, 54]]          DNN0.h5  \n",
       "3       0.524576  [[512, 25], [35, 38]]     DNN_modT0.h5  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14834 samples, validate on 1931 samples\n",
      "Epoch 1/100\n",
      "14834/14834 [==============================] - 3s 221us/step - loss: 1.0468 - acc: 0.4847 - val_loss: 0.9818 - val_acc: 0.4443\n",
      "Epoch 2/100\n",
      "14834/14834 [==============================] - 3s 221us/step - loss: 0.9139 - acc: 0.5450 - val_loss: 0.9631 - val_acc: 0.4459\n",
      "Epoch 3/100\n",
      "14834/14834 [==============================] - 3s 215us/step - loss: 0.8575 - acc: 0.5810 - val_loss: 0.8899 - val_acc: 0.4858\n",
      "Epoch 4/100\n",
      "14834/14834 [==============================] - 3s 217us/step - loss: 0.8338 - acc: 0.5866 - val_loss: 0.8453 - val_acc: 0.5049\n",
      "Epoch 5/100\n",
      "14834/14834 [==============================] - 3s 217us/step - loss: 0.8115 - acc: 0.6066 - val_loss: 0.8269 - val_acc: 0.5308\n",
      "Epoch 6/100\n",
      "14834/14834 [==============================] - 3s 209us/step - loss: 0.7909 - acc: 0.6201 - val_loss: 0.8087 - val_acc: 0.5526\n",
      "Epoch 7/100\n",
      "14834/14834 [==============================] - 3s 212us/step - loss: 0.7809 - acc: 0.6283 - val_loss: 0.7948 - val_acc: 0.5769\n",
      "Epoch 8/100\n",
      "14834/14834 [==============================] - 3s 207us/step - loss: 0.7699 - acc: 0.6393 - val_loss: 0.7865 - val_acc: 0.6100\n",
      "Epoch 9/100\n",
      "14834/14834 [==============================] - 3s 209us/step - loss: 0.7588 - acc: 0.6507 - val_loss: 0.7722 - val_acc: 0.6277\n",
      "Epoch 10/100\n",
      "14834/14834 [==============================] - 3s 212us/step - loss: 0.7521 - acc: 0.6536 - val_loss: 0.7701 - val_acc: 0.6282\n",
      "Epoch 11/100\n",
      "14834/14834 [==============================] - 3s 209us/step - loss: 0.7520 - acc: 0.6553 - val_loss: 0.7625 - val_acc: 0.6308\n",
      "Epoch 12/100\n",
      "14834/14834 [==============================] - 3s 207us/step - loss: 0.7426 - acc: 0.6642 - val_loss: 0.7514 - val_acc: 0.6349\n",
      "Epoch 13/100\n",
      "14834/14834 [==============================] - 3s 208us/step - loss: 0.7341 - acc: 0.6741 - val_loss: 0.7471 - val_acc: 0.6359\n",
      "Epoch 14/100\n",
      "14834/14834 [==============================] - 3s 211us/step - loss: 0.7322 - acc: 0.6757 - val_loss: 0.7426 - val_acc: 0.6401\n",
      "Epoch 15/100\n",
      "14834/14834 [==============================] - 3s 205us/step - loss: 0.7234 - acc: 0.6853 - val_loss: 0.7338 - val_acc: 0.6432\n",
      "Epoch 16/100\n",
      "14834/14834 [==============================] - 3s 203us/step - loss: 0.7211 - acc: 0.6891 - val_loss: 0.7311 - val_acc: 0.6432\n",
      "Epoch 17/100\n",
      "14834/14834 [==============================] - 3s 208us/step - loss: 0.7185 - acc: 0.6904 - val_loss: 0.7301 - val_acc: 0.6484\n",
      "Epoch 18/100\n",
      "14834/14834 [==============================] - 3s 204us/step - loss: 0.7134 - acc: 0.6927 - val_loss: 0.7207 - val_acc: 0.6530\n",
      "Epoch 19/100\n",
      "14834/14834 [==============================] - 3s 204us/step - loss: 0.7119 - acc: 0.6976 - val_loss: 0.7194 - val_acc: 0.6546\n",
      "Epoch 20/100\n",
      "14834/14834 [==============================] - 3s 205us/step - loss: 0.7103 - acc: 0.6972 - val_loss: 0.7152 - val_acc: 0.6587\n",
      "Epoch 21/100\n",
      "14834/14834 [==============================] - 3s 201us/step - loss: 0.7053 - acc: 0.7008 - val_loss: 0.7118 - val_acc: 0.6598\n",
      "Epoch 22/100\n",
      "14834/14834 [==============================] - 3s 219us/step - loss: 0.7015 - acc: 0.7063 - val_loss: 0.7116 - val_acc: 0.6608\n",
      "Epoch 23/100\n",
      "14834/14834 [==============================] - 3s 205us/step - loss: 0.7019 - acc: 0.7053 - val_loss: 0.7125 - val_acc: 0.6613\n",
      "Epoch 24/100\n",
      "14834/14834 [==============================] - 3s 212us/step - loss: 0.6991 - acc: 0.7087 - val_loss: 0.7069 - val_acc: 0.6639\n",
      "Epoch 25/100\n",
      "14834/14834 [==============================] - 3s 202us/step - loss: 0.6962 - acc: 0.7134 - val_loss: 0.7000 - val_acc: 0.6665\n",
      "Epoch 26/100\n",
      "14834/14834 [==============================] - 3s 204us/step - loss: 0.6957 - acc: 0.7093 - val_loss: 0.7000 - val_acc: 0.6655\n",
      "Epoch 27/100\n",
      "14834/14834 [==============================] - 3s 200us/step - loss: 0.6924 - acc: 0.7108 - val_loss: 0.6999 - val_acc: 0.6649\n",
      "Epoch 28/100\n",
      "14834/14834 [==============================] - 3s 207us/step - loss: 0.6893 - acc: 0.7152 - val_loss: 0.6927 - val_acc: 0.6696\n",
      "Epoch 29/100\n",
      "14834/14834 [==============================] - 3s 212us/step - loss: 0.6886 - acc: 0.7161 - val_loss: 0.6924 - val_acc: 0.6706\n",
      "Epoch 30/100\n",
      "14834/14834 [==============================] - 3s 203us/step - loss: 0.6873 - acc: 0.7136 - val_loss: 0.6918 - val_acc: 0.6712\n",
      "Epoch 31/100\n",
      "14834/14834 [==============================] - 3s 202us/step - loss: 0.6852 - acc: 0.7179 - val_loss: 0.6865 - val_acc: 0.6743\n",
      "Epoch 32/100\n",
      "14834/14834 [==============================] - 3s 201us/step - loss: 0.6816 - acc: 0.7201 - val_loss: 0.6897 - val_acc: 0.6727\n",
      "Epoch 33/100\n",
      "14834/14834 [==============================] - 3s 211us/step - loss: 0.6827 - acc: 0.7188 - val_loss: 0.6831 - val_acc: 0.6774\n",
      "Epoch 34/100\n",
      "14834/14834 [==============================] - 3s 218us/step - loss: 0.6815 - acc: 0.7245 - val_loss: 0.6882 - val_acc: 0.6753\n",
      "Epoch 35/100\n",
      "14834/14834 [==============================] - 3s 200us/step - loss: 0.6784 - acc: 0.7219 - val_loss: 0.6830 - val_acc: 0.6789\n",
      "Epoch 36/100\n",
      "14834/14834 [==============================] - 3s 205us/step - loss: 0.6777 - acc: 0.7226 - val_loss: 0.6808 - val_acc: 0.6800\n",
      "Epoch 37/100\n",
      "14834/14834 [==============================] - 3s 201us/step - loss: 0.6747 - acc: 0.7241 - val_loss: 0.6827 - val_acc: 0.6794\n",
      "Epoch 38/100\n",
      "14834/14834 [==============================] - 3s 202us/step - loss: 0.6711 - acc: 0.7267 - val_loss: 0.6767 - val_acc: 0.6815\n",
      "Epoch 39/100\n",
      "14834/14834 [==============================] - 3s 211us/step - loss: 0.6739 - acc: 0.7264 - val_loss: 0.6728 - val_acc: 0.6836\n",
      "Epoch 40/100\n",
      "14834/14834 [==============================] - 3s 218us/step - loss: 0.6725 - acc: 0.7226 - val_loss: 0.6704 - val_acc: 0.6846\n",
      "Epoch 41/100\n",
      "14834/14834 [==============================] - 3s 221us/step - loss: 0.6710 - acc: 0.7265 - val_loss: 0.6765 - val_acc: 0.6831\n",
      "Epoch 42/100\n",
      "14834/14834 [==============================] - 3s 209us/step - loss: 0.6683 - acc: 0.7299 - val_loss: 0.6699 - val_acc: 0.6882\n",
      "Epoch 43/100\n",
      "14834/14834 [==============================] - 3s 214us/step - loss: 0.6661 - acc: 0.7270 - val_loss: 0.6664 - val_acc: 0.6898\n",
      "Epoch 44/100\n",
      "14834/14834 [==============================] - 3s 200us/step - loss: 0.6609 - acc: 0.7363 - val_loss: 0.6657 - val_acc: 0.6919\n",
      "Epoch 45/100\n",
      "14834/14834 [==============================] - 3s 206us/step - loss: 0.6661 - acc: 0.7308 - val_loss: 0.6673 - val_acc: 0.6914\n",
      "Epoch 46/100\n",
      "14834/14834 [==============================] - 3s 205us/step - loss: 0.6634 - acc: 0.7304 - val_loss: 0.6627 - val_acc: 0.6929\n",
      "Epoch 47/100\n",
      "14834/14834 [==============================] - 3s 202us/step - loss: 0.6588 - acc: 0.7340 - val_loss: 0.6579 - val_acc: 0.6950\n",
      "Epoch 48/100\n",
      "14834/14834 [==============================] - 3s 207us/step - loss: 0.6595 - acc: 0.7340 - val_loss: 0.6645 - val_acc: 0.6929\n",
      "Epoch 49/100\n",
      "14834/14834 [==============================] - 3s 211us/step - loss: 0.6568 - acc: 0.7337 - val_loss: 0.6578 - val_acc: 0.6965\n",
      "Epoch 50/100\n",
      "14834/14834 [==============================] - 3s 207us/step - loss: 0.6545 - acc: 0.7350 - val_loss: 0.6500 - val_acc: 0.6996\n",
      "Epoch 51/100\n",
      "14834/14834 [==============================] - 3s 206us/step - loss: 0.6552 - acc: 0.7355 - val_loss: 0.6539 - val_acc: 0.6981\n",
      "Epoch 52/100\n",
      "14834/14834 [==============================] - 3s 206us/step - loss: 0.6546 - acc: 0.7355 - val_loss: 0.6441 - val_acc: 0.7033\n",
      "Epoch 53/100\n",
      "14834/14834 [==============================] - 3s 204us/step - loss: 0.6525 - acc: 0.7329 - val_loss: 0.6566 - val_acc: 0.6991\n",
      "Epoch 54/100\n",
      "14834/14834 [==============================] - 3s 201us/step - loss: 0.6486 - acc: 0.7361 - val_loss: 0.6474 - val_acc: 0.7027\n",
      "Epoch 55/100\n",
      "14834/14834 [==============================] - 3s 213us/step - loss: 0.6481 - acc: 0.7407 - val_loss: 0.6474 - val_acc: 0.7043\n",
      "\n",
      "Epoch 00055: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "Epoch 56/100\n",
      "14834/14834 [==============================] - 3s 202us/step - loss: 0.6482 - acc: 0.7383 - val_loss: 0.6472 - val_acc: 0.7038\n",
      "Epoch 57/100\n",
      "14834/14834 [==============================] - 3s 204us/step - loss: 0.6509 - acc: 0.7384 - val_loss: 0.6416 - val_acc: 0.7074\n",
      "Epoch 58/100\n",
      "14834/14834 [==============================] - 3s 202us/step - loss: 0.6473 - acc: 0.7371 - val_loss: 0.6424 - val_acc: 0.7090\n",
      "Epoch 59/100\n",
      "14834/14834 [==============================] - 3s 220us/step - loss: 0.6479 - acc: 0.7403 - val_loss: 0.6392 - val_acc: 0.7095\n",
      "Epoch 60/100\n",
      "14834/14834 [==============================] - 3s 203us/step - loss: 0.6470 - acc: 0.7397 - val_loss: 0.6413 - val_acc: 0.7084\n",
      "Epoch 61/100\n",
      "14834/14834 [==============================] - 3s 203us/step - loss: 0.6461 - acc: 0.7375 - val_loss: 0.6384 - val_acc: 0.7100\n",
      "Epoch 62/100\n",
      "14834/14834 [==============================] - 3s 205us/step - loss: 0.6441 - acc: 0.7381 - val_loss: 0.6373 - val_acc: 0.7100\n",
      "Epoch 63/100\n",
      "14834/14834 [==============================] - 3s 214us/step - loss: 0.6458 - acc: 0.7405 - val_loss: 0.6407 - val_acc: 0.7090\n",
      "Epoch 64/100\n",
      "14834/14834 [==============================] - 3s 209us/step - loss: 0.6437 - acc: 0.7392 - val_loss: 0.6404 - val_acc: 0.7084\n",
      "Epoch 65/100\n",
      "14834/14834 [==============================] - 3s 207us/step - loss: 0.6416 - acc: 0.7410 - val_loss: 0.6358 - val_acc: 0.7141\n",
      "Epoch 66/100\n",
      "14834/14834 [==============================] - 3s 208us/step - loss: 0.6403 - acc: 0.7414 - val_loss: 0.6347 - val_acc: 0.7147\n",
      "Epoch 67/100\n",
      "14834/14834 [==============================] - 3s 203us/step - loss: 0.6412 - acc: 0.7417 - val_loss: 0.6335 - val_acc: 0.7147\n",
      "Epoch 68/100\n",
      "14834/14834 [==============================] - 3s 203us/step - loss: 0.6412 - acc: 0.7392 - val_loss: 0.6373 - val_acc: 0.7136\n",
      "Epoch 69/100\n",
      "14834/14834 [==============================] - 3s 201us/step - loss: 0.6404 - acc: 0.7396 - val_loss: 0.6307 - val_acc: 0.7157\n",
      "Epoch 70/100\n",
      "14834/14834 [==============================] - 3s 206us/step - loss: 0.6370 - acc: 0.7425 - val_loss: 0.6339 - val_acc: 0.7141\n",
      "Epoch 71/100\n",
      "14834/14834 [==============================] - 3s 204us/step - loss: 0.6425 - acc: 0.7370 - val_loss: 0.6329 - val_acc: 0.7141\n",
      "Epoch 72/100\n",
      "14834/14834 [==============================] - 3s 207us/step - loss: 0.6358 - acc: 0.7437 - val_loss: 0.6319 - val_acc: 0.7162\n",
      "\n",
      "Epoch 00072: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "Epoch 73/100\n",
      "14834/14834 [==============================] - 3s 200us/step - loss: 0.6385 - acc: 0.7396 - val_loss: 0.6294 - val_acc: 0.7172\n",
      "Epoch 74/100\n",
      "14834/14834 [==============================] - 3s 204us/step - loss: 0.6360 - acc: 0.7432 - val_loss: 0.6295 - val_acc: 0.7172\n",
      "Epoch 75/100\n",
      "14834/14834 [==============================] - 3s 202us/step - loss: 0.6371 - acc: 0.7459 - val_loss: 0.6288 - val_acc: 0.7178\n",
      "Epoch 76/100\n",
      "14834/14834 [==============================] - 3s 210us/step - loss: 0.6363 - acc: 0.7440 - val_loss: 0.6299 - val_acc: 0.7178\n",
      "Epoch 77/100\n",
      "14834/14834 [==============================] - 3s 204us/step - loss: 0.6368 - acc: 0.7387 - val_loss: 0.6263 - val_acc: 0.7188\n",
      "Epoch 78/100\n",
      "14834/14834 [==============================] - 3s 205us/step - loss: 0.6379 - acc: 0.7449 - val_loss: 0.6283 - val_acc: 0.7183\n",
      "Epoch 79/100\n",
      "14834/14834 [==============================] - 3s 207us/step - loss: 0.6361 - acc: 0.7454 - val_loss: 0.6271 - val_acc: 0.7193\n",
      "Epoch 80/100\n",
      "14834/14834 [==============================] - 3s 207us/step - loss: 0.6364 - acc: 0.7444 - val_loss: 0.6230 - val_acc: 0.7214\n",
      "Epoch 81/100\n",
      "14834/14834 [==============================] - 3s 201us/step - loss: 0.6374 - acc: 0.7439 - val_loss: 0.6251 - val_acc: 0.7204\n",
      "Epoch 82/100\n",
      "14834/14834 [==============================] - 3s 205us/step - loss: 0.6359 - acc: 0.7435 - val_loss: 0.6254 - val_acc: 0.7198\n",
      "Epoch 83/100\n",
      "14834/14834 [==============================] - 3s 203us/step - loss: 0.6355 - acc: 0.7449 - val_loss: 0.6255 - val_acc: 0.7204\n",
      "\n",
      "Epoch 00083: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
      "Epoch 84/100\n",
      "14834/14834 [==============================] - 3s 204us/step - loss: 0.6337 - acc: 0.7447 - val_loss: 0.6267 - val_acc: 0.7193\n",
      "Epoch 85/100\n",
      "14834/14834 [==============================] - 3s 205us/step - loss: 0.6345 - acc: 0.7437 - val_loss: 0.6248 - val_acc: 0.7209\n",
      "Epoch 86/100\n",
      "14834/14834 [==============================] - 3s 206us/step - loss: 0.6341 - acc: 0.7436 - val_loss: 0.6255 - val_acc: 0.7204\n",
      "\n",
      "Epoch 00086: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
      "Epoch 87/100\n",
      "14834/14834 [==============================] - 3s 202us/step - loss: 0.6353 - acc: 0.7441 - val_loss: 0.6252 - val_acc: 0.7209\n",
      "Epoch 88/100\n",
      "14834/14834 [==============================] - 3s 208us/step - loss: 0.6361 - acc: 0.7421 - val_loss: 0.6250 - val_acc: 0.7214\n",
      "Epoch 89/100\n",
      "14834/14834 [==============================] - 3s 208us/step - loss: 0.6325 - acc: 0.7479 - val_loss: 0.6248 - val_acc: 0.7214\n",
      "\n",
      "Epoch 00089: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
      "Epoch 90/100\n",
      "14834/14834 [==============================] - 3s 204us/step - loss: 0.6316 - acc: 0.7439 - val_loss: 0.6247 - val_acc: 0.7214\n",
      "Epoch 91/100\n",
      "14834/14834 [==============================] - 3s 203us/step - loss: 0.6362 - acc: 0.7445 - val_loss: 0.6245 - val_acc: 0.7219\n",
      "Epoch 92/100\n",
      "14834/14834 [==============================] - 3s 210us/step - loss: 0.6372 - acc: 0.7421 - val_loss: 0.6245 - val_acc: 0.7219\n",
      "\n",
      "Epoch 00092: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.\n",
      "Epoch 93/100\n",
      "14834/14834 [==============================] - 3s 205us/step - loss: 0.6326 - acc: 0.7457 - val_loss: 0.6245 - val_acc: 0.7214\n",
      "Epoch 94/100\n",
      "14834/14834 [==============================] - 3s 207us/step - loss: 0.6328 - acc: 0.7454 - val_loss: 0.6247 - val_acc: 0.7214\n",
      "Epoch 95/100\n",
      "14834/14834 [==============================] - 3s 208us/step - loss: 0.6340 - acc: 0.7419 - val_loss: 0.6246 - val_acc: 0.7214\n",
      "\n",
      "Epoch 00095: ReduceLROnPlateau reducing learning rate to 6.103515914901436e-08.\n",
      "Epoch 96/100\n",
      "14834/14834 [==============================] - 3s 207us/step - loss: 0.6356 - acc: 0.7445 - val_loss: 0.6246 - val_acc: 0.7219\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00096: early stopping\n",
      "          NR.AR: 0.74151\n",
      "Model saved and metrics table updated.\n",
      "Model saved and metrics table updated.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>threshold</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>auc_roc</th>\n",
       "      <th>avg_precision</th>\n",
       "      <th>confusion_matrix</th>\n",
       "      <th>model_filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.981229</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.678934</td>\n",
       "      <td>0.232723</td>\n",
       "      <td>[[573, 1], [10, 2]]</td>\n",
       "      <td>RF0.joblib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>RF_modT</td>\n",
       "      <td>0.730000</td>\n",
       "      <td>0.982935</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.678934</td>\n",
       "      <td>0.232723</td>\n",
       "      <td>[[574, 0], [10, 2]]</td>\n",
       "      <td>RF_modT0.joblib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>DNN</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.721843</td>\n",
       "      <td>0.042424</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.079096</td>\n",
       "      <td>0.741507</td>\n",
       "      <td>0.168663</td>\n",
       "      <td>[[416, 158], [5, 7]]</td>\n",
       "      <td>DNN0.h5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>DNN_modT</td>\n",
       "      <td>0.792447</td>\n",
       "      <td>0.965870</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.741507</td>\n",
       "      <td>0.168663</td>\n",
       "      <td>[[562, 12], [8, 4]]</td>\n",
       "      <td>DNN_modT0.h5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      model  threshold  accuracy  precision    recall        f1   auc_roc  \\\n",
       "0        RF   0.500000  0.981229   0.666667  0.166667  0.266667  0.678934   \n",
       "1   RF_modT   0.730000  0.982935   1.000000  0.166667  0.285714  0.678934   \n",
       "2       DNN   0.500000  0.721843   0.042424  0.583333  0.079096  0.741507   \n",
       "3  DNN_modT   0.792447  0.965870   0.250000  0.333333  0.285714  0.741507   \n",
       "\n",
       "   avg_precision      confusion_matrix   model_filename  \n",
       "0       0.232723   [[573, 1], [10, 2]]       RF0.joblib  \n",
       "1       0.232723   [[574, 0], [10, 2]]  RF_modT0.joblib  \n",
       "2       0.168663  [[416, 158], [5, 7]]          DNN0.h5  \n",
       "3       0.168663   [[562, 12], [8, 4]]     DNN_modT0.h5  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13670 samples, validate on 1771 samples\n",
      "Epoch 1/100\n",
      "13670/13670 [==============================] - 3s 207us/step - loss: 0.6926 - acc: 0.7134 - val_loss: 0.5940 - val_acc: 0.7493\n",
      "Epoch 2/100\n",
      "13670/13670 [==============================] - 3s 214us/step - loss: 0.6916 - acc: 0.7143 - val_loss: 0.5947 - val_acc: 0.7493\n",
      "Epoch 3/100\n",
      "13670/13670 [==============================] - 3s 212us/step - loss: 0.6977 - acc: 0.7115 - val_loss: 0.5955 - val_acc: 0.7487\n",
      "Epoch 4/100\n",
      "13670/13670 [==============================] - 3s 211us/step - loss: 0.6946 - acc: 0.7108 - val_loss: 0.5961 - val_acc: 0.7487\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 3.051757957450718e-08.\n",
      "Epoch 5/100\n",
      "13670/13670 [==============================] - 3s 210us/step - loss: 0.6950 - acc: 0.7102 - val_loss: 0.5964 - val_acc: 0.7487\n",
      "Epoch 6/100\n",
      "13670/13670 [==============================] - 3s 212us/step - loss: 0.6970 - acc: 0.7123 - val_loss: 0.5967 - val_acc: 0.7487\n",
      "Epoch 7/100\n",
      "13670/13670 [==============================] - 3s 208us/step - loss: 0.6924 - acc: 0.7127 - val_loss: 0.5971 - val_acc: 0.7487\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 1.525878978725359e-08.\n",
      "Epoch 8/100\n",
      "13670/13670 [==============================] - 3s 214us/step - loss: 0.6927 - acc: 0.7119 - val_loss: 0.5972 - val_acc: 0.7487\n",
      "Epoch 9/100\n",
      "13670/13670 [==============================] - 3s 212us/step - loss: 0.6910 - acc: 0.7113 - val_loss: 0.5974 - val_acc: 0.7487\n",
      "Epoch 10/100\n",
      "13670/13670 [==============================] - 3s 212us/step - loss: 0.6926 - acc: 0.7102 - val_loss: 0.5976 - val_acc: 0.7487\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 7.629394893626795e-09.\n",
      "Epoch 11/100\n",
      "13670/13670 [==============================] - 3s 209us/step - loss: 0.6916 - acc: 0.7091 - val_loss: 0.5976 - val_acc: 0.7487\n",
      "Epoch 12/100\n",
      "13670/13670 [==============================] - 3s 208us/step - loss: 0.6926 - acc: 0.7125 - val_loss: 0.5977 - val_acc: 0.7487\n",
      "Epoch 13/100\n",
      "13670/13670 [==============================] - 3s 204us/step - loss: 0.6938 - acc: 0.7119 - val_loss: 0.5978 - val_acc: 0.7487\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 3.814697446813398e-09.\n",
      "Epoch 14/100\n",
      "13670/13670 [==============================] - 3s 206us/step - loss: 0.6929 - acc: 0.7125 - val_loss: 0.5978 - val_acc: 0.7487\n",
      "Epoch 15/100\n",
      "13670/13670 [==============================] - 3s 212us/step - loss: 0.6896 - acc: 0.7121 - val_loss: 0.5979 - val_acc: 0.7487\n",
      "Epoch 16/100\n",
      "13670/13670 [==============================] - 3s 209us/step - loss: 0.6905 - acc: 0.7138 - val_loss: 0.5979 - val_acc: 0.7487\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.907348723406699e-09.\n",
      "Epoch 17/100\n",
      "13670/13670 [==============================] - 3s 207us/step - loss: 0.6933 - acc: 0.7110 - val_loss: 0.5979 - val_acc: 0.7487\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00017: early stopping\n",
      "      NR.AR.LBD: 0.63513\n",
      "Model saved and metrics table updated.\n",
      "Model saved and metrics table updated.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>threshold</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>auc_roc</th>\n",
       "      <th>avg_precision</th>\n",
       "      <th>confusion_matrix</th>\n",
       "      <th>model_filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.982818</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.763284</td>\n",
       "      <td>0.060344</td>\n",
       "      <td>[[572, 2], [8, 0]]</td>\n",
       "      <td>RF0.joblib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>RF_modT</td>\n",
       "      <td>0.270000</td>\n",
       "      <td>0.979381</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.763284</td>\n",
       "      <td>0.060344</td>\n",
       "      <td>[[569, 5], [7, 1]]</td>\n",
       "      <td>RF_modT0.joblib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>DNN</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.738832</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.037975</td>\n",
       "      <td>0.635126</td>\n",
       "      <td>0.030153</td>\n",
       "      <td>[[427, 147], [5, 3]]</td>\n",
       "      <td>DNN0.h5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>DNN_modT</td>\n",
       "      <td>0.762549</td>\n",
       "      <td>0.929553</td>\n",
       "      <td>0.054054</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.088889</td>\n",
       "      <td>0.635126</td>\n",
       "      <td>0.030153</td>\n",
       "      <td>[[539, 35], [6, 2]]</td>\n",
       "      <td>DNN_modT0.h5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      model  threshold  accuracy  precision  recall        f1   auc_roc  \\\n",
       "0        RF   0.500000  0.982818   0.000000   0.000  0.000000  0.763284   \n",
       "1   RF_modT   0.270000  0.979381   0.166667   0.125  0.142857  0.763284   \n",
       "2       DNN   0.500000  0.738832   0.020000   0.375  0.037975  0.635126   \n",
       "3  DNN_modT   0.762549  0.929553   0.054054   0.250  0.088889  0.635126   \n",
       "\n",
       "   avg_precision      confusion_matrix   model_filename  \n",
       "0       0.060344    [[572, 2], [8, 0]]       RF0.joblib  \n",
       "1       0.060344    [[569, 5], [7, 1]]  RF_modT0.joblib  \n",
       "2       0.030153  [[427, 147], [5, 3]]          DNN0.h5  \n",
       "3       0.030153   [[539, 35], [6, 2]]     DNN_modT0.h5  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11300 samples, validate on 1488 samples\n",
      "Epoch 1/100\n",
      "11300/11300 [==============================] - 2s 201us/step - loss: 0.7350 - acc: 0.6379 - val_loss: 0.5618 - val_acc: 0.7708\n",
      "Epoch 2/100\n",
      "11300/11300 [==============================] - 2s 209us/step - loss: 0.7377 - acc: 0.6377 - val_loss: 0.5619 - val_acc: 0.7708\n",
      "Epoch 3/100\n",
      "11300/11300 [==============================] - 2s 214us/step - loss: 0.7319 - acc: 0.6388 - val_loss: 0.5619 - val_acc: 0.7708\n",
      "Epoch 4/100\n",
      "11300/11300 [==============================] - 2s 210us/step - loss: 0.7355 - acc: 0.6370 - val_loss: 0.5620 - val_acc: 0.7708\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 9.536743617033494e-10.\n",
      "Epoch 5/100\n",
      "11300/11300 [==============================] - 2s 213us/step - loss: 0.7381 - acc: 0.6356 - val_loss: 0.5620 - val_acc: 0.7708\n",
      "Epoch 6/100\n",
      "11300/11300 [==============================] - 2s 214us/step - loss: 0.7360 - acc: 0.6382 - val_loss: 0.5620 - val_acc: 0.7708\n",
      "Epoch 7/100\n",
      "11300/11300 [==============================] - 2s 215us/step - loss: 0.7324 - acc: 0.6381 - val_loss: 0.5621 - val_acc: 0.7702\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 4.768371808516747e-10.\n",
      "Epoch 8/100\n",
      "11300/11300 [==============================] - 2s 215us/step - loss: 0.7318 - acc: 0.6371 - val_loss: 0.5621 - val_acc: 0.7702\n",
      "Epoch 9/100\n",
      "11300/11300 [==============================] - 2s 215us/step - loss: 0.7365 - acc: 0.6379 - val_loss: 0.5621 - val_acc: 0.7702\n",
      "Epoch 10/100\n",
      "11300/11300 [==============================] - 2s 212us/step - loss: 0.7357 - acc: 0.6411 - val_loss: 0.5621 - val_acc: 0.7702\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 2.3841859042583735e-10.\n",
      "Epoch 11/100\n",
      "11300/11300 [==============================] - 3s 224us/step - loss: 0.7339 - acc: 0.6389 - val_loss: 0.5621 - val_acc: 0.7702\n",
      "Epoch 12/100\n",
      "11300/11300 [==============================] - 2s 219us/step - loss: 0.7361 - acc: 0.6339 - val_loss: 0.5621 - val_acc: 0.7702\n",
      "Epoch 13/100\n",
      "11300/11300 [==============================] - 2s 215us/step - loss: 0.7371 - acc: 0.6361 - val_loss: 0.5621 - val_acc: 0.7702\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 1.1920929521291868e-10.\n",
      "Epoch 14/100\n",
      "11300/11300 [==============================] - 2s 214us/step - loss: 0.7354 - acc: 0.6356 - val_loss: 0.5621 - val_acc: 0.7702\n",
      "Epoch 15/100\n",
      "11300/11300 [==============================] - 2s 214us/step - loss: 0.7362 - acc: 0.6336 - val_loss: 0.5621 - val_acc: 0.7702\n",
      "Epoch 16/100\n",
      "11300/11300 [==============================] - 2s 204us/step - loss: 0.7342 - acc: 0.6396 - val_loss: 0.5621 - val_acc: 0.7702\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 5.960464760645934e-11.\n",
      "Epoch 17/100\n",
      "11300/11300 [==============================] - 2s 210us/step - loss: 0.7347 - acc: 0.6350 - val_loss: 0.5621 - val_acc: 0.7702\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00017: early stopping\n",
      "   NR.Aromatase: 0.71816\n",
      "Model saved and metrics table updated.\n",
      "Model saved and metrics table updated.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>threshold</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>auc_roc</th>\n",
       "      <th>avg_precision</th>\n",
       "      <th>confusion_matrix</th>\n",
       "      <th>model_filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.928030</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.771931</td>\n",
       "      <td>0.311884</td>\n",
       "      <td>[[489, 0], [38, 1]]</td>\n",
       "      <td>RF0.joblib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>RF_modT</td>\n",
       "      <td>0.130000</td>\n",
       "      <td>0.840909</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.512821</td>\n",
       "      <td>0.322581</td>\n",
       "      <td>0.771931</td>\n",
       "      <td>0.311884</td>\n",
       "      <td>[[424, 65], [19, 20]]</td>\n",
       "      <td>RF_modT0.joblib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>DNN</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.772727</td>\n",
       "      <td>0.159664</td>\n",
       "      <td>0.487179</td>\n",
       "      <td>0.240506</td>\n",
       "      <td>0.718158</td>\n",
       "      <td>0.143086</td>\n",
       "      <td>[[389, 100], [20, 19]]</td>\n",
       "      <td>DNN0.h5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>DNN_modT</td>\n",
       "      <td>0.475931</td>\n",
       "      <td>0.767045</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.254545</td>\n",
       "      <td>0.718158</td>\n",
       "      <td>0.143086</td>\n",
       "      <td>[[384, 105], [18, 21]]</td>\n",
       "      <td>DNN_modT0.h5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      model  threshold  accuracy  precision    recall        f1   auc_roc  \\\n",
       "0        RF   0.500000  0.928030   1.000000  0.025641  0.050000  0.771931   \n",
       "1   RF_modT   0.130000  0.840909   0.235294  0.512821  0.322581  0.771931   \n",
       "2       DNN   0.500000  0.772727   0.159664  0.487179  0.240506  0.718158   \n",
       "3  DNN_modT   0.475931  0.767045   0.166667  0.538462  0.254545  0.718158   \n",
       "\n",
       "   avg_precision        confusion_matrix   model_filename  \n",
       "0       0.311884     [[489, 0], [38, 1]]       RF0.joblib  \n",
       "1       0.311884   [[424, 65], [19, 20]]  RF_modT0.joblib  \n",
       "2       0.143086  [[389, 100], [20, 19]]          DNN0.h5  \n",
       "3       0.143086  [[384, 105], [18, 21]]     DNN_modT0.h5  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11196 samples, validate on 1593 samples\n",
      "Epoch 1/100\n",
      "11196/11196 [==============================] - 2s 200us/step - loss: 0.7824 - acc: 0.6451 - val_loss: 0.5880 - val_acc: 0.7589\n",
      "Epoch 2/100\n",
      "11196/11196 [==============================] - 2s 217us/step - loss: 0.7853 - acc: 0.6436 - val_loss: 0.5880 - val_acc: 0.7589\n",
      "Epoch 3/100\n",
      "11196/11196 [==============================] - 2s 214us/step - loss: 0.7875 - acc: 0.6437 - val_loss: 0.5880 - val_acc: 0.7589\n",
      "Epoch 4/100\n",
      "11196/11196 [==============================] - 2s 209us/step - loss: 0.7822 - acc: 0.6476 - val_loss: 0.5880 - val_acc: 0.7589\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 2.980232380322967e-11.\n",
      "Epoch 5/100\n",
      "11196/11196 [==============================] - 2s 208us/step - loss: 0.7862 - acc: 0.6417 - val_loss: 0.5880 - val_acc: 0.7589\n",
      "Epoch 6/100\n",
      "11196/11196 [==============================] - 2s 212us/step - loss: 0.7820 - acc: 0.6442 - val_loss: 0.5880 - val_acc: 0.7589\n",
      "Epoch 7/100\n",
      "11196/11196 [==============================] - 2s 214us/step - loss: 0.7882 - acc: 0.6391 - val_loss: 0.5880 - val_acc: 0.7589\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 1.4901161901614834e-11.\n",
      "Epoch 8/100\n",
      "11196/11196 [==============================] - 2s 211us/step - loss: 0.7848 - acc: 0.6426 - val_loss: 0.5880 - val_acc: 0.7589\n",
      "Epoch 9/100\n",
      "11196/11196 [==============================] - 2s 208us/step - loss: 0.7884 - acc: 0.6458 - val_loss: 0.5880 - val_acc: 0.7589\n",
      "Epoch 10/100\n",
      "11196/11196 [==============================] - 2s 209us/step - loss: 0.7862 - acc: 0.6430 - val_loss: 0.5880 - val_acc: 0.7589\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 7.450580950807417e-12.\n",
      "Epoch 11/100\n",
      "11196/11196 [==============================] - 2s 215us/step - loss: 0.7821 - acc: 0.6445 - val_loss: 0.5880 - val_acc: 0.7589\n",
      "Epoch 12/100\n",
      "11196/11196 [==============================] - 2s 209us/step - loss: 0.7886 - acc: 0.6399 - val_loss: 0.5880 - val_acc: 0.7589\n",
      "Epoch 13/100\n",
      "11196/11196 [==============================] - 2s 209us/step - loss: 0.7858 - acc: 0.6439 - val_loss: 0.5880 - val_acc: 0.7589\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 3.725290475403709e-12.\n",
      "Epoch 14/100\n",
      "11196/11196 [==============================] - 2s 209us/step - loss: 0.7855 - acc: 0.6454 - val_loss: 0.5880 - val_acc: 0.7589\n",
      "Epoch 15/100\n",
      "11196/11196 [==============================] - 2s 203us/step - loss: 0.7872 - acc: 0.6462 - val_loss: 0.5880 - val_acc: 0.7589\n",
      "Epoch 16/100\n",
      "11196/11196 [==============================] - 2s 208us/step - loss: 0.7828 - acc: 0.6470 - val_loss: 0.5880 - val_acc: 0.7589\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.8626452377018543e-12.\n",
      "Epoch 17/100\n",
      "11196/11196 [==============================] - 2s 199us/step - loss: 0.7822 - acc: 0.6457 - val_loss: 0.5880 - val_acc: 0.7589\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00017: early stopping\n",
      "          NR.ER: 0.73637\n",
      "Model saved and metrics table updated.\n",
      "Model saved and metrics table updated.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>threshold</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>auc_roc</th>\n",
       "      <th>avg_precision</th>\n",
       "      <th>confusion_matrix</th>\n",
       "      <th>model_filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.215686</td>\n",
       "      <td>0.338462</td>\n",
       "      <td>0.769007</td>\n",
       "      <td>0.445565</td>\n",
       "      <td>[[462, 3], [40, 11]]</td>\n",
       "      <td>RF0.joblib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>RF_modT</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.912791</td>\n",
       "      <td>0.607143</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.430380</td>\n",
       "      <td>0.769007</td>\n",
       "      <td>0.445565</td>\n",
       "      <td>[[454, 11], [34, 17]]</td>\n",
       "      <td>RF_modT0.joblib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>DNN</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.748062</td>\n",
       "      <td>0.203008</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.293478</td>\n",
       "      <td>0.736369</td>\n",
       "      <td>0.280064</td>\n",
       "      <td>[[359, 106], [24, 27]]</td>\n",
       "      <td>DNN0.h5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>DNN_modT</td>\n",
       "      <td>0.625295</td>\n",
       "      <td>0.815891</td>\n",
       "      <td>0.270833</td>\n",
       "      <td>0.509804</td>\n",
       "      <td>0.353741</td>\n",
       "      <td>0.736369</td>\n",
       "      <td>0.280064</td>\n",
       "      <td>[[395, 70], [25, 26]]</td>\n",
       "      <td>DNN_modT0.h5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      model  threshold  accuracy  precision    recall        f1   auc_roc  \\\n",
       "0        RF   0.500000  0.916667   0.785714  0.215686  0.338462  0.769007   \n",
       "1   RF_modT   0.400000  0.912791   0.607143  0.333333  0.430380  0.769007   \n",
       "2       DNN   0.500000  0.748062   0.203008  0.529412  0.293478  0.736369   \n",
       "3  DNN_modT   0.625295  0.815891   0.270833  0.509804  0.353741  0.736369   \n",
       "\n",
       "   avg_precision        confusion_matrix   model_filename  \n",
       "0       0.445565    [[462, 3], [40, 11]]       RF0.joblib  \n",
       "1       0.445565   [[454, 11], [34, 17]]  RF_modT0.joblib  \n",
       "2       0.280064  [[359, 106], [24, 27]]          DNN0.h5  \n",
       "3       0.280064   [[395, 70], [25, 26]]     DNN_modT0.h5  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13734 samples, validate on 1808 samples\n",
      "Epoch 1/100\n",
      "13734/13734 [==============================] - 3s 200us/step - loss: 0.7634 - acc: 0.6589 - val_loss: 0.6040 - val_acc: 0.7406\n",
      "Epoch 2/100\n",
      "13734/13734 [==============================] - 3s 211us/step - loss: 0.7638 - acc: 0.6589 - val_loss: 0.6040 - val_acc: 0.7406\n",
      "Epoch 3/100\n",
      "13734/13734 [==============================] - 3s 211us/step - loss: 0.7642 - acc: 0.6591 - val_loss: 0.6040 - val_acc: 0.7406\n",
      "Epoch 4/100\n",
      "13734/13734 [==============================] - 3s 208us/step - loss: 0.7649 - acc: 0.6557 - val_loss: 0.6040 - val_acc: 0.7406\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 9.313226188509272e-13.\n",
      "Epoch 5/100\n",
      "13734/13734 [==============================] - 3s 212us/step - loss: 0.7620 - acc: 0.6594 - val_loss: 0.6040 - val_acc: 0.7406\n",
      "Epoch 6/100\n",
      "13734/13734 [==============================] - 3s 210us/step - loss: 0.7610 - acc: 0.6589 - val_loss: 0.6040 - val_acc: 0.7406\n",
      "Epoch 7/100\n",
      "13734/13734 [==============================] - 3s 209us/step - loss: 0.7661 - acc: 0.6593 - val_loss: 0.6040 - val_acc: 0.7406\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 4.656613094254636e-13.\n",
      "Epoch 8/100\n",
      "13734/13734 [==============================] - 3s 213us/step - loss: 0.7624 - acc: 0.6576 - val_loss: 0.6040 - val_acc: 0.7406\n",
      "Epoch 9/100\n",
      "13734/13734 [==============================] - 3s 211us/step - loss: 0.7625 - acc: 0.6600 - val_loss: 0.6040 - val_acc: 0.7406\n",
      "Epoch 10/100\n",
      "13734/13734 [==============================] - 3s 209us/step - loss: 0.7645 - acc: 0.6574 - val_loss: 0.6040 - val_acc: 0.7406\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 2.328306547127318e-13.\n",
      "Epoch 11/100\n",
      "13734/13734 [==============================] - 3s 212us/step - loss: 0.7647 - acc: 0.6549 - val_loss: 0.6040 - val_acc: 0.7406\n",
      "Epoch 12/100\n",
      "13734/13734 [==============================] - 3s 205us/step - loss: 0.7610 - acc: 0.6553 - val_loss: 0.6040 - val_acc: 0.7406\n",
      "Epoch 13/100\n",
      "13734/13734 [==============================] - 3s 206us/step - loss: 0.7646 - acc: 0.6568 - val_loss: 0.6040 - val_acc: 0.7406\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 1.164153273563659e-13.\n",
      "Epoch 14/100\n",
      "13734/13734 [==============================] - 3s 201us/step - loss: 0.7617 - acc: 0.6571 - val_loss: 0.6040 - val_acc: 0.7406\n",
      "Epoch 15/100\n",
      "13734/13734 [==============================] - 3s 203us/step - loss: 0.7621 - acc: 0.6591 - val_loss: 0.6040 - val_acc: 0.7406\n",
      "Epoch 16/100\n",
      "13734/13734 [==============================] - 3s 207us/step - loss: 0.7641 - acc: 0.6596 - val_loss: 0.6040 - val_acc: 0.7406\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 5.820766367818295e-14.\n",
      "Epoch 17/100\n",
      "13734/13734 [==============================] - 3s 200us/step - loss: 0.7643 - acc: 0.6552 - val_loss: 0.6040 - val_acc: 0.7406\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00017: early stopping\n",
      "      NR.ER.LBD: 0.67832\n",
      "Model saved and metrics table updated.\n",
      "Model saved and metrics table updated.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>threshold</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>auc_roc</th>\n",
       "      <th>avg_precision</th>\n",
       "      <th>confusion_matrix</th>\n",
       "      <th>model_filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.50000</td>\n",
       "      <td>0.971667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.260870</td>\n",
       "      <td>0.750345</td>\n",
       "      <td>0.268778</td>\n",
       "      <td>[[580, 0], [17, 3]]</td>\n",
       "      <td>RF0.joblib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>RF_modT</td>\n",
       "      <td>0.30500</td>\n",
       "      <td>0.965000</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.750345</td>\n",
       "      <td>0.268778</td>\n",
       "      <td>[[573, 7], [14, 6]]</td>\n",
       "      <td>RF_modT0.joblib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>DNN</td>\n",
       "      <td>0.50000</td>\n",
       "      <td>0.725000</td>\n",
       "      <td>0.049689</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.088398</td>\n",
       "      <td>0.678319</td>\n",
       "      <td>0.069591</td>\n",
       "      <td>[[427, 153], [12, 8]]</td>\n",
       "      <td>DNN0.h5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>DNN_modT</td>\n",
       "      <td>0.74828</td>\n",
       "      <td>0.915000</td>\n",
       "      <td>0.102564</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.135593</td>\n",
       "      <td>0.678319</td>\n",
       "      <td>0.069591</td>\n",
       "      <td>[[545, 35], [16, 4]]</td>\n",
       "      <td>DNN_modT0.h5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      model  threshold  accuracy  precision  recall        f1   auc_roc  \\\n",
       "0        RF    0.50000  0.971667   1.000000    0.15  0.260870  0.750345   \n",
       "1   RF_modT    0.30500  0.965000   0.461538    0.30  0.363636  0.750345   \n",
       "2       DNN    0.50000  0.725000   0.049689    0.40  0.088398  0.678319   \n",
       "3  DNN_modT    0.74828  0.915000   0.102564    0.20  0.135593  0.678319   \n",
       "\n",
       "   avg_precision       confusion_matrix   model_filename  \n",
       "0       0.268778    [[580, 0], [17, 3]]       RF0.joblib  \n",
       "1       0.268778    [[573, 7], [14, 6]]  RF_modT0.joblib  \n",
       "2       0.069591  [[427, 153], [12, 8]]          DNN0.h5  \n",
       "3       0.069591   [[545, 35], [16, 4]]     DNN_modT0.h5  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13140 samples, validate on 1691 samples\n",
      "Epoch 1/100\n",
      "13140/13140 [==============================] - 3s 201us/step - loss: 0.8473 - acc: 0.6414 - val_loss: 0.5922 - val_acc: 0.7463\n",
      "Epoch 2/100\n",
      "13140/13140 [==============================] - 3s 213us/step - loss: 0.8494 - acc: 0.6419 - val_loss: 0.5922 - val_acc: 0.7463\n",
      "Epoch 3/100\n",
      "13140/13140 [==============================] - 3s 213us/step - loss: 0.8480 - acc: 0.6416 - val_loss: 0.5922 - val_acc: 0.7463\n",
      "Epoch 4/100\n",
      "13140/13140 [==============================] - 3s 211us/step - loss: 0.8501 - acc: 0.6409 - val_loss: 0.5922 - val_acc: 0.7463\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 2.9103831839091474e-14.\n",
      "Epoch 5/100\n",
      "13140/13140 [==============================] - 3s 210us/step - loss: 0.8500 - acc: 0.6400 - val_loss: 0.5922 - val_acc: 0.7463\n",
      "Epoch 6/100\n",
      "13140/13140 [==============================] - 3s 211us/step - loss: 0.8496 - acc: 0.6407 - val_loss: 0.5922 - val_acc: 0.7463\n",
      "Epoch 7/100\n",
      "13140/13140 [==============================] - 3s 207us/step - loss: 0.8504 - acc: 0.6414 - val_loss: 0.5922 - val_acc: 0.7463\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 1.4551915919545737e-14.\n",
      "Epoch 8/100\n",
      "13140/13140 [==============================] - 3s 211us/step - loss: 0.8466 - acc: 0.6377 - val_loss: 0.5922 - val_acc: 0.7463\n",
      "Epoch 9/100\n",
      "13140/13140 [==============================] - 3s 212us/step - loss: 0.8507 - acc: 0.6404 - val_loss: 0.5922 - val_acc: 0.7463\n",
      "Epoch 10/100\n",
      "13140/13140 [==============================] - 3s 210us/step - loss: 0.8489 - acc: 0.6435 - val_loss: 0.5922 - val_acc: 0.7463\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 7.275957959772868e-15.\n",
      "Epoch 11/100\n",
      "13140/13140 [==============================] - 3s 208us/step - loss: 0.8507 - acc: 0.6406 - val_loss: 0.5922 - val_acc: 0.7463\n",
      "Epoch 12/100\n",
      "13140/13140 [==============================] - 3s 209us/step - loss: 0.8512 - acc: 0.6407 - val_loss: 0.5922 - val_acc: 0.7463\n",
      "Epoch 13/100\n",
      "13140/13140 [==============================] - 3s 214us/step - loss: 0.8524 - acc: 0.6384 - val_loss: 0.5922 - val_acc: 0.7463\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 3.637978979886434e-15.\n",
      "Epoch 14/100\n",
      "13140/13140 [==============================] - 3s 200us/step - loss: 0.8479 - acc: 0.6425 - val_loss: 0.5922 - val_acc: 0.7463\n",
      "Epoch 15/100\n",
      "13140/13140 [==============================] - 3s 202us/step - loss: 0.8506 - acc: 0.6368 - val_loss: 0.5922 - val_acc: 0.7463\n",
      "Epoch 16/100\n",
      "13140/13140 [==============================] - 3s 203us/step - loss: 0.8475 - acc: 0.6420 - val_loss: 0.5922 - val_acc: 0.7463\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.818989489943217e-15.\n",
      "Epoch 17/100\n",
      "13140/13140 [==============================] - 3s 203us/step - loss: 0.8496 - acc: 0.6376 - val_loss: 0.5922 - val_acc: 0.7463\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00017: early stopping\n",
      "  NR.PPAR.gamma: 0.74039\n",
      "Model saved and metrics table updated.\n",
      "Model saved and metrics table updated.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>threshold</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>auc_roc</th>\n",
       "      <th>avg_precision</th>\n",
       "      <th>confusion_matrix</th>\n",
       "      <th>model_filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.947107</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.709846</td>\n",
       "      <td>0.129989</td>\n",
       "      <td>[[573, 1], [31, 0]]</td>\n",
       "      <td>RF0.joblib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>RF_modT</td>\n",
       "      <td>0.130000</td>\n",
       "      <td>0.890909</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.322581</td>\n",
       "      <td>0.232558</td>\n",
       "      <td>0.709846</td>\n",
       "      <td>0.129989</td>\n",
       "      <td>[[529, 45], [21, 10]]</td>\n",
       "      <td>RF_modT0.joblib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>DNN</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.750413</td>\n",
       "      <td>0.115385</td>\n",
       "      <td>0.580645</td>\n",
       "      <td>0.192513</td>\n",
       "      <td>0.740390</td>\n",
       "      <td>0.109550</td>\n",
       "      <td>[[436, 138], [13, 18]]</td>\n",
       "      <td>DNN0.h5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>DNN_modT</td>\n",
       "      <td>0.577705</td>\n",
       "      <td>0.798347</td>\n",
       "      <td>0.130081</td>\n",
       "      <td>0.516129</td>\n",
       "      <td>0.207792</td>\n",
       "      <td>0.740390</td>\n",
       "      <td>0.109550</td>\n",
       "      <td>[[467, 107], [15, 16]]</td>\n",
       "      <td>DNN_modT0.h5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      model  threshold  accuracy  precision    recall        f1   auc_roc  \\\n",
       "0        RF   0.500000  0.947107   0.000000  0.000000  0.000000  0.709846   \n",
       "1   RF_modT   0.130000  0.890909   0.181818  0.322581  0.232558  0.709846   \n",
       "2       DNN   0.500000  0.750413   0.115385  0.580645  0.192513  0.740390   \n",
       "3  DNN_modT   0.577705  0.798347   0.130081  0.516129  0.207792  0.740390   \n",
       "\n",
       "   avg_precision        confusion_matrix   model_filename  \n",
       "0       0.129989     [[573, 1], [31, 0]]       RF0.joblib  \n",
       "1       0.129989   [[529, 45], [21, 10]]  RF_modT0.joblib  \n",
       "2       0.109550  [[436, 138], [13, 18]]          DNN0.h5  \n",
       "3       0.109550  [[467, 107], [15, 16]]     DNN_modT0.h5  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10006 samples, validate on 1481 samples\n",
      "Epoch 1/100\n",
      "10006/10006 [==============================] - 2s 198us/step - loss: 0.8277 - acc: 0.6072 - val_loss: 0.6143 - val_acc: 0.7387\n",
      "Epoch 2/100\n",
      "10006/10006 [==============================] - 2s 209us/step - loss: 0.8290 - acc: 0.6072 - val_loss: 0.6143 - val_acc: 0.7387\n",
      "Epoch 3/100\n",
      "10006/10006 [==============================] - 2s 214us/step - loss: 0.8288 - acc: 0.6097 - val_loss: 0.6143 - val_acc: 0.7387\n",
      "Epoch 4/100\n",
      "10006/10006 [==============================] - 2s 217us/step - loss: 0.8261 - acc: 0.6060 - val_loss: 0.6143 - val_acc: 0.7387\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 9.094947449716085e-16.\n",
      "Epoch 5/100\n",
      "10006/10006 [==============================] - 2s 211us/step - loss: 0.8253 - acc: 0.6076 - val_loss: 0.6143 - val_acc: 0.7387\n",
      "Epoch 6/100\n",
      "10006/10006 [==============================] - 2s 213us/step - loss: 0.8243 - acc: 0.6088 - val_loss: 0.6143 - val_acc: 0.7387\n",
      "Epoch 7/100\n",
      "10006/10006 [==============================] - 2s 213us/step - loss: 0.8269 - acc: 0.6092 - val_loss: 0.6143 - val_acc: 0.7387\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 4.547473724858043e-16.\n",
      "Epoch 8/100\n",
      "10006/10006 [==============================] - 2s 211us/step - loss: 0.8283 - acc: 0.6095 - val_loss: 0.6143 - val_acc: 0.7387\n",
      "Epoch 9/100\n",
      "10006/10006 [==============================] - 2s 218us/step - loss: 0.8291 - acc: 0.6070 - val_loss: 0.6143 - val_acc: 0.7387\n",
      "Epoch 10/100\n",
      "10006/10006 [==============================] - 2s 212us/step - loss: 0.8243 - acc: 0.6079 - val_loss: 0.6143 - val_acc: 0.7387\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 2.2737368624290214e-16.\n",
      "Epoch 11/100\n",
      "10006/10006 [==============================] - 2s 208us/step - loss: 0.8267 - acc: 0.6096 - val_loss: 0.6143 - val_acc: 0.7387\n",
      "Epoch 12/100\n",
      "10006/10006 [==============================] - 2s 218us/step - loss: 0.8263 - acc: 0.6059 - val_loss: 0.6143 - val_acc: 0.7387\n",
      "Epoch 13/100\n",
      "10006/10006 [==============================] - 2s 211us/step - loss: 0.8271 - acc: 0.6047 - val_loss: 0.6143 - val_acc: 0.7387\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 1.1368684312145107e-16.\n",
      "Epoch 14/100\n",
      "10006/10006 [==============================] - 2s 214us/step - loss: 0.8230 - acc: 0.6084 - val_loss: 0.6143 - val_acc: 0.7387\n",
      "Epoch 15/100\n",
      "10006/10006 [==============================] - 2s 208us/step - loss: 0.8242 - acc: 0.6107 - val_loss: 0.6143 - val_acc: 0.7387\n",
      "Epoch 16/100\n",
      "10006/10006 [==============================] - 2s 212us/step - loss: 0.8264 - acc: 0.6108 - val_loss: 0.6143 - val_acc: 0.7387\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 5.684342156072553e-17.\n",
      "Epoch 17/100\n",
      "10006/10006 [==============================] - 2s 214us/step - loss: 0.8261 - acc: 0.6097 - val_loss: 0.6143 - val_acc: 0.7387\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00017: early stopping\n",
      "         SR.ARE: 0.70550\n",
      "Model saved and metrics table updated.\n",
      "Model saved and metrics table updated.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>threshold</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>auc_roc</th>\n",
       "      <th>avg_precision</th>\n",
       "      <th>confusion_matrix</th>\n",
       "      <th>model_filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.841441</td>\n",
       "      <td>0.592593</td>\n",
       "      <td>0.172043</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.780838</td>\n",
       "      <td>0.430711</td>\n",
       "      <td>[[451, 11], [77, 16]]</td>\n",
       "      <td>RF0.joblib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>RF_modT</td>\n",
       "      <td>0.340000</td>\n",
       "      <td>0.823423</td>\n",
       "      <td>0.476190</td>\n",
       "      <td>0.537634</td>\n",
       "      <td>0.505051</td>\n",
       "      <td>0.780838</td>\n",
       "      <td>0.430711</td>\n",
       "      <td>[[407, 55], [43, 50]]</td>\n",
       "      <td>RF_modT0.joblib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>DNN</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.762162</td>\n",
       "      <td>0.355556</td>\n",
       "      <td>0.516129</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>0.705500</td>\n",
       "      <td>0.347718</td>\n",
       "      <td>[[375, 87], [45, 48]]</td>\n",
       "      <td>DNN0.h5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>DNN_modT</td>\n",
       "      <td>0.457539</td>\n",
       "      <td>0.753153</td>\n",
       "      <td>0.353333</td>\n",
       "      <td>0.569892</td>\n",
       "      <td>0.436214</td>\n",
       "      <td>0.705500</td>\n",
       "      <td>0.347718</td>\n",
       "      <td>[[365, 97], [40, 53]]</td>\n",
       "      <td>DNN_modT0.h5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      model  threshold  accuracy  precision    recall        f1   auc_roc  \\\n",
       "0        RF   0.500000  0.841441   0.592593  0.172043  0.266667  0.780838   \n",
       "1   RF_modT   0.340000  0.823423   0.476190  0.537634  0.505051  0.780838   \n",
       "2       DNN   0.500000  0.762162   0.355556  0.516129  0.421053  0.705500   \n",
       "3  DNN_modT   0.457539  0.753153   0.353333  0.569892  0.436214  0.705500   \n",
       "\n",
       "   avg_precision       confusion_matrix   model_filename  \n",
       "0       0.430711  [[451, 11], [77, 16]]       RF0.joblib  \n",
       "1       0.430711  [[407, 55], [43, 50]]  RF_modT0.joblib  \n",
       "2       0.347718  [[375, 87], [45, 48]]          DNN0.h5  \n",
       "3       0.347718  [[365, 97], [40, 53]]     DNN_modT0.h5  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14400 samples, validate on 1873 samples\n",
      "Epoch 1/100\n",
      "14400/14400 [==============================] - 3s 207us/step - loss: 0.7856 - acc: 0.6460 - val_loss: 0.6163 - val_acc: 0.7277\n",
      "Epoch 2/100\n",
      "14400/14400 [==============================] - 3s 216us/step - loss: 0.7849 - acc: 0.6498 - val_loss: 0.6163 - val_acc: 0.7277\n",
      "Epoch 3/100\n",
      "14400/14400 [==============================] - 3s 213us/step - loss: 0.7839 - acc: 0.6488 - val_loss: 0.6163 - val_acc: 0.7277\n",
      "Epoch 4/100\n",
      "14400/14400 [==============================] - 3s 215us/step - loss: 0.7871 - acc: 0.6468 - val_loss: 0.6163 - val_acc: 0.7277\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 2.842171078036277e-17.\n",
      "Epoch 5/100\n",
      "14400/14400 [==============================] - 3s 210us/step - loss: 0.7841 - acc: 0.6508 - val_loss: 0.6163 - val_acc: 0.7277\n",
      "Epoch 6/100\n",
      "14400/14400 [==============================] - 3s 213us/step - loss: 0.7849 - acc: 0.6462 - val_loss: 0.6163 - val_acc: 0.7277\n",
      "Epoch 7/100\n",
      "14400/14400 [==============================] - 3s 212us/step - loss: 0.7881 - acc: 0.6475 - val_loss: 0.6163 - val_acc: 0.7277\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 1.4210855390181384e-17.\n",
      "Epoch 8/100\n",
      "14400/14400 [==============================] - 3s 212us/step - loss: 0.7859 - acc: 0.6503 - val_loss: 0.6163 - val_acc: 0.7277\n",
      "Epoch 9/100\n",
      "14400/14400 [==============================] - 3s 213us/step - loss: 0.7861 - acc: 0.6492 - val_loss: 0.6163 - val_acc: 0.7277\n",
      "Epoch 10/100\n",
      "14400/14400 [==============================] - 3s 212us/step - loss: 0.7865 - acc: 0.6467 - val_loss: 0.6163 - val_acc: 0.7277\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 7.105427695090692e-18.\n",
      "Epoch 11/100\n",
      "14400/14400 [==============================] - 3s 210us/step - loss: 0.7842 - acc: 0.6473 - val_loss: 0.6163 - val_acc: 0.7277\n",
      "Epoch 12/100\n",
      "14400/14400 [==============================] - 3s 214us/step - loss: 0.7872 - acc: 0.6451 - val_loss: 0.6163 - val_acc: 0.7277\n",
      "Epoch 13/100\n",
      "14400/14400 [==============================] - 3s 210us/step - loss: 0.7902 - acc: 0.6468 - val_loss: 0.6163 - val_acc: 0.7277\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 3.552713847545346e-18.\n",
      "Epoch 14/100\n",
      "14400/14400 [==============================] - 3s 214us/step - loss: 0.7862 - acc: 0.6485 - val_loss: 0.6163 - val_acc: 0.7277\n",
      "Epoch 15/100\n",
      "14400/14400 [==============================] - 3s 206us/step - loss: 0.7876 - acc: 0.6485 - val_loss: 0.6163 - val_acc: 0.7277\n",
      "Epoch 16/100\n",
      "14400/14400 [==============================] - 3s 201us/step - loss: 0.7894 - acc: 0.6469 - val_loss: 0.6163 - val_acc: 0.7277\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.776356923772673e-18.\n",
      "Epoch 17/100\n",
      "14400/14400 [==============================] - 3s 214us/step - loss: 0.7857 - acc: 0.6475 - val_loss: 0.6163 - val_acc: 0.7277\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00017: early stopping\n",
      "       SR.ATAD5: 0.71161\n",
      "Model saved and metrics table updated.\n",
      "Model saved and metrics table updated.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>threshold</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>auc_roc</th>\n",
       "      <th>avg_precision</th>\n",
       "      <th>confusion_matrix</th>\n",
       "      <th>model_filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.938907</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.795625</td>\n",
       "      <td>0.427828</td>\n",
       "      <td>[[584, 0], [38, 0]]</td>\n",
       "      <td>RF0.joblib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>RF_modT</td>\n",
       "      <td>0.255000</td>\n",
       "      <td>0.948553</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.795625</td>\n",
       "      <td>0.427828</td>\n",
       "      <td>[[576, 8], [24, 14]]</td>\n",
       "      <td>RF_modT0.joblib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>DNN</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.736334</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.552632</td>\n",
       "      <td>0.203883</td>\n",
       "      <td>0.711608</td>\n",
       "      <td>0.153415</td>\n",
       "      <td>[[437, 147], [17, 21]]</td>\n",
       "      <td>DNN0.h5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>DNN_modT</td>\n",
       "      <td>0.577705</td>\n",
       "      <td>0.789389</td>\n",
       "      <td>0.155556</td>\n",
       "      <td>0.552632</td>\n",
       "      <td>0.242775</td>\n",
       "      <td>0.711608</td>\n",
       "      <td>0.153415</td>\n",
       "      <td>[[470, 114], [17, 21]]</td>\n",
       "      <td>DNN_modT0.h5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      model  threshold  accuracy  precision    recall        f1   auc_roc  \\\n",
       "0        RF   0.500000  0.938907   0.000000  0.000000  0.000000  0.795625   \n",
       "1   RF_modT   0.255000  0.948553   0.636364  0.368421  0.466667  0.795625   \n",
       "2       DNN   0.500000  0.736334   0.125000  0.552632  0.203883  0.711608   \n",
       "3  DNN_modT   0.577705  0.789389   0.155556  0.552632  0.242775  0.711608   \n",
       "\n",
       "   avg_precision        confusion_matrix   model_filename  \n",
       "0       0.427828     [[584, 0], [38, 0]]       RF0.joblib  \n",
       "1       0.427828    [[576, 8], [24, 14]]  RF_modT0.joblib  \n",
       "2       0.153415  [[437, 147], [17, 21]]          DNN0.h5  \n",
       "3       0.153415  [[470, 114], [17, 21]]     DNN_modT0.h5  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12766 samples, validate on 1684 samples\n",
      "Epoch 1/100\n",
      "12766/12766 [==============================] - 3s 201us/step - loss: 0.8939 - acc: 0.5726 - val_loss: 0.6040 - val_acc: 0.7387\n",
      "Epoch 2/100\n",
      "12766/12766 [==============================] - 3s 216us/step - loss: 0.8961 - acc: 0.5725 - val_loss: 0.6040 - val_acc: 0.7387\n",
      "Epoch 3/100\n",
      "12766/12766 [==============================] - 3s 213us/step - loss: 0.8974 - acc: 0.5715 - val_loss: 0.6040 - val_acc: 0.7387\n",
      "Epoch 4/100\n",
      "12766/12766 [==============================] - 3s 213us/step - loss: 0.8990 - acc: 0.5734 - val_loss: 0.6040 - val_acc: 0.7387\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 8.881784618863365e-19.\n",
      "Epoch 5/100\n",
      "12766/12766 [==============================] - 3s 210us/step - loss: 0.8931 - acc: 0.5728 - val_loss: 0.6040 - val_acc: 0.7387\n",
      "Epoch 6/100\n",
      "12766/12766 [==============================] - 3s 209us/step - loss: 0.8964 - acc: 0.5714 - val_loss: 0.6040 - val_acc: 0.7387\n",
      "Epoch 7/100\n",
      "12766/12766 [==============================] - 3s 225us/step - loss: 0.8986 - acc: 0.5725 - val_loss: 0.6040 - val_acc: 0.7387\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 4.440892309431682e-19.\n",
      "Epoch 8/100\n",
      "12766/12766 [==============================] - 3s 214us/step - loss: 0.8973 - acc: 0.5717 - val_loss: 0.6040 - val_acc: 0.7387\n",
      "Epoch 9/100\n",
      "12766/12766 [==============================] - 3s 213us/step - loss: 0.8969 - acc: 0.5734 - val_loss: 0.6040 - val_acc: 0.7387\n",
      "Epoch 10/100\n",
      "12766/12766 [==============================] - 3s 207us/step - loss: 0.8970 - acc: 0.5706 - val_loss: 0.6040 - val_acc: 0.7387\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 2.220446154715841e-19.\n",
      "Epoch 11/100\n",
      "12766/12766 [==============================] - 3s 210us/step - loss: 0.8962 - acc: 0.5743 - val_loss: 0.6040 - val_acc: 0.7387\n",
      "Epoch 12/100\n",
      "12766/12766 [==============================] - 3s 204us/step - loss: 0.8947 - acc: 0.5739 - val_loss: 0.6040 - val_acc: 0.7387\n",
      "Epoch 13/100\n",
      "12766/12766 [==============================] - 3s 214us/step - loss: 0.8954 - acc: 0.5743 - val_loss: 0.6040 - val_acc: 0.7387\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 1.1102230773579206e-19.\n",
      "Epoch 14/100\n",
      "12766/12766 [==============================] - 3s 211us/step - loss: 0.8960 - acc: 0.5734 - val_loss: 0.6040 - val_acc: 0.7387\n",
      "Epoch 15/100\n",
      "12766/12766 [==============================] - 3s 202us/step - loss: 0.9010 - acc: 0.5701 - val_loss: 0.6040 - val_acc: 0.7387\n",
      "Epoch 16/100\n",
      "12766/12766 [==============================] - 3s 202us/step - loss: 0.8929 - acc: 0.5774 - val_loss: 0.6040 - val_acc: 0.7387\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 5.551115386789603e-20.\n",
      "Epoch 17/100\n",
      "12766/12766 [==============================] - 3s 204us/step - loss: 0.8962 - acc: 0.5713 - val_loss: 0.6040 - val_acc: 0.7387\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00017: early stopping\n",
      "         SR.HSE: 0.62674\n",
      "Model saved and metrics table updated.\n",
      "Model saved and metrics table updated.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>threshold</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>auc_roc</th>\n",
       "      <th>avg_precision</th>\n",
       "      <th>confusion_matrix</th>\n",
       "      <th>model_filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.965574</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.759586</td>\n",
       "      <td>0.239563</td>\n",
       "      <td>[[586, 2], [19, 3]]</td>\n",
       "      <td>RF0.joblib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>RF_modT</td>\n",
       "      <td>0.340000</td>\n",
       "      <td>0.965574</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.759586</td>\n",
       "      <td>0.239563</td>\n",
       "      <td>[[583, 5], [16, 6]]</td>\n",
       "      <td>RF_modT0.joblib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>DNN</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.726230</td>\n",
       "      <td>0.055215</td>\n",
       "      <td>0.409091</td>\n",
       "      <td>0.097297</td>\n",
       "      <td>0.626739</td>\n",
       "      <td>0.054621</td>\n",
       "      <td>[[434, 154], [13, 9]]</td>\n",
       "      <td>DNN0.h5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>DNN_modT</td>\n",
       "      <td>0.450345</td>\n",
       "      <td>0.708197</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.118812</td>\n",
       "      <td>0.626739</td>\n",
       "      <td>0.054621</td>\n",
       "      <td>[[420, 168], [10, 12]]</td>\n",
       "      <td>DNN_modT0.h5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      model  threshold  accuracy  precision    recall        f1   auc_roc  \\\n",
       "0        RF   0.500000  0.965574   0.600000  0.136364  0.222222  0.759586   \n",
       "1   RF_modT   0.340000  0.965574   0.545455  0.272727  0.363636  0.759586   \n",
       "2       DNN   0.500000  0.726230   0.055215  0.409091  0.097297  0.626739   \n",
       "3  DNN_modT   0.450345  0.708197   0.066667  0.545455  0.118812  0.626739   \n",
       "\n",
       "   avg_precision        confusion_matrix   model_filename  \n",
       "0       0.239563     [[586, 2], [19, 3]]       RF0.joblib  \n",
       "1       0.239563     [[583, 5], [16, 6]]  RF_modT0.joblib  \n",
       "2       0.054621   [[434, 154], [13, 9]]          DNN0.h5  \n",
       "3       0.054621  [[420, 168], [10, 12]]     DNN_modT0.h5  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10204 samples, validate on 1512 samples\n",
      "Epoch 1/100\n",
      "10204/10204 [==============================] - 2s 201us/step - loss: 0.7619 - acc: 0.6424 - val_loss: 0.6140 - val_acc: 0.7421\n",
      "Epoch 2/100\n",
      "10204/10204 [==============================] - 2s 218us/step - loss: 0.7654 - acc: 0.6400 - val_loss: 0.6140 - val_acc: 0.7421\n",
      "Epoch 3/100\n",
      "10204/10204 [==============================] - 2s 215us/step - loss: 0.7604 - acc: 0.6392 - val_loss: 0.6140 - val_acc: 0.7421\n",
      "Epoch 4/100\n",
      "10204/10204 [==============================] - 2s 219us/step - loss: 0.7587 - acc: 0.6449 - val_loss: 0.6140 - val_acc: 0.7421\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 2.7755576933948015e-20.\n",
      "Epoch 5/100\n",
      "10204/10204 [==============================] - 2s 214us/step - loss: 0.7617 - acc: 0.6446 - val_loss: 0.6140 - val_acc: 0.7421\n",
      "Epoch 6/100\n",
      "10204/10204 [==============================] - 2s 213us/step - loss: 0.7611 - acc: 0.6400 - val_loss: 0.6140 - val_acc: 0.7421\n",
      "Epoch 7/100\n",
      "10204/10204 [==============================] - 2s 212us/step - loss: 0.7633 - acc: 0.6404 - val_loss: 0.6140 - val_acc: 0.7421\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 1.3877788466974007e-20.\n",
      "Epoch 8/100\n",
      "10204/10204 [==============================] - 2s 216us/step - loss: 0.7594 - acc: 0.6465 - val_loss: 0.6140 - val_acc: 0.7421\n",
      "Epoch 9/100\n",
      "10204/10204 [==============================] - 2s 216us/step - loss: 0.7610 - acc: 0.6450 - val_loss: 0.6140 - val_acc: 0.7421\n",
      "Epoch 10/100\n",
      "10204/10204 [==============================] - 2s 215us/step - loss: 0.7598 - acc: 0.6421 - val_loss: 0.6140 - val_acc: 0.7421\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 6.938894233487004e-21.\n",
      "Epoch 11/100\n",
      "10204/10204 [==============================] - 2s 221us/step - loss: 0.7635 - acc: 0.6415 - val_loss: 0.6140 - val_acc: 0.7421\n",
      "Epoch 12/100\n",
      "10204/10204 [==============================] - 2s 213us/step - loss: 0.7603 - acc: 0.6439 - val_loss: 0.6140 - val_acc: 0.7421\n",
      "Epoch 13/100\n",
      "10204/10204 [==============================] - 2s 216us/step - loss: 0.7582 - acc: 0.6419 - val_loss: 0.6140 - val_acc: 0.7421\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 3.469447116743502e-21.\n",
      "Epoch 14/100\n",
      "10204/10204 [==============================] - 2s 216us/step - loss: 0.7601 - acc: 0.6458 - val_loss: 0.6140 - val_acc: 0.7421\n",
      "Epoch 15/100\n",
      "10204/10204 [==============================] - 2s 215us/step - loss: 0.7570 - acc: 0.6437 - val_loss: 0.6140 - val_acc: 0.7421\n",
      "Epoch 16/100\n",
      "10204/10204 [==============================] - 2s 213us/step - loss: 0.7629 - acc: 0.6403 - val_loss: 0.6140 - val_acc: 0.7421\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.734723558371751e-21.\n",
      "Epoch 17/100\n",
      "10204/10204 [==============================] - 2s 209us/step - loss: 0.7598 - acc: 0.6424 - val_loss: 0.6140 - val_acc: 0.7421\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00017: early stopping\n",
      "         SR.MMP: 0.80388\n",
      "Model saved and metrics table updated.\n",
      "Model saved and metrics table updated.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>threshold</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>auc_roc</th>\n",
       "      <th>avg_precision</th>\n",
       "      <th>confusion_matrix</th>\n",
       "      <th>model_filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.906077</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.484848</td>\n",
       "      <td>0.930331</td>\n",
       "      <td>0.560184</td>\n",
       "      <td>[[468, 15], [36, 24]]</td>\n",
       "      <td>RF0.joblib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>RF_modT</td>\n",
       "      <td>0.382500</td>\n",
       "      <td>0.913444</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.624000</td>\n",
       "      <td>0.930331</td>\n",
       "      <td>0.560184</td>\n",
       "      <td>[[457, 26], [21, 39]]</td>\n",
       "      <td>RF_modT0.joblib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>DNN</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.815838</td>\n",
       "      <td>0.330508</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.438202</td>\n",
       "      <td>0.803882</td>\n",
       "      <td>0.296565</td>\n",
       "      <td>[[404, 79], [21, 39]]</td>\n",
       "      <td>DNN0.h5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>DNN_modT</td>\n",
       "      <td>0.505391</td>\n",
       "      <td>0.817680</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.440678</td>\n",
       "      <td>0.803882</td>\n",
       "      <td>0.296565</td>\n",
       "      <td>[[405, 78], [21, 39]]</td>\n",
       "      <td>DNN_modT0.h5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      model  threshold  accuracy  precision  recall        f1   auc_roc  \\\n",
       "0        RF   0.500000  0.906077   0.615385    0.40  0.484848  0.930331   \n",
       "1   RF_modT   0.382500  0.913444   0.600000    0.65  0.624000  0.930331   \n",
       "2       DNN   0.500000  0.815838   0.330508    0.65  0.438202  0.803882   \n",
       "3  DNN_modT   0.505391  0.817680   0.333333    0.65  0.440678  0.803882   \n",
       "\n",
       "   avg_precision       confusion_matrix   model_filename  \n",
       "0       0.560184  [[468, 15], [36, 24]]       RF0.joblib  \n",
       "1       0.560184  [[457, 26], [21, 39]]  RF_modT0.joblib  \n",
       "2       0.296565  [[404, 79], [21, 39]]          DNN0.h5  \n",
       "3       0.296565  [[405, 78], [21, 39]]     DNN_modT0.h5  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13340 samples, validate on 1781 samples\n",
      "Epoch 1/100\n",
      "13340/13340 [==============================] - 3s 206us/step - loss: 0.7926 - acc: 0.6395 - val_loss: 0.6058 - val_acc: 0.7344\n",
      "Epoch 2/100\n",
      "13340/13340 [==============================] - 3s 218us/step - loss: 0.7896 - acc: 0.6397 - val_loss: 0.6058 - val_acc: 0.7344\n",
      "Epoch 3/100\n",
      "13340/13340 [==============================] - 3s 215us/step - loss: 0.7886 - acc: 0.6389 - val_loss: 0.6058 - val_acc: 0.7344\n",
      "Epoch 4/100\n",
      "13340/13340 [==============================] - 3s 214us/step - loss: 0.7935 - acc: 0.6389 - val_loss: 0.6058 - val_acc: 0.7344\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 8.673617791858755e-22.\n",
      "Epoch 5/100\n",
      "13340/13340 [==============================] - 3s 213us/step - loss: 0.7917 - acc: 0.6403 - val_loss: 0.6058 - val_acc: 0.7344\n",
      "Epoch 6/100\n",
      "13340/13340 [==============================] - 3s 212us/step - loss: 0.7906 - acc: 0.6421 - val_loss: 0.6058 - val_acc: 0.7344\n",
      "Epoch 7/100\n",
      "13340/13340 [==============================] - 3s 215us/step - loss: 0.7909 - acc: 0.6407 - val_loss: 0.6058 - val_acc: 0.7344\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 4.336808895929377e-22.\n",
      "Epoch 8/100\n",
      "13340/13340 [==============================] - 3s 219us/step - loss: 0.7873 - acc: 0.6408 - val_loss: 0.6058 - val_acc: 0.7344\n",
      "Epoch 9/100\n",
      "13340/13340 [==============================] - 3s 224us/step - loss: 0.7938 - acc: 0.6364 - val_loss: 0.6058 - val_acc: 0.7344\n",
      "Epoch 10/100\n",
      "13340/13340 [==============================] - 3s 225us/step - loss: 0.7931 - acc: 0.6383 - val_loss: 0.6058 - val_acc: 0.7344\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 2.1684044479646887e-22.\n",
      "Epoch 11/100\n",
      "13340/13340 [==============================] - 3s 221us/step - loss: 0.7897 - acc: 0.6435 - val_loss: 0.6058 - val_acc: 0.7344\n",
      "Epoch 12/100\n",
      "13340/13340 [==============================] - 3s 211us/step - loss: 0.7910 - acc: 0.6357 - val_loss: 0.6058 - val_acc: 0.7344\n",
      "Epoch 13/100\n",
      "13340/13340 [==============================] - 3s 217us/step - loss: 0.7885 - acc: 0.6378 - val_loss: 0.6058 - val_acc: 0.7344\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 1.0842022239823443e-22.\n",
      "Epoch 14/100\n",
      "13340/13340 [==============================] - 3s 209us/step - loss: 0.7954 - acc: 0.6359 - val_loss: 0.6058 - val_acc: 0.7344\n",
      "Epoch 15/100\n",
      "13340/13340 [==============================] - 3s 208us/step - loss: 0.7901 - acc: 0.6382 - val_loss: 0.6058 - val_acc: 0.7344\n",
      "Epoch 16/100\n",
      "13340/13340 [==============================] - 3s 213us/step - loss: 0.7933 - acc: 0.6373 - val_loss: 0.6058 - val_acc: 0.7344\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 5.421011119911722e-23.\n",
      "Epoch 17/100\n",
      "13340/13340 [==============================] - 3s 207us/step - loss: 0.7926 - acc: 0.6397 - val_loss: 0.6058 - val_acc: 0.7344\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00017: early stopping\n",
      "         SR.p53: 0.76870\n",
      "Model saved and metrics table updated.\n",
      "Model saved and metrics table updated.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>threshold</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>auc_roc</th>\n",
       "      <th>avg_precision</th>\n",
       "      <th>confusion_matrix</th>\n",
       "      <th>model_filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.933442</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.024390</td>\n",
       "      <td>0.046512</td>\n",
       "      <td>0.806702</td>\n",
       "      <td>0.221990</td>\n",
       "      <td>[[574, 1], [40, 1]]</td>\n",
       "      <td>RF0.joblib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>RF_modT</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>0.886364</td>\n",
       "      <td>0.254237</td>\n",
       "      <td>0.365854</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.806702</td>\n",
       "      <td>0.221990</td>\n",
       "      <td>[[531, 44], [26, 15]]</td>\n",
       "      <td>RF_modT0.joblib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>DNN</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.753247</td>\n",
       "      <td>0.150943</td>\n",
       "      <td>0.585366</td>\n",
       "      <td>0.240000</td>\n",
       "      <td>0.768696</td>\n",
       "      <td>0.168234</td>\n",
       "      <td>[[440, 135], [17, 24]]</td>\n",
       "      <td>DNN0.h5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>DNN_modT</td>\n",
       "      <td>0.540362</td>\n",
       "      <td>0.780844</td>\n",
       "      <td>0.169014</td>\n",
       "      <td>0.585366</td>\n",
       "      <td>0.262295</td>\n",
       "      <td>0.768696</td>\n",
       "      <td>0.168234</td>\n",
       "      <td>[[457, 118], [17, 24]]</td>\n",
       "      <td>DNN_modT0.h5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      model  threshold  accuracy  precision    recall        f1   auc_roc  \\\n",
       "0        RF   0.500000  0.933442   0.500000  0.024390  0.046512  0.806702   \n",
       "1   RF_modT   0.220000  0.886364   0.254237  0.365854  0.300000  0.806702   \n",
       "2       DNN   0.500000  0.753247   0.150943  0.585366  0.240000  0.768696   \n",
       "3  DNN_modT   0.540362  0.780844   0.169014  0.585366  0.262295  0.768696   \n",
       "\n",
       "   avg_precision        confusion_matrix   model_filename  \n",
       "0       0.221990     [[574, 1], [40, 1]]       RF0.joblib  \n",
       "1       0.221990   [[531, 44], [26, 15]]  RF_modT0.joblib  \n",
       "2       0.168234  [[440, 135], [17, 24]]          DNN0.h5  \n",
       "3       0.168234  [[457, 118], [17, 24]]     DNN_modT0.h5  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for target in y_tr.columns:\n",
    "    # Determine rows with available data\n",
    "    rows_tr = np.isfinite(y_tr[target]).values\n",
    "    rows_te = np.isfinite(y_te[target]).values\n",
    "    x,y = x_tr[rows_tr], y_tr[target][rows_tr]\n",
    "    \n",
    "    # Address Class Imbalance\n",
    "    x_train, x_val, y_train, y_val = train_test_split(x, y, stratify=y, \\\n",
    "                                                      test_size=0.2, random_state=42)\n",
    "    ros = RandomOverSampler(random_state=0)\n",
    "    x_resampled, y_resampled = ros.fit_sample(x_train,y_train)\n",
    "    \n",
    "    # Train the DNN\n",
    "    DNN.fit(\n",
    "        x_resampled, y_resampled, batch_size=512, epochs=100,\\\n",
    "        validation_data=(x_val,y_val), verbose=1,\n",
    "        callbacks=[\n",
    "            keras.callbacks.EarlyStopping(patience=16,verbose=1,\\\n",
    "                                          restore_best_weights=True),\n",
    "            keras.callbacks.ReduceLROnPlateau(factor=0.5,patience=3,verbose=1)\n",
    "        ])\n",
    "    \n",
    "    # Get predictions, calculate model performance and save info\n",
    "    p_te = DNN.predict(x_te[rows_te])[:,0]\n",
    "    y_testing=y_te[target][rows_te]\n",
    "    auc_te = roc_auc_score(y_testing, p_te)\n",
    "    print(\"%15s: %3.5f\" % (target, auc_te))\n",
    "\n",
    "    y_hat_testing=DNN.predict_classes(x_te[rows_te])\n",
    "    average_precision=average_precision_score(y_testing,p_te)\n",
    "    mv=evaluate_model_predictions(target,'DNN',0.5,y_testing,y_hat_testing,\\\n",
    "                                  auc_te,average_precision)\n",
    "    filename = check_and_save(target,mv,DNN,True)\n",
    "    \n",
    "    # Find max F1 varying probability threshold, calculate modified performance, save\n",
    "    precision, recall, thresholds = precision_recall_curve(y_testing,p_te)\n",
    "    # f1 = 2*precision*recall/(precision+recall)  # Sometimes precision=recall=0!\n",
    "    p,r,t = zip(*[i for i in zip(precision,recall,np.append(thresholds,1)) if i[0:2]!=(0,0)])\n",
    "    p,r,t = np.array(p),np.array(r),np.array(t)\n",
    "    f1 = 2*p*r/(p+r)\n",
    "    m_idx = np.argmax(f1)\n",
    "    m_thresh = thresholds[m_idx]\n",
    "    y_hat_testing_adj=adjusted_classes(p_te,m_thresh)\n",
    "    mv=evaluate_model_predictions(target,'DNN_modT',m_thresh,y_testing,y_hat_testing_adj,\\\n",
    "                                  auc_te,average_precision)\n",
    "    if filename is None:\n",
    "        check_and_save(target,mv,DNN,True)\n",
    "    else:\n",
    "        check_and_save(target,mv,filename,True)\n",
    "    display(get_model_perfs(target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Performance Metrics Saving Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T05:43:49.580236Z",
     "start_time": "2019-12-02T05:43:49.383833Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "p_te = DNN.predict(x_te[rows_te])[:,0]\n",
    "y_testing=y_te[target][rows_te]\n",
    "auc_te = roc_auc_score(y_testing, p_te)\n",
    "print(\"%15s: %3.5f\" % (target, auc_te))\n",
    "\n",
    "y_hat_testing=DNN.predict_classes(x_te[rows_te])\n",
    "average_precision=average_precision_score(y_testing,p_te)\n",
    "mv=evaluate_model_predictions(target,'DNN',0.5,y_testing,y_hat_testing,\\\n",
    "                              auc_te,average_precision)\n",
    "#     check_and_save(target,mv,DNN,True)\n",
    "display(mv)\n",
    "precision, recall, thresholds = precision_recall_curve(y_testing,p_te)\n",
    "# f1 = 2*precision*recall/(precision+recall)\n",
    "p,r,t = zip(*[i for i in zip(precision,recall,np.append(thresholds,1)) if i[0:2]!=(0,0)])\n",
    "p,r,t = np.array(p),np.array(r),np.array(t)\n",
    "f1 = 2*p*r/(p+r)\n",
    "m_idx = np.argmax(f1)\n",
    "m_thresh = thresholds[m_idx]\n",
    "y_hat_testing_adj=adjusted_classes(p_te,m_thresh)\n",
    "mv=evaluate_model_predictions(target,'DNN_modT',m_thresh,y_testing,y_hat_testing_adj,\\\n",
    "                              auc_te,average_precision)\n",
    "#     check_and_save(target,mv,DNN,True)\n",
    "# display(get_model_perfs(target))\n",
    "display(mv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Troubleshooting Precision-Recall Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T05:08:08.069861Z",
     "start_time": "2019-12-02T05:08:08.008719Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "f1 = 2*precision*recall/(precision+recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T05:52:54.150467Z",
     "start_time": "2019-12-02T05:52:54.089918Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "a, b = zip(*zip(precision,recall))\n",
    "type(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T05:41:49.450684Z",
     "start_time": "2019-12-02T05:41:49.389635Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "p,r,t = zip(*[i for i in zip(precision,recall,np.append(thresholds,1)) if i[0:2]!=(0,0)])\n",
    "p,r,t = np.array(p),np.array(r),np.array(t)\n",
    "f1 = 2*p*r/(p+r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Testing DNN Model Saving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T00:48:15.368581Z",
     "start_time": "2019-12-03T00:48:15.274930Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DNN_test0.h5'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = save_model(target,'DNN_test',DNN,True)\n",
    "filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T00:48:29.774885Z",
     "start_time": "2019-12-03T00:48:28.661557Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.engine.sequential.Sequential at 0x1276ace10>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_model(target,filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T00:48:35.160766Z",
     "start_time": "2019-12-03T00:48:35.156689Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.5'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Saved DNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-12T04:17:38.891552Z",
     "start_time": "2019-12-12T04:17:38.222878Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "target = 'NR.AhR'\n",
    "DNN = read_model(target,'DNN_modT0.h5')\n",
    "\n",
    "# target = 'SR.HSE'\n",
    "# DNN = read_model(target,'DNN_MaxMCC4.h5')\n",
    "\n",
    "# target = 'SR.MPP'\n",
    "# DNN = read_model(target,'DNN5.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-12T04:17:39.767205Z",
     "start_time": "2019-12-12T04:17:39.763319Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_144\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "h0_sigmoid_activation (Dense (None, 64)                105280    \n",
      "_________________________________________________________________\n",
      "Dropout0 (Dropout)           (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_144 (Dense)            (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 105,345\n",
      "Trainable params: 105,345\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "DNN.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance of Read DNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-12T04:17:42.037918Z",
     "start_time": "2019-12-12T04:17:41.946526Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         NR.AhR: 0.85306\n"
     ]
    }
   ],
   "source": [
    "rows_tr = np.isfinite(y_tr[target]).values\n",
    "rows_te = np.isfinite(y_te[target]).values\n",
    "y_testing=y_te[target][rows_te]\n",
    "p_te = DNN.predict(x_te[rows_te])[:,0]\n",
    "auc_te = roc_auc_score(y_te[target][rows_te], p_te)\n",
    "average_precision=average_precision_score(y_testing, p_te)\n",
    "print(\"%15s: %3.5f\" % (target, auc_te))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-12T04:17:42.830809Z",
     "start_time": "2019-12-12T04:17:42.799769Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model                                  DNN\n",
       "threshold                              0.5\n",
       "accuracy                               0.8\n",
       "precision                         0.347826\n",
       "recall                            0.767123\n",
       "f1                                0.478632\n",
       "auc_roc                           0.853065\n",
       "mcc                               0.420919\n",
       "avg_precision                      0.49692\n",
       "confusion_matrix    [[432, 105], [17, 56]]\n",
       "dtype: object"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat_testing=DNN.predict_classes(x_te[rows_te])\n",
    "evaluate_model_predictions(target, 'DNN', 0.5, y_testing, y_hat_testing, auc_te, average_precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-12T04:17:44.378647Z",
     "start_time": "2019-12-12T04:17:44.375957Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-12T04:17:45.282082Z",
     "start_time": "2019-12-12T04:17:45.140895Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_te[target][rows_te], p_te)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "         lw=2, label='ROC curve (area = %0.3f)' % auc_te)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title(target+' Receiver operating characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix at Alternate Decision Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-12T04:17:47.633590Z",
     "start_time": "2019-12-12T04:17:47.615784Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision-Recall Curve \"Average Precision\": 0.4969\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "model                                 DNN\n",
       "threshold                          0.7035\n",
       "accuracy                         0.893443\n",
       "precision                        0.547619\n",
       "recall                           0.630137\n",
       "f1                               0.585987\n",
       "auc_roc                          0.853065\n",
       "mcc                              0.526888\n",
       "avg_precision                     0.49692\n",
       "confusion_matrix    [[499, 38], [27, 46]]\n",
       "dtype: object"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decision_threshold = 0.7035\n",
    "y_testing=y_te[target][rows_te]\n",
    "y_hat_testing_adj=adjusted_classes(p_te,decision_threshold)\n",
    "print('Precision-Recall Curve \"Average Precision\": %0.4f' %average_precision)\n",
    "mv=evaluate_model_predictions(target,'DNN',decision_threshold,y_testing,y_hat_testing_adj,\\\n",
    "                              auc_te,average_precision)\n",
    "mv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision-Recall Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-12T04:17:49.581336Z",
     "start_time": "2019-12-12T04:17:49.573770Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max F1: 0.59494, threshold prob: 0.70347\n",
      "Max F1 Precision: 0.55294, Recall: 0.64384\n"
     ]
    }
   ],
   "source": [
    "precision, recall, thresholds = precision_recall_curve(y_testing,p_te)\n",
    "precision, recall, thresholds = zip(*[i for i in zip(precision,recall,np.append(thresholds,1)) if i[0:2]!=(0,0)])\n",
    "precision, recall, thresholds = np.array(precision),np.array(recall),np.array(thresholds)\n",
    "f1 = 2*precision*recall/(precision+recall)\n",
    "m_idx = np.argmax(f1)\n",
    "m_thresh = thresholds[m_idx]\n",
    "print('Max F1: %0.5f, threshold prob: %0.5f' % (f1[m_idx], m_thresh))\n",
    "print('Max F1 Precision: %0.5f, Recall: %0.5f' % (precision[m_idx],recall[m_idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-12T04:17:51.086555Z",
     "start_time": "2019-12-12T04:17:51.081812Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17328140139579773"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thresholds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-12T04:17:52.542525Z",
     "start_time": "2019-12-12T04:17:51.756083Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sean/anaconda3/envs/metis/lib/python3.7/site-packages/sklearn/metrics/classification.py:872: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
     ]
    }
   ],
   "source": [
    "mccs = []\n",
    "for th in thresholds:\n",
    "    y_hat_testing_adj=adjusted_classes(p_te,th)\n",
    "    mccs.append(matthews_corrcoef(y_testing,y_hat_testing_adj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-12T04:18:42.440205Z",
     "start_time": "2019-12-12T04:18:42.067279Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "no_skill = len(y_testing[y_testing==1]) / len(y_testing)\n",
    "fig = plt.figure(figsize=[8,8*4.8/6.4])\n",
    "plt.step(recall, precision, color='crimson', alpha=0.3, where='post')\n",
    "plt.fill_between(recall, precision, step='post', alpha=0.3, color='crimson',\\\n",
    "                 label='DNN')\n",
    "plt.plot([0, 1], [no_skill, no_skill], color='r', linestyle='--', label='No Skill:'+' %0.3f' % no_skill)\n",
    "plt.plot([0, 1], [precision[m_idx],precision[m_idx]], color='k', alpha=0.4, linestyle=':')\n",
    "plt.plot([recall[m_idx],recall[m_idx]],[0, 1], color='k', alpha=0.4,linestyle=':',\\\n",
    "         label='Prec/Rec @ Max F1')\n",
    "plt.text(recall[m_idx], f1[m_idx]+0.01, 'Max F1={0:0.3f}'.format(f1[m_idx]))\n",
    "plt.plot(recall,f1,color='k',label='F1')\n",
    "plt.plot(recall,mccs,label=\"Matthew's Corr Coef\")\n",
    "plt.xlabel('Recall', fontsize=14)\n",
    "plt.ylabel('Precision or F1 Score', fontsize=14)\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.legend()\n",
    "title=target+' Neural Network\\n Precision-Recall curve: AP={0:0.2f}'.format(\n",
    "          average_precision)\n",
    "plt.title(title, fontsize=16);\n",
    "fig.savefig('./reports/figures/'+target+'_DNN_PrecisionRecallCurve2.svg',\\\n",
    "            format='svg', dpi=1200, transparent=True, bbox_inches = \"tight\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T09:09:41.402783Z",
     "start_time": "2019-12-09T09:09:41.392980Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.16704805491990846, 1.0, 0.28627450980392155, 0.23198324564311515, 0.17328140139579773)\n",
      "(0.1651376146788991, 0.9863013698630136, 0.2829076620825148, 0.22173369633779677, 0.17465224862098694)\n",
      "(0.16551724137931034, 0.9863013698630136, 0.28346456692913385, 0.2226895755682889, 0.1813332438468933)\n",
      "(0.16589861751152074, 0.9863013698630136, 0.28402366863905326, 0.22364576427189672, 0.1842179298400879)\n",
      "(0.16628175519630484, 0.9863013698630136, 0.2845849802371542, 0.2246022885079155, 0.18537762761116028)\n",
      "(0.16511627906976745, 0.9726027397260274, 0.2823061630218688, 0.216399972952515, 0.18541648983955383)\n",
      "(0.1655011655011655, 0.9726027397260274, 0.28286852589641437, 0.2173758670266673, 0.18947604298591614)\n",
      "(0.1658878504672897, 0.9726027397260274, 0.2834331337325349, 0.2183519976700012, 0.1915532648563385)\n",
      "(0.16627634660421545, 0.9726027397260274, 0.284, 0.21932839276507035, 0.19245806336402893)\n",
      "(0.16666666666666666, 0.9726027397260274, 0.2845691382765531, 0.22030508001372517, 0.19489645957946777)\n",
      "(0.16705882352941176, 0.9726027397260274, 0.285140562248996, 0.22128208694377893, 0.19898191094398499)\n",
      "(0.16745283018867924, 0.9726027397260274, 0.2857142857142857, 0.2222594409155313, 0.20502963662147522)\n",
      "(0.16784869976359337, 0.9726027397260274, 0.28629032258064513, 0.22323716912815372, 0.21005719900131226)\n",
      "(0.16587677725118483, 0.958904109589041, 0.2828282828282828, 0.21327709227902755, 0.21884813904762268)\n",
      "(0.166270783847981, 0.958904109589041, 0.2834008097165992, 0.21427167674288639, 0.21893590688705444)\n",
      "(0.16666666666666666, 0.958904109589041, 0.2839756592292089, 0.21526650922957605, 0.22046548128128052)\n",
      "(0.16467780429594273, 0.9452054794520548, 0.2804878048780488, 0.20537087420303693, 0.22505328059196472)\n",
      "(0.16507177033492823, 0.9452054794520548, 0.28105906313645623, 0.2063817021262738, 0.22538992762565613)\n",
      "(0.16546762589928057, 0.9452054794520548, 0.2816326530612245, 0.20739266406115034, 0.22658228874206543)\n",
      "(0.1658653846153846, 0.9452054794520548, 0.2822085889570552, 0.20840379054879396, 0.22743678092956543)\n",
      "(0.16626506024096385, 0.9452054794520548, 0.2827868852459016, 0.20941511193011525, 0.2280375361442566)\n",
      "(0.16666666666666666, 0.9452054794520548, 0.28336755646817247, 0.21042665835305346, 0.2308761477470398)\n",
      "(0.16707021791767554, 0.9452054794520548, 0.2839506172839506, 0.21143845977967277, 0.230894535779953)\n",
      "(0.16747572815533981, 0.9452054794520548, 0.2845360824742268, 0.21245054599311503, 0.23091742396354675)\n",
      "(0.1678832116788321, 0.9452054794520548, 0.28512396694214875, 0.21346294660441392, 0.23093682527542114)\n",
      "(0.16829268292682928, 0.9452054794520548, 0.28571428571428575, 0.2144756910591756, 0.23449993133544922)\n",
      "(0.1687041564792176, 0.9452054794520548, 0.28630705394190875, 0.21548880864413, 0.2361835241317749)\n",
      "(0.16911764705882354, 0.9452054794520548, 0.28690228690228686, 0.2165023284935588, 0.24313059449195862)\n",
      "(0.16953316953316952, 0.9452054794520548, 0.2875, 0.2175162795956027, 0.24752092361450195)\n",
      "(0.16995073891625614, 0.9452054794520548, 0.2881002087682672, 0.21853069079845414, 0.2511894702911377)\n",
      "(0.17037037037037037, 0.9452054794520548, 0.2887029288702929, 0.21954559081643807, 0.2520563304424286)\n",
      "(0.1707920792079208, 0.9452054794520548, 0.289308176100629, 0.2205610082359862, 0.25265181064605713)\n",
      "(0.17121588089330025, 0.9452054794520548, 0.28991596638655465, 0.2215769715215076, 0.25850510597229004)\n",
      "(0.17164179104477612, 0.9452054794520548, 0.2905263157894737, 0.2225935090211599, 0.26115885376930237)\n",
      "(0.17336683417085427, 0.9452054794520548, 0.2929936305732484, 0.22666596437071188, 0.2611609101295471)\n",
      "(0.17380352644836272, 0.9452054794520548, 0.29361702127659567, 0.2276857944868091, 0.26182693243026733)\n",
      "(0.17424242424242425, 0.9452054794520548, 0.2942430703624734, 0.22870636677627437, 0.2626449763774872)\n",
      "(0.17468354430379746, 0.9452054794520548, 0.2948717948717948, 0.22972770892749622, 0.26353734731674194)\n",
      "(0.1751269035532995, 0.9452054794520548, 0.29550321199143476, 0.23074984855562236, 0.2664661407470703)\n",
      "(0.17557251908396945, 0.9452054794520548, 0.296137339055794, 0.23177281320758947, 0.2691808342933655)\n",
      "(0.1760204081632653, 0.9452054794520548, 0.29677419354838713, 0.23279663036708545, 0.2701396942138672)\n",
      "(0.17647058823529413, 0.9452054794520548, 0.2974137931034483, 0.23382132745944795, 0.27947187423706055)\n",
      "(0.17692307692307693, 0.9452054794520548, 0.2980561555075594, 0.2348469318565005, 0.28066617250442505)\n",
      "(0.17737789203084833, 0.9452054794520548, 0.2987012987012987, 0.2358734708813301, 0.2826305031776428)\n",
      "(0.17783505154639176, 0.9452054794520548, 0.299349240780911, 0.23690097181300804, 0.2844478189945221)\n",
      "(0.17829457364341086, 0.9452054794520548, 0.3, 0.2379294618912565, 0.2878362834453583)\n",
      "(0.17922077922077922, 0.9452054794520548, 0.3013100436681223, 0.23998951827724943, 0.28785502910614014)\n",
      "(0.1796875, 0.9452054794520548, 0.30196936542669583, 0.24102113890898402, 0.28961360454559326)\n",
      "(0.18110236220472442, 0.9452054794520548, 0.3039647577092512, 0.24412269605809403, 0.29591673612594604)\n",
      "(0.18157894736842106, 0.9452054794520548, 0.30463576158940403, 0.24515887052643942, 0.29658931493759155)\n",
      "(0.1820580474934037, 0.9452054794520548, 0.30530973451327437, 0.2461962511865791, 0.3028804063796997)\n",
      "(0.18253968253968253, 0.9452054794520548, 0.30598669623059865, 0.24723486512631915, 0.30307096242904663)\n",
      "(0.1830238726790451, 0.9452054794520548, 0.30666666666666664, 0.24827473943831163, 0.30376410484313965)\n",
      "(0.18351063829787234, 0.9452054794520548, 0.30734966592427615, 0.24931590122427322, 0.30378958582878113)\n",
      "(0.184, 0.9452054794520548, 0.3080357142857143, 0.2503583775991756, 0.31390249729156494)\n",
      "(0.18449197860962566, 0.9452054794520548, 0.30872483221476504, 0.25140219569540917, 0.3148218095302582)\n",
      "(0.18498659517426275, 0.9452054794520548, 0.3094170403587444, 0.2524473826669214, 0.3149172365665436)\n",
      "(0.18548387096774194, 0.9452054794520548, 0.3101123595505618, 0.25349396569333293, 0.32115602493286133)\n",
      "(0.18598382749326145, 0.9452054794520548, 0.3108108108108108, 0.25454197198403106, 0.3213396668434143)\n",
      "(0.1837837837837838, 0.9315068493150684, 0.30699774266365687, 0.2452525182015308, 0.3221105933189392)\n",
      "(0.1842818428184282, 0.9315068493150684, 0.30769230769230776, 0.24631095429795052, 0.32274746894836426)\n",
      "(0.18478260869565216, 0.9315068493150684, 0.30839002267573695, 0.24737076327577912, 0.3255764842033386)\n",
      "(0.19101123595505617, 0.9315068493150684, 0.317016317016317, 0.2602058511245016, 0.327288955450058)\n",
      "(0.192090395480226, 0.9315068493150684, 0.3185011709601874, 0.2623680951392541, 0.3273174464702606)\n",
      "(0.19263456090651557, 0.9315068493150684, 0.31924882629107976, 0.2634519038366168, 0.3282669484615326)\n",
      "(0.19318181818181818, 0.9315068493150684, 0.32, 0.26453754243433975, 0.335477352142334)\n",
      "(0.19428571428571428, 0.9315068493150684, 0.3215130023640662, 0.26671442622519625, 0.3356223702430725)\n",
      "(0.19484240687679083, 0.9315068493150684, 0.3222748815165877, 0.2678057301189512, 0.33631378412246704)\n",
      "(0.19653179190751446, 0.9315068493150684, 0.324582338902148, 0.27109144426703724, 0.3374090790748596)\n",
      "(0.19710144927536233, 0.9315068493150684, 0.3253588516746411, 0.2721907156066147, 0.3415788412094116)\n",
      "(0.19767441860465115, 0.9315068493150684, 0.32613908872901676, 0.2732920534625334, 0.34907305240631104)\n",
      "(0.19825072886297376, 0.9315068493150684, 0.3269230769230769, 0.2743954879488086, 0.35145437717437744)\n",
      "(0.19883040935672514, 0.9315068493150684, 0.32771084337349393, 0.2755010493129492, 0.3523406684398651)\n",
      "(0.19941348973607037, 0.9315068493150684, 0.32850241545893716, 0.27660876794025185, 0.3578851521015167)\n",
      "(0.2, 0.9315068493150684, 0.3292978208232446, 0.2777186743581163, 0.35950174927711487)\n",
      "(0.20058997050147492, 0.9315068493150684, 0.3300970873786408, 0.27883079924038284, 0.3601006865501404)\n",
      "(0.20118343195266272, 0.9315068493150684, 0.3309002433090025, 0.2799451734116934, 0.3634076714515686)\n",
      "(0.20178041543026706, 0.9315068493150684, 0.3317073170731707, 0.2810618278518785, 0.36704957485198975)\n",
      "(0.22388059701492538, 0.821917808219178, 0.3519061583577713, 0.2842095116928305, 0.3719310760498047)\n",
      "(0.2247191011235955, 0.821917808219178, 0.35294117647058826, 0.28554421284083487, 0.3727127015590668)\n",
      "(0.22556390977443608, 0.821917808219178, 0.3539823008849557, 0.28688319052989053, 0.3776320517063141)\n",
      "(0.22641509433962265, 0.821917808219178, 0.35502958579881655, 0.2882264987905576, 0.37996989488601685)\n",
      "(0.22727272727272727, 0.821917808219178, 0.3560830860534125, 0.2895741922240921, 0.381315678358078)\n",
      "(0.22813688212927757, 0.821917808219178, 0.35714285714285715, 0.2909263260137553, 0.3820660412311554)\n",
      "(0.22900763358778625, 0.821917808219178, 0.3582089552238806, 0.2922829559363349, 0.38274821639060974)\n",
      "(0.22988505747126436, 0.821917808219178, 0.35928143712574856, 0.2936441383738822, 0.38496947288513184)\n",
      "(0.23076923076923078, 0.821917808219178, 0.36036036036036034, 0.2950099303256722, 0.3851257860660553)\n",
      "(0.23166023166023167, 0.821917808219178, 0.3614457831325301, 0.29638038942039235, 0.385318398475647)\n",
      "(0.23255813953488372, 0.821917808219178, 0.36253776435045315, 0.29775557392856566, 0.3880103528499603)\n",
      "(0.22957198443579765, 0.8082191780821918, 0.35757575757575755, 0.2889066801095456, 0.3955206274986267)\n",
      "(0.23046875, 0.8082191780821918, 0.3586626139817629, 0.2902860200856487, 0.39750370383262634)\n",
      "(0.23137254901960785, 0.8082191780821918, 0.35975609756097565, 0.29167014235656474, 0.40052223205566406)\n",
      "(0.23228346456692914, 0.8082191780821918, 0.3608562691131499, 0.2930591073083078, 0.40716466307640076)\n",
      "(0.233201581027668, 0.8082191780821918, 0.36196319018404904, 0.29445297602014026, 0.40972521901130676)\n",
      "(0.23412698412698413, 0.8082191780821918, 0.3630769230769231, 0.2958518102783651, 0.4136120080947876)\n",
      "(0.2350597609561753, 0.8082191780821918, 0.3641975308641976, 0.29725567259039654, 0.41694873571395874)\n",
      "(0.236, 0.8082191780821918, 0.3653250773993808, 0.29866462619911543, 0.4180801510810852)\n",
      "(0.23694779116465864, 0.8082191780821918, 0.36645962732919257, 0.3000787350975182, 0.41919639706611633)\n",
      "(0.23577235772357724, 0.7945205479452054, 0.3636363636363637, 0.2940567758257069, 0.42404794692993164)\n",
      "(0.23265306122448978, 0.7808219178082192, 0.3584905660377359, 0.2851825957360278, 0.43023058772087097)\n",
      "(0.296875, 0.7808219178082192, 0.43018867924528303, 0.37001087127925764, 0.4336148798465729)\n",
      "(0.29842931937172773, 0.7808219178082192, 0.43181818181818177, 0.3718385809811919, 0.43379145860671997)\n",
      "(0.3, 0.7808219178082192, 0.4334600760456273, 0.37367691386197166, 0.43380308151245117)\n",
      "(0.30158730158730157, 0.7808219178082192, 0.43511450381679384, 0.37552601958122805, 0.4338268041610718)\n",
      "(0.30319148936170215, 0.7808219178082192, 0.43678160919540227, 0.37738605045438156, 0.44090986251831055)\n",
      "(0.3048128342245989, 0.7808219178082192, 0.4384615384615385, 0.379257161516972, 0.44210466742515564)\n",
      "(0.3064516129032258, 0.7808219178082192, 0.44015444015444016, 0.3811395105908674, 0.44248050451278687)\n",
      "(0.3081081081081081, 0.7808219178082192, 0.44186046511627913, 0.38303325835241836, 0.4425780475139618)\n",
      "(0.30978260869565216, 0.7808219178082192, 0.443579766536965, 0.3849385684026259, 0.44821488857269287)\n",
      "(0.3131868131868132, 0.7808219178082192, 0.4470588235294118, 0.3887845448319498, 0.4482244551181793)\n",
      "(0.3149171270718232, 0.7808219178082192, 0.44881889763779526, 0.39072555369747913, 0.44839003682136536)\n",
      "(0.31666666666666665, 0.7808219178082192, 0.450592885375494, 0.392678809980109, 0.44890159368515015)\n",
      "(0.31843575418994413, 0.7808219178082192, 0.4523809523809524, 0.3946444930322696, 0.44973453879356384)\n",
      "(0.3202247191011236, 0.7808219178082192, 0.4541832669322709, 0.3966227855985539, 0.450889527797699)\n",
      "(0.3220338983050847, 0.7808219178082192, 0.45599999999999996, 0.39861387390215625, 0.45164141058921814)\n",
      "(0.32386363636363635, 0.7808219178082192, 0.45783132530120485, 0.4006179477339859, 0.45217472314834595)\n",
      "(0.32571428571428573, 0.7808219178082192, 0.45967741935483875, 0.4026352005445552, 0.45397713780403137)\n",
      "(0.3275862068965517, 0.7808219178082192, 0.46153846153846156, 0.4046658295387445, 0.45853912830352783)\n",
      "(0.32947976878612717, 0.7808219178082192, 0.46341463414634143, 0.4067100357735539, 0.4633287787437439)\n",
      "(0.3313953488372093, 0.7808219178082192, 0.46530612244897956, 0.4087680242589512, 0.4638950824737549)\n",
      "(0.3333333333333333, 0.7808219178082192, 0.4672131147540984, 0.41084000406193644, 0.46691304445266724)\n",
      "(0.3352941176470588, 0.7808219178082192, 0.4691358024691358, 0.41292618841394346, 0.47025978565216064)\n",
      "(0.33727810650887574, 0.7808219178082192, 0.47107438016528924, 0.4150267948217078, 0.47523051500320435)\n",
      "(0.3413173652694611, 0.7808219178082192, 0.4750000000000001, 0.41927216589849775, 0.4752364754676819)\n",
      "(0.3433734939759036, 0.7808219178082192, 0.4769874476987447, 0.42141738800654555, 0.4776079058647156)\n",
      "(0.34545454545454546, 0.7808219178082192, 0.4789915966386555, 0.4235779472966141, 0.47870761156082153)\n",
      "(0.3475609756097561, 0.7808219178082192, 0.48101265822784806, 0.42575408444596025, 0.4796389937400818)\n",
      "(0.34355828220858897, 0.7671232876712328, 0.47457627118644063, 0.4165321547440876, 0.4827778935432434)\n",
      "(0.345679012345679, 0.7671232876712328, 0.4765957446808511, 0.4187178011201899, 0.4845709800720215)\n",
      "(0.34782608695652173, 0.7671232876712328, 0.4786324786324786, 0.42091948729564005, 0.5028659105300903)\n",
      "(0.35, 0.7671232876712328, 0.48068669527896984, 0.4231374707334297, 0.5120404958724976)\n",
      "(0.3522012578616352, 0.7671232876712328, 0.48275862068965514, 0.42537201440845973, 0.5121070742607117)\n",
      "(0.35443037974683544, 0.7671232876712328, 0.48484848484848486, 0.4276233869641479, 0.5136734843254089)\n",
      "(0.35668789808917195, 0.7671232876712328, 0.48695652173913045, 0.4298918628744842, 0.5142155885696411)\n",
      "(0.358974358974359, 0.7671232876712328, 0.48908296943231433, 0.4321777226117572, 0.5142219662666321)\n",
      "(0.36129032258064514, 0.7671232876712328, 0.4912280701754385, 0.4344812528201906, 0.5179610252380371)\n",
      "(0.36363636363636365, 0.7671232876712328, 0.49339207048458145, 0.43680274649573786, 0.5200366973876953)\n",
      "(0.3660130718954248, 0.7671232876712328, 0.49557522123893805, 0.439142503172294, 0.5200368165969849)\n",
      "(0.3618421052631579, 0.7534246575342466, 0.48888888888888893, 0.42982395148019603, 0.5264448523521423)\n",
      "(0.36423841059602646, 0.7534246575342466, 0.49107142857142855, 0.4321753274482815, 0.5319575071334839)\n",
      "(0.36666666666666664, 0.7534246575342466, 0.4932735426008969, 0.4345455640235713, 0.5340980291366577)\n",
      "(0.3691275167785235, 0.7534246575342466, 0.4954954954954955, 0.4369349835255295, 0.5355691313743591)\n",
      "(0.3716216216216216, 0.7534246575342466, 0.4977375565610859, 0.43934391571820525, 0.5400340557098389)\n",
      "(0.3741496598639456, 0.7534246575342466, 0.5, 0.4417726980372222, 0.5410366654396057)\n",
      "(0.3698630136986301, 0.7397260273972602, 0.4931506849315068, 0.43238456145109644, 0.5411461591720581)\n",
      "(0.3724137931034483, 0.7397260273972602, 0.4954128440366973, 0.43482611946858185, 0.5431700944900513)\n",
      "(0.375, 0.7397260273972602, 0.4976958525345623, 0.43728821191848627, 0.5432511568069458)\n",
      "(0.3776223776223776, 0.7397260273972602, 0.5, 0.4397712031544869, 0.5433512330055237)\n",
      "(0.38028169014084506, 0.7397260273972602, 0.5023255813953488, 0.44227546631508113, 0.5433924794197083)\n",
      "(0.3829787234042553, 0.7397260273972602, 0.5046728971962616, 0.4448013836026574, 0.5435643196105957)\n",
      "(0.38571428571428573, 0.7397260273972602, 0.5070422535211268, 0.4473493465734601, 0.5491951704025269)\n",
      "(0.38848920863309355, 0.7397260273972602, 0.5094339622641509, 0.4499197564389544, 0.5498624444007874)\n",
      "(0.391304347826087, 0.7397260273972602, 0.5118483412322274, 0.4525130243791265, 0.5549741387367249)\n",
      "(0.39416058394160586, 0.7397260273972602, 0.5142857142857143, 0.4551295718682841, 0.5551242828369141)\n",
      "(0.39705882352941174, 0.7397260273972602, 0.5167464114832535, 0.45776983101394964, 0.5551245808601379)\n",
      "(0.4, 0.7397260273972602, 0.5192307692307693, 0.46043424490947676, 0.5551498532295227)\n",
      "(0.40298507462686567, 0.7397260273972602, 0.5217391304347826, 0.46312326800105114, 0.558928906917572)\n",
      "(0.40601503759398494, 0.7397260273972602, 0.5242718446601942, 0.46583736646977547, 0.5596955418586731)\n",
      "(0.4015151515151515, 0.726027397260274, 0.5170731707317072, 0.4563116572596843, 0.5599369406700134)\n",
      "(0.40458015267175573, 0.726027397260274, 0.5196078431372549, 0.45904348719619065, 0.5644659399986267)\n",
      "(0.4, 0.7123287671232876, 0.5123152709359605, 0.4494678111186665, 0.5647518038749695)\n",
      "(0.40310077519379844, 0.7123287671232876, 0.5148514851485148, 0.452217451469057, 0.5698819756507874)\n",
      "(0.40625, 0.7123287671232876, 0.5174129353233831, 0.4549936928788712, 0.5821796655654907)\n",
      "(0.4094488188976378, 0.7123287671232876, 0.52, 0.4577970619991535, 0.5833263397216797)\n",
      "(0.4126984126984127, 0.7123287671232876, 0.5226130653266331, 0.46062809984755, 0.5848428606987)\n",
      "(0.416, 0.7123287671232876, 0.5252525252525253, 0.46348736232246696, 0.5879695415496826)\n",
      "(0.41935483870967744, 0.7123287671232876, 0.5279187817258884, 0.46637542073989785, 0.5905325412750244)\n",
      "(0.42276422764227645, 0.7123287671232876, 0.5306122448979592, 0.4692928623941122, 0.5988749265670776)\n",
      "(0.4180327868852459, 0.6986301369863014, 0.5230769230769231, 0.45961354539097243, 0.5992032885551453)\n",
      "(0.4214876033057851, 0.6986301369863014, 0.5257731958762886, 0.4625524836528188, 0.6000864505767822)\n",
      "(0.425, 0.6986301369863014, 0.5284974093264249, 0.4655220873514139, 0.6016438603401184)\n",
      "(0.42857142857142855, 0.6986301369863014, 0.53125, 0.4685230019448543, 0.605911910533905)\n",
      "(0.4322033898305085, 0.6986301369863014, 0.5340314136125655, 0.47155589172162327, 0.607475757598877)\n",
      "(0.4358974358974359, 0.6986301369863014, 0.5368421052631579, 0.4746214405202779, 0.6074758172035217)\n",
      "(0.4396551724137931, 0.6986301369863014, 0.5396825396825397, 0.4777203524830585, 0.607756495475769)\n",
      "(0.4434782608695652, 0.6986301369863014, 0.5425531914893617, 0.48085335284532826, 0.6141749620437622)\n",
      "(0.4473684210526316, 0.6986301369863014, 0.5454545454545455, 0.48402118876287875, 0.6158145666122437)\n",
      "(0.45132743362831856, 0.6986301369863014, 0.5483870967741936, 0.4872246301792675, 0.615862250328064)\n",
      "(0.44642857142857145, 0.684931506849315, 0.5405405405405406, 0.4774190653483623, 0.6209880709648132)\n",
      "(0.44144144144144143, 0.6712328767123288, 0.532608695652174, 0.4675597290529613, 0.6261598467826843)\n",
      "(0.44545454545454544, 0.6712328767123288, 0.5355191256830601, 0.47078242319401425, 0.6312482357025146)\n",
      "(0.44954128440366975, 0.6712328767123288, 0.5384615384615385, 0.47404258074605204, 0.6399688720703125)\n",
      "(0.4537037037037037, 0.6712328767123288, 0.5414364640883979, 0.4773410668274493, 0.6504337191581726)\n",
      "(0.45794392523364486, 0.6712328767123288, 0.5444444444444444, 0.48067877443095997, 0.6526235342025757)\n",
      "(0.46226415094339623, 0.6712328767123288, 0.5474860335195532, 0.48405662559904206, 0.6549820899963379)\n",
      "(0.4666666666666667, 0.6712328767123288, 0.550561797752809, 0.4874755726603849, 0.6602199077606201)\n",
      "(0.47115384615384615, 0.6712328767123288, 0.5536723163841807, 0.49093659953144553, 0.663005530834198)\n",
      "(0.47572815533980584, 0.6712328767123288, 0.5568181818181819, 0.49444072308707887, 0.6661838293075562)\n",
      "(0.4803921568627451, 0.6712328767123288, 0.5599999999999999, 0.4979889946046414, 0.6677386164665222)\n",
      "(0.48514851485148514, 0.6712328767123288, 0.5632183908045977, 0.5015825012862766, 0.6680070161819458)\n",
      "(0.49, 0.6712328767123288, 0.5664739884393063, 0.5052223678644394, 0.6685200333595276)\n",
      "(0.494949494949495, 0.6712328767123288, 0.569767441860465, 0.5089097582960982, 0.6685491800308228)\n",
      "(0.5, 0.6712328767123288, 0.5730994152046783, 0.5126458775514693, 0.6696735620498657)\n",
      "(0.5051546391752577, 0.6712328767123288, 0.5764705882352941, 0.5164319735035874, 0.67081618309021)\n",
      "(0.5104166666666666, 0.6712328767123288, 0.5798816568047337, 0.5202693389255078, 0.6714624166488647)\n",
      "(0.5157894736842106, 0.6712328767123288, 0.5833333333333334, 0.5241593136024689, 0.6749551296234131)\n",
      "(0.5212765957446809, 0.6712328767123288, 0.5868263473053892, 0.5281032865669272, 0.6763989925384521)\n",
      "(0.5268817204301075, 0.6712328767123288, 0.5903614457831325, 0.5321026984650106, 0.6812368631362915)\n",
      "(0.5217391304347826, 0.6575342465753424, 0.5818181818181818, 0.5220459424463529, 0.6812376976013184)\n",
      "(0.5274725274725275, 0.6575342465753424, 0.5853658536585367, 0.5260971181786139, 0.6918179988861084)\n",
      "(0.5333333333333333, 0.6575342465753424, 0.588957055214724, 0.5302072168374239, 0.6931205987930298)\n",
      "(0.5393258426966292, 0.6575342465753424, 0.5925925925925926, 0.5343778794974818, 0.6961268782615662)\n",
      "(0.5340909090909091, 0.6438356164383562, 0.5838509316770186, 0.5242359169169968, 0.699643611907959)\n",
      "(0.5402298850574713, 0.6438356164383562, 0.5875, 0.5284643417793875, 0.7009394764900208)\n",
      "(0.5465116279069767, 0.6438356164383562, 0.5911949685534591, 0.5327573419224498, 0.7015763521194458)\n",
      "(0.5529411764705883, 0.6438356164383562, 0.5949367088607594, 0.5371167987126113, 0.703474760055542)\n",
      "(0.5476190476190477, 0.6301369863013698, 0.5859872611464968, 0.5268875474612481, 0.7047172784805298)\n",
      "(0.5421686746987951, 0.6164383561643836, 0.576923076923077, 0.5165806816519808, 0.7116811871528625)\n",
      "(0.5365853658536586, 0.6027397260273972, 0.567741935483871, 0.5061939017981382, 0.7117826342582703)\n",
      "(0.5308641975308642, 0.589041095890411, 0.5584415584415585, 0.4957248182676853, 0.7145785093307495)\n",
      "(0.5375, 0.589041095890411, 0.5620915032679739, 0.5001332455961321, 0.7175037860870361)\n",
      "(0.5443037974683544, 0.589041095890411, 0.5657894736842106, 0.5046147317167998, 0.7195552587509155)\n",
      "(0.5512820512820513, 0.589041095890411, 0.5695364238410597, 0.5091715920181218, 0.7261609435081482)\n",
      "(0.5584415584415584, 0.589041095890411, 0.5733333333333333, 0.5138062458110418, 0.7385987043380737)\n",
      "(0.5526315789473685, 0.5753424657534246, 0.563758389261745, 0.5032278298380919, 0.7396842241287231)\n",
      "(0.56, 0.5753424657534246, 0.5675675675675674, 0.5079385514567926, 0.7407654523849487)\n",
      "(0.5540540540540541, 0.5616438356164384, 0.5578231292517007, 0.4972633719731432, 0.743514895439148)\n",
      "(0.5616438356164384, 0.5616438356164384, 0.5616438356164384, 0.5020535190428815, 0.7512584924697876)\n",
      "(0.5555555555555556, 0.547945205479452, 0.5517241379310345, 0.491277332912799, 0.7515912055969238)\n",
      "(0.5633802816901409, 0.547945205479452, 0.5555555555555555, 0.4961505307570091, 0.7522017955780029)\n",
      "(0.5714285714285714, 0.547945205479452, 0.5594405594405595, 0.5011154061974199, 0.7522019147872925)\n",
      "(0.5588235294117647, 0.5205479452054794, 0.5390070921985816, 0.4792370046719041, 0.7522023320198059)\n",
      "(0.5522388059701493, 0.5068493150684932, 0.5285714285714286, 0.4681357486130678, 0.7537569999694824)\n",
      "(0.5606060606060606, 0.5068493150684932, 0.5323741007194245, 0.4731810010325658, 0.7537797689437866)\n",
      "(0.5692307692307692, 0.5068493150684932, 0.536231884057971, 0.47832825567542336, 0.753780722618103)\n",
      "(0.578125, 0.5068493150684932, 0.5401459854014599, 0.48358143814563354, 0.7537810802459717)\n",
      "(0.5873015873015873, 0.5068493150684932, 0.5441176470588236, 0.4889446893084347, 0.7537811398506165)\n",
      "(0.5806451612903226, 0.4931506849315068, 0.5333333333333333, 0.47770781348396407, 0.7537870407104492)\n",
      "(0.5737704918032787, 0.4794520547945205, 0.5223880597014925, 0.46634780979230545, 0.754169225692749)\n",
      "(0.5833333333333334, 0.4794520547945205, 0.5263157894736842, 0.471819949156107, 0.7552183866500854)\n",
      "(0.5932203389830508, 0.4794520547945205, 0.5303030303030303, 0.47741456662462617, 0.7633869647979736)\n",
      "(0.5862068965517241, 0.4657534246575342, 0.5190839694656488, 0.4659182608758292, 0.7639918923377991)\n",
      "(0.5789473684210527, 0.4520547945205479, 0.5076923076923077, 0.4542857922397978, 0.7660188674926758)\n",
      "(0.5892857142857143, 0.4520547945205479, 0.5116279069767442, 0.46000340164634734, 0.7682046890258789)\n",
      "(0.6, 0.4520547945205479, 0.515625, 0.46585836768084427, 0.7785959839820862)\n",
      "(0.6111111111111112, 0.4520547945205479, 0.5196850393700787, 0.47185694015035906, 0.7831449508666992)\n",
      "(0.6037735849056604, 0.4383561643835616, 0.507936507936508, 0.4600743128476988, 0.7891136407852173)\n",
      "(0.6153846153846154, 0.4383561643835616, 0.512, 0.46622514312940966, 0.7992188930511475)\n",
      "(0.6078431372549019, 0.4246575342465753, 0.4999999999999999, 0.45428921649007115, 0.7994481325149536)\n",
      "(0.6, 0.410958904109589, 0.4878048780487805, 0.4421911395490339, 0.7995601892471313)\n",
      "(0.5918367346938775, 0.3972602739726027, 0.47540983606557374, 0.4299236121530034, 0.7995797395706177)\n",
      "(0.6041666666666666, 0.3972602739726027, 0.4793388429752066, 0.43623710856590525, 0.7995799779891968)\n",
      "(0.5957446808510638, 0.3835616438356164, 0.4666666666666667, 0.4237884678175238, 0.7995803952217102)\n",
      "(0.5869565217391305, 0.3698630136986301, 0.453781512605042, 0.41115151968174446, 0.799659252166748)\n",
      "(0.6, 0.3698630136986301, 0.45762711864406774, 0.4176390319369136, 0.8004468083381653)\n",
      "(0.5909090909090909, 0.3561643835616438, 0.4444444444444445, 0.404798346803754, 0.8010056614875793)\n",
      "(0.5813953488372093, 0.3424657534246575, 0.43103448275862066, 0.39174699518237865, 0.8020696043968201)\n",
      "(0.5714285714285714, 0.3287671232876712, 0.41739130434782606, 0.3784740365125331, 0.8085611462593079)\n",
      "(0.5853658536585366, 0.3287671232876712, 0.42105263157894735, 0.3851389490657178, 0.8114547729492188)\n",
      "(0.575, 0.3150684931506849, 0.4070796460176991, 0.3716191302286109, 0.8154312372207642)\n",
      "(0.5897435897435898, 0.3150684931506849, 0.4107142857142857, 0.37849434427245765, 0.8156155347824097)\n",
      "(0.5789473684210527, 0.3013698630136986, 0.39639639639639634, 0.3647103399453675, 0.8161847591400146)\n",
      "(0.5675675675675675, 0.2876712328767123, 0.3818181818181817, 0.35065614226542297, 0.8162030577659607)\n",
      "(0.5833333333333334, 0.2876712328767123, 0.3853211009174312, 0.3577480895454716, 0.8162057399749756)\n",
      "(0.6, 0.2876712328767123, 0.3888888888888889, 0.3651061490130469, 0.8222827315330505)\n",
      "(0.5882352941176471, 0.273972602739726, 0.37383177570093457, 0.35073380080321387, 0.824730634689331)\n",
      "(0.5757575757575758, 0.2602739726027397, 0.3584905660377358, 0.33604430362909005, 0.8259192705154419)\n",
      "(0.59375, 0.2602739726027397, 0.3619047619047619, 0.34367030939302357, 0.8278613090515137)\n",
      "(0.6129032258064516, 0.2602739726027397, 0.36538461538461536, 0.3516197615442867, 0.8333424925804138)\n",
      "(0.6333333333333333, 0.2602739726027397, 0.36893203883495146, 0.35991890306534524, 0.8337457180023193)\n",
      "(0.6206896551724138, 0.2465753424657534, 0.35294117647058826, 0.3448618163481686, 0.8345327377319336)\n",
      "(0.6071428571428571, 0.2328767123287671, 0.33663366336633666, 0.32941799963625634, 0.8347922563552856)\n",
      "(0.5925925925925926, 0.2191780821917808, 0.32, 0.313557339948705, 0.8368188142776489)\n",
      "(0.6153846153846154, 0.2191780821917808, 0.3232323232323232, 0.3222488447007484, 0.8368812799453735)\n",
      "(0.6, 0.2054794520547945, 0.3061224489795918, 0.30592228802269406, 0.8392333984375)\n",
      "(0.625, 0.2054794520547945, 0.30927835051546393, 0.315073099429182, 0.8409807682037354)\n",
      "(0.6086956521739131, 0.1917808219178082, 0.29166666666666663, 0.2982331239975658, 0.846588134765625)\n",
      "(0.5909090909090909, 0.1780821917808219, 0.2736842105263158, 0.2808298609337591, 0.8532367944717407)\n",
      "(0.5714285714285714, 0.1643835616438356, 0.2553191489361702, 0.26280741511945294, 0.8537203073501587)\n",
      "(0.55, 0.1506849315068493, 0.23655913978494625, 0.24410109457453164, 0.8537205457687378)\n",
      "(0.5263157894736842, 0.136986301369863, 0.21739130434782608, 0.2246354665754645, 0.8537259101867676)\n",
      "(0.5, 0.1232876712328767, 0.19780219780219777, 0.20432184952152485, 0.8538134694099426)\n",
      "(0.5294117647058824, 0.1232876712328767, 0.2, 0.21374026859538542, 0.8568501472473145)\n",
      "(0.5, 0.1095890410958904, 0.1797752808988764, 0.19231190982771443, 0.8622720241546631)\n",
      "(0.5333333333333333, 0.1095890410958904, 0.18181818181818182, 0.2023546545849771, 0.8652635812759399)\n",
      "(0.5714285714285714, 0.1095890410958904, 0.1839080459770115, 0.21331750855352405, 0.8660643100738525)\n",
      "(0.5384615384615384, 0.0958904109589041, 0.1627906976744186, 0.19039759616679794, 0.8697233200073242)\n",
      "(0.5833333333333334, 0.0958904109589041, 0.16470588235294117, 0.2023587788935551, 0.872649073600769)\n",
      "(0.5454545454545454, 0.0821917808219178, 0.14285714285714285, 0.17776731533644813, 0.898523211479187)\n",
      "(0.5, 0.0684931506849315, 0.12048192771084336, 0.15127382447685542, 0.9055663347244263)\n",
      "(0.5555555555555556, 0.0684931506849315, 0.1219512195121951, 0.1643371061004847, 0.9058200120925903)\n",
      "(0.625, 0.0684931506849315, 0.12345679012345677, 0.17947388347530574, 0.905823826789856)\n",
      "(0.7142857142857143, 0.0684931506849315, 0.125, 0.19738150309845418, 0.9058247208595276)\n",
      "(0.6666666666666666, 0.0547945205479452, 0.10126582278481013, 0.16796600999030734, 0.9105516672134399)\n",
      "(0.6, 0.0410958904109589, 0.07692307692307691, 0.13453223656299196, 0.9157296419143677)\n",
      "(0.75, 0.0410958904109589, 0.07792207792207792, 0.15777618649678907, 0.9169957637786865)\n",
      "(1.0, 0.0410958904109589, 0.07894736842105263, 0.1906742270845093, 0.9172015190124512)\n",
      "(1.0, 0.0273972602739726, 0.05333333333333332, 0.15555677149019512, 0.9172077178955078)\n",
      "(1.0, 0.0136986301369863, 0.027027027027027025, 0.10990490279208061, 0.9172743558883667)\n",
      "(1.0, 0.0, 0.0, 0.0, 1.0)\n"
     ]
    }
   ],
   "source": [
    "for i in zip(precision,recall,f1,mccs,np.append(thresholds,1)):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-06T19:15:05.178430Z",
     "start_time": "2019-12-06T19:15:05.169778Z"
    }
   },
   "source": [
    "## Compare Predictions with Actually Toxic Compounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T09:20:03.395375Z",
     "start_time": "2019-12-09T09:20:03.380869Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max F1 Decision Threshold: 0.7035\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NR.AhR</th>\n",
       "      <th>DNN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>NCGC00261776-01</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>NCGC00261662-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>NCGC00261119-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>NCGC00260831-01</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>NCGC00261395-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>NCGC00357175-01</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>NCGC00356994-01</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>NCGC00357111-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>NCGC00357249-01</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>NCGC00357051-01</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>73 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 NR.AhR  DNN\n",
       "NCGC00261776-01       1    1\n",
       "NCGC00261662-01       1    0\n",
       "NCGC00261119-01       1    0\n",
       "NCGC00260831-01       1    1\n",
       "NCGC00261395-01       1    0\n",
       "...                 ...  ...\n",
       "NCGC00357175-01       1    1\n",
       "NCGC00356994-01       1    1\n",
       "NCGC00357111-01       1    0\n",
       "NCGC00357249-01       1    1\n",
       "NCGC00357051-01       1    1\n",
       "\n",
       "[73 rows x 2 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Max F1 Decision Threshold: %0.4f' % m_thresh)\n",
    "y_hat_testing_adj=adjusted_classes(p_te,m_thresh)\n",
    "y_hat=pd.DataFrame(y_hat_testing_adj,columns=['DNN'],index=y_te[rows_te].index)\n",
    "compare_TP = pd.concat([y_te[target][rows_te].astype('int'), y_hat],axis=1)\n",
    "compare_TP[compare_TP[target]==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:metis] *",
   "language": "python",
   "name": "conda-env-metis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
